‚úÖ Ê®°ÂûãÂä†ËΩΩÂÆåÊàêÔºå‰ΩøÁî®ËÆæÂ§á: cuda
Currently using DataParallel (DP) for multi-gpu training, while DistributedDataParallel (DDP) is recommended for faster training. See https://sbert.net/docs/sentence_transformer/training/distributed.html for more information.
[34m[1mwandb[0m: [33mWARNING[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.
[34m[1mwandb[0m: [33mWARNING[0m The get_url method is deprecated and will be removed in a future release. Please use `run.url` instead.
 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                                                   | 52/285 [00:07<00:31,  7.32it/s]Traceback (most recent call last):
  File "/home/algo/hrz/db_construct/reranker_ft.py", line 96, in <module>
    model.fit(
  File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/sentence_transformers/cross_encoder/fit_mixin.py", line 368, in fit
    trainer.train()
  File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/transformers/trainer.py", line 1885, in train
    return inner_training_loop(
  File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/transformers/trainer.py", line 2216, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/transformers/trainer.py", line 3238, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/sentence_transformers/trainer.py", line 406, in compute_loss
    loss = loss_fn(features, labels)
  File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/sentence_transformers/cross_encoder/losses/BinaryCrossEntropyLoss.py", line 100, in forward
    tokens = self.model.tokenizer(
  File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 2883, in __call__
    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)
  File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 2969, in _call_one
    return self.batch_encode_plus(
  File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 3160, in batch_encode_plus
    return self._batch_encode_plus(
  File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/transformers/tokenization_utils_fast.py", line 511, in _batch_encode_plus
    encodings = self._tokenizer.encode_batch(
KeyboardInterrupt
Traceback (most recent call last):
  File "/home/algo/hrz/db_construct/reranker_ft.py", line 96, in <module>
    model.fit(
  File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/sentence_transformers/cross_encoder/fit_mixin.py", line 368, in fit
    trainer.train()
  File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/transformers/trainer.py", line 1885, in train
    return inner_training_loop(
  File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/transformers/trainer.py", line 2216, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/transformers/trainer.py", line 3238, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/sentence_transformers/trainer.py", line 406, in compute_loss
    loss = loss_fn(features, labels)
  File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/sentence_transformers/cross_encoder/losses/BinaryCrossEntropyLoss.py", line 100, in forward
    tokens = self.model.tokenizer(
  File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 2883, in __call__
    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)
  File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 2969, in _call_one
    return self.batch_encode_plus(
  File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 3160, in batch_encode_plus
    return self._batch_encode_plus(
  File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/transformers/tokenization_utils_fast.py", line 511, in _batch_encode_plus
    encodings = self._tokenizer.encode_batch(
KeyboardInterrupt
