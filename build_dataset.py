from typing import List, Dict
import random
import json
from openai import OpenAI
import os
from pymilvus import connections, Collection
import random
from utils.db_utils import query_milvus_blocks, query_es_blocks, deduplicate_ranked_blocks
from langchain_huggingface import HuggingFaceEmbeddings
import torch
import os
import json
import random
from typing import List, Dict
from langchain_openai import ChatOpenAI
import warnings
warnings.filterwarnings("ignore", category=UserWarning, module="sklearn")


os.environ["OPENAI_API_KEY"] = "sk-d14cb74d17974a1fb4db738eaf5e15e6"

# âœ… åˆå§‹åŒ– DeepSeek æ¨¡å‹
llm = ChatOpenAI(
    model="deepseek-chat",
    api_key=os.getenv("sk-d14cb74d17974a1fb4db738eaf5e15e6"),
    base_url="https://api.deepseek.com",
)


def format_chunk_for_reranker(chunk: Dict) -> str:
    """
    å°† chunk ä¸­çš„ç»“æ„åŒ–å­—æ®µæ‹¼æ¥ä¸ºé€‚ç”¨äº reranker çš„ doc å­—æ®µã€‚
    """
    page_name = chunk.get("page_name", "").strip()
    title = chunk.get("title", "").strip()
    page_url = chunk.get("page_url", "").strip()
    summary = chunk.get("summary", "").strip()
    text = chunk.get("text", "").strip()

    return (
        f"ã€é¡µé¢åç§°ã€‘ï¼š{page_name}\n"
        f"ã€æ®µè½æ ‡é¢˜ã€‘ï¼š{title}\n"
        f"ã€æ¥æºè·¯å¾„ã€‘ï¼š{page_url}\n"
        f"ã€æ‘˜è¦ã€‘ï¼š{summary}\n"
        f"ã€æ­£æ–‡ã€‘ï¼š{text}"
    )


def build_contrastive_prompt_with_selection(chunks: List[Dict]) -> str:
    prompt = (
        "ä½ æ˜¯ä¸€ä¸ªç”µå•†å¹³å°æ™ºèƒ½é—®ç­”ç³»ç»Ÿè®­ç»ƒæ•°æ®æ„å»ºåŠ©æ‰‹ã€‚\n"
        "ä»¥ä¸‹æ˜¯å‡ æ®µç”¨æˆ·æ£€ç´¢å¯èƒ½å‘½ä¸­çš„å¹³å°çŸ¥è¯†å†…å®¹ï¼Œæ¯æ®µåŒ…å«ï¼šé¡µé¢åç§°ã€æ®µè½æ ‡é¢˜ã€æ¥æºè·¯å¾„ã€æ‘˜è¦ã€æ­£æ–‡ç‰‡æ®µã€‚\n\n"
        "ğŸ¯ ä½ çš„ä»»åŠ¡ï¼š\n"
        "1. é˜…è¯»æ‰€æœ‰æ®µè½ï¼Œæ„é€ ä¸€ä¸ªè‡ªç„¶å£è¯­åŒ–çš„é—®é¢˜ï¼›\n"
        "2. è¯¥é—®é¢˜å¿…é¡»åªèƒ½ç”±å…¶ä¸­ä¸€æ®µå‡†ç¡®å›ç­”ï¼Œå…¶ä»–æ®µå®¹æ˜“æ··æ·†ï¼›\n"
        "3. é—®é¢˜å¿…é¡»ä½“ç°å‡ºåŒºåˆ†æ€§çš„ç‰¹å¾ï¼Œå¯ä»¥åŸºäºè·¯å¾„ã€é¡µé¢åç§°ã€æ ‡é¢˜æˆ–æ‘˜è¦ï¼Œè€Œä¸ä»…ä»…æ˜¯æ­£æ–‡å†…å®¹ï¼›\n"
        "4. ç„¶åå‘Šè¯‰æˆ‘é—®é¢˜æœ€åŒ¹é…çš„æ˜¯å“ªä¸€æ®µï¼ˆç”¨ç¼–å·1~6è¡¨ç¤ºï¼‰ã€‚\n\n"
        "ğŸ“Œ è¾“å‡ºæ ¼å¼ï¼š\n"
        "é—®é¢˜ï¼š<ä¸€å¥è‡ªç„¶ä¸­æ–‡é—®é¢˜>\n"
        "å¯¹åº”æ®µç¼–å·ï¼š<1~>\n"
    )

    for i, chunk in enumerate(chunks):
        prompt += (
            f"\nã€å€™é€‰å†…å®¹ç¼–å·{i+1}ã€‘\n"
            f"é¡µé¢åç§°ï¼š{chunk.get('page_name', '')}\n"
            f"æ®µè½æ ‡é¢˜ï¼š{chunk.get('title', '')}\n"
            f"æ¥æºè·¯å¾„ï¼š{chunk.get('page_url', '')}\n"
            f"æ‘˜è¦ï¼š{chunk.get('summary', '')}\n"
            f"æ­£æ–‡ï¼š{chunk.get('text', '')}\n"
        )

    return prompt


def call_deepseek_chat(prompt: str) -> str:
    return llm.invoke(prompt).content.strip()


def parse_and_save_qa_pairs(response: str, chunks: List[Dict], output_path: str):
    try:
        lines = [line.strip() for line in response.splitlines() if line.strip()]
        question_line = next((line for line in lines if "é—®é¢˜ï¼š" in line), None)
        index_line = next((line for line in lines if "ç¼–å·" in line or "å¯¹åº”æ®µç¼–å·" in line), None)

        query = question_line.split("é—®é¢˜ï¼š", 1)[-1].strip()
        idx_str = index_line.split("ç¼–å·", 1)[-1].strip("ï¼š:.)ï¼‰ ")
        local_idx = int(idx_str) - 1
    except Exception as e:
        print(f"âš ï¸ è§£æå¤±è´¥ï¼š{e}ï¼Œä½¿ç”¨ fallback")
        query = response.strip()
        local_idx = random.randint(0, 5)

    data = []
    for i, chunk in enumerate(chunks):
        label = 1 if i == local_idx else 0
        formatted_doc = format_chunk_for_reranker(chunk)
        data.append({
            "query": query,
            "doc": formatted_doc,
            "label": label
        })

    with open(output_path, "a", encoding="utf-8") as f:
        for item in data:
            f.write(json.dumps(item, ensure_ascii=False) + "\n")

# âœ… ä¸»æµç¨‹ï¼šå¯¹ä¸€ä¸ª sample æ‰§è¡Œæ„å»º QA è®­ç»ƒæ ·æœ¬

def process_one_sample(sample: Dict, query_fn, output_path: str):
    chunklist = query_fn(sample)  # ä½ è´Ÿè´£å®ç°æ­¤å‡½æ•°ï¼Œè¿”å› List[Dict]ï¼Œå…±6æ¡ chunk
    # print((len(chunklist)))
    # print(chunklist)
    # exit()
    if len(chunklist) < 2:
        print("âŒ æ£€ç´¢ç»“æœä¸è¶³2æ¡ï¼Œè·³è¿‡è¯¥æ ·æœ¬")
        return

    prompt = build_contrastive_prompt_with_selection(chunklist)
    response = call_deepseek_chat(prompt)
    parse_and_save_qa_pairs(response, chunklist, output_path)




if __name__ == "__main__":

    # âœ… å‚æ•°è®¾ç½®
    embedder_model = "/data/huangruizhi/htmlRAG/bce-embedding-base_v1"
    device = "cuda:1" if torch.cuda.is_available() else "cpu"
    Milvus_host = "192.168.7.247"
    ES_host = "192.168.7.247"
    COLLECTION_NAME = "test_env"
    SAMPLE_SIZE = 2500
    top_k = 2
    OUTPUT_PATH = "reranker_qa_dataset.jsonl"
    
    print("ğŸ“¦ åŠ è½½ Embedder æ¨¡å‹...")
    embedder = HuggingFaceEmbeddings(model_name=embedder_model,model_kwargs={"device": device})


    # âœ… è¿æ¥ Milvus å¹¶åŠ è½½é›†åˆ
    connections.connect(alias="default", host=Milvus_host, port="19530")
    collection = Collection(name=COLLECTION_NAME)
    all_ids = collection.query(
        expr="",
        output_fields=["global_chunk_idx"],
        limit=10000
    )
    valid_ids = [item["global_chunk_idx"] for item in all_ids]
    sample_ids = random.sample(valid_ids, SAMPLE_SIZE)

    def query_fn(sample: Dict) -> List[Dict]:
        global_chunk_idx = sample["global_chunk_idx"]
        # âœ… æŸ¥è¯¢ä¸»é”®å¯¹åº”é—®é¢˜
        res = collection.query(
            expr=f"global_chunk_idx == {global_chunk_idx}",
            output_fields=["question"]
        )
        print(f"ğŸ” æŸ¥è¯¢ä¸»é”®ï¼š{global_chunk_idx}")
        if not res or not res[0].get("question"):
            return []
        question = res[0]["question"]
        print(f"ğŸ” æŸ¥è¯¢é—®é¢˜ï¼š{question}")

        # ä½ éœ€è¦å®ç°è¿™ä¸¤ä¸ªå‡½æ•°ï¼ˆè¯­ä¹‰+å…³é”®è¯ï¼‰
        milvus_results = query_milvus_blocks(Milvus_host, question, embedder, top_k=top_k, milvus_collection_name=COLLECTION_NAME)
        es_results = query_es_blocks(ES_host, question, top_k=top_k, es_index_name=COLLECTION_NAME)
        # print(f"ğŸ” Milvus æ£€ç´¢ç»“æœï¼š{len(milvus_results)}")
        # print(f"ğŸ” ES æ£€ç´¢ç»“æœï¼š{len(es_results)}")
        results = deduplicate_ranked_blocks(milvus_results + es_results)
        return results

    # âœ… ä¸»æµç¨‹ï¼šé‡‡æ ·å¹¶æ„å»ºè®­ç»ƒæ ·æœ¬
    for idx in sample_ids:
        sample = {"global_chunk_idx": idx}
        process_one_sample(sample, query_fn=query_fn, output_path=OUTPUT_PATH)
