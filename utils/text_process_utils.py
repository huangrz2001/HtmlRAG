import os
import re
import jieba
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import json

# å…³é—­å¹¶è¡ŒåŒ–è­¦å‘Šï¼Œé¿å…æ§åˆ¶å°å†—ä½™ä¿¡æ¯
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# åˆå§‹åŒ–è‡ªå®šä¹‰åˆ†è¯å™¨ï¼Œä¸ä½¿ç”¨å…¨å±€å½±å“çš„ jieba åŠ è½½è¯å…¸
pure_tokenizer = jieba.Tokenizer(dictionary=jieba.DEFAULT_DICT)
jieba.load_userdict("./user_dict.txt")


# ======================== æ–‡æœ¬å¤„ç†å·¥å…·å‡½æ•° ========================

def clean_text(text: str) -> str:
    """ç§»é™¤æ–‡æœ¬ä¸­çš„ç‰¹æ®Šå­—ç¬¦ï¼Œä»…ä¿ç•™ä¸­è‹±æ–‡ä¸æ•°å­—"""
    return "".join(re.findall(r"[\u4e00-\u9fa5a-zA-Z0-9]+", text))


def jieba_cut_clean(text: str) -> list:
    """ç»“åˆ clean_text ä¸è‡ªå®šä¹‰åˆ†è¯å™¨è¿›è¡Œåˆ†è¯å¤„ç†"""
    text = clean_text(text)
    return list(pure_tokenizer.cut(text, HMM=False))


def clean_invisible(text):
    # å»é™¤æ‰€æœ‰ Unicode æ§åˆ¶å­—ç¬¦
    return re.sub(r'[\u200b-\u200f\u202a-\u202e\u2060-\u206f]', '', text)

# ======================== æ–‡æ¡£å—æ ‡é¢˜æå–å‡½æ•° ========================

def extract_title_from_block(tag) -> str:
    """
    ä» HTML tag ä¸­æå–ç¬¬ä¸€ä¸ª heading æ ‡ç­¾ï¼ˆh1~h6ï¼‰ä½œä¸ºæ ‡é¢˜
    è‹¥ä¸å­˜åœ¨ï¼Œåˆ™å›é€€ä¸ºç¬¬ä¸€ä¸ªéç©ºæ–‡æœ¬
    """
    from bs4 import Tag

    for descendant in tag.descendants:
        if isinstance(descendant, Tag) and descendant.name and descendant.name.lower().startswith("h"):
            return descendant.get_text(separator="", strip=True)[:48]

    for t in tag.stripped_strings:
        if t.strip():
            return t.strip()[:48]
    return ""


# ======================== Elasticsearch å»é‡ ========================

def is_duplicate_in_es(
    es,
    index_name,
    text,
    page_name,
    threshold_content=0.9,
    threshold_page_name=0.6,
    top_k=5,
) -> bool:
    """
    åŸºäº ES æŸ¥è¯¢ç›¸ä¼¼å†…å®¹å—ï¼Œåˆ¤æ–­æ˜¯å¦é‡å¤ï¼š
    - å†…å®¹ç›¸ä¼¼åº¦ >= é˜ˆå€¼ ä¸”
    - é¡µé¢åç§°ç›¸ä¼¼åº¦ >= é˜ˆå€¼
    """
    query = {"query": {"match": {"content": text}}}

    try:
        resp = es.search(index=index_name, body=query, size=top_k)
    except Exception as e:
        print(f"âš ï¸ Elasticsearch æŸ¥è¯¢å¤±è´¥: {e}")
        return False

    text_cleaned = clean_text(text)
    page_name_cleaned = clean_text(page_name)

    for hit in resp["hits"]["hits"]:
        content_existing = hit["_source"].get("content", "")
        page_name_existing = hit["_source"].get("page_name", "")

        try:
            vectorizer = TfidfVectorizer(tokenizer=jieba_cut_clean)
            content_sim = cosine_similarity(
                vectorizer.fit_transform([text_cleaned, clean_text(content_existing)])
            )[0, 1]
            title_sim = cosine_similarity(
                vectorizer.fit_transform([page_name_cleaned, clean_text(page_name_existing)])
            )[0, 1]
        except Exception as e:
            print(f"âš ï¸ ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
            continue

        if content_sim >= threshold_content and title_sim >= threshold_page_name:
            print(f"\nâ›”ï¸ å†…å®¹é‡å¤åº¦ {content_sim:.3f}ï¼Œæ ‡é¢˜é‡å¤åº¦ {title_sim:.3f}ï¼Œåˆ¤ä¸ºé‡å¤")
            print("ğŸ‘‰ å½“å‰æ–‡æœ¬ï¼š", text_cleaned[:300] + ("..." if len(text_cleaned) > 300 else ""))
            print("ğŸ‘‰ ç›¸ä¼¼ ES æ–‡æœ¬ï¼š", clean_text(content_existing)[:300] + ("..." if len(content_existing) > 300 else ""))
            print("ğŸ‘‰ å½“å‰æ ‡é¢˜ï¼š", page_name_cleaned)
            print("ğŸ‘‰ ES ä¸­æ ‡é¢˜ï¼š", clean_text(page_name_existing))
            print("=" * 80)
            return True

    return False


# ======================== æ–‡æ¡£å†…é‡å¤å—è¿‡æ»¤ ========================

def filter_duplicate_blocks(texts: list, threshold=0.9) -> list:
    """
    åŸºäº TF-IDF å‘é‡ä¸ cosine ç›¸ä¼¼åº¦ï¼Œè¿‡æ»¤é‡å¤æ–‡æœ¬å—
    è¿”å›ä¿ç•™çš„ç´¢å¼•åˆ—è¡¨
    """
    if len(texts) <= 1:
        return list(range(len(texts)))

    cleaned_texts = [clean_text(t) for t in texts]
    vectorizer = TfidfVectorizer(tokenizer=jieba_cut_clean)
    tfidf_matrix = vectorizer.fit_transform(cleaned_texts)
    similarity_matrix = cosine_similarity(tfidf_matrix)

    keep_indices = []
    seen = set()
    for i in range(len(cleaned_texts)):
        if i in seen:
            continue
        keep_indices.append(i)
        for j in range(i + 1, len(cleaned_texts)):
            if similarity_matrix[i][j] >= threshold:
                seen.add(j)

    return keep_indices


# ======================== æ£€ç´¢ç»“æœå»é‡å‡½æ•°ï¼ˆé€‚ç”¨äº Milvus/ESï¼‰ ========================

def deduplicate_ranked_blocks(docs: list,
                              threshold_content=0.9,
                              threshold_page_name=0.6) -> list:
    """
    å»é‡æ£€ç´¢ç»“æœåˆ—è¡¨ï¼Œåˆ¤æ–­ä¾æ®ï¼š
    - å†…å®¹ç›¸ä¼¼åº¦
    - é¡µé¢åç›¸ä¼¼åº¦
    """
    if len(docs) <= 1:
        return docs

    keep, seen = [], set()

    for i, base in enumerate(docs):
        if i in seen:
            continue

        base_text = clean_text(base.get("text", ""))
        base_name = clean_text(base.get("page_name", ""))
        keep.append(base)

        for j in range(i + 1, len(docs)):
            if j in seen:
                continue

            comp = docs[j]
            comp_text = clean_text(comp.get("text", ""))
            comp_name = clean_text(comp.get("page_name", ""))

            try:
                vectorizer = TfidfVectorizer(tokenizer=jieba_cut_clean)
                sim_text = cosine_similarity(vectorizer.fit_transform([base_text, comp_text]))[0, 1]
                sim_name = cosine_similarity(vectorizer.fit_transform([base_name, comp_name]))[0, 1]
            except Exception as e:
                print(f"âš ï¸ ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
                continue
            if sim_text >= threshold_content and sim_name >= threshold_page_name:
                print(f"\nğŸ” æ¯”è¾ƒå— i={i} vs j={j}")
                print(f"ğŸ“ å†…å®¹ç›¸ä¼¼åº¦: {sim_text:.3f}ï¼Œæ ‡é¢˜ç›¸ä¼¼åº¦: {sim_name:.3f}")
                print("â›”ï¸ åˆ¤ä¸ºé‡å¤ï¼Œè·³è¿‡å— j\n" + "=" * 80)
                seen.add(j)

    return keep



def generate_summary_ChatGLM(
        text, model, tokenizer, 
        max_new_tokens=200, 
        min_trigger_length=200, 
        fallback_length=100
    ):
        """
        ç”¨ ChatGLM ç”Ÿæˆæ‘˜è¦ï¼š
        - å¦‚æœæ­£æ–‡é•¿åº¦å°äº min_trigger_lengthï¼Œåˆ™ç›´æ¥è¿”å›å…¨æ–‡ï¼›
        - å¦‚æœæ‘˜è¦å¤±è´¥ï¼Œåˆ™å…œåº•è¿”å›æ­£æ–‡å‰ fallback_length ä¸ªå­—ç¬¦ã€‚
        """
        text = text.strip().replace("\x00", "")
                
        if len(text) < min_trigger_length * 2:
            return text[:min_trigger_length]  # æ­£æ–‡å¤ªçŸ­ï¼Œç›´æ¥è¿”å›
        
        prompt = (
            "è¯·ä½ é˜…è¯»ä»¥ä¸‹å†…å®¹ï¼Œå¹¶ç”¨ç®€æ´çš„è¯­è¨€æ€»ç»“å‡ºå…¶ä¸»è¦ä¿¡æ¯å’Œæ ¸å¿ƒè¦ç‚¹ï¼Œ"
            "çªå‡ºè¿è¥ç­–ç•¥æˆ–å¹³å°è§„åˆ™ï¼Œé™åˆ¶åœ¨100å­—ä»¥å†…ï¼š\n\n"
            f"ã€æ–‡æ¡£å†…å®¹ã€‘\n{text[:6000]}"
        )
        
        try:
            response, _ = model.chat(
                tokenizer=tokenizer,
                query=prompt,
                history=[],
                max_new_tokens=max_new_tokens,
                do_sample=True,
                temperature=0.4,
                top_p=0.8,
            )
            response = response.strip()
            if response:
                return response
            else:
                print("âš ï¸ ChatGLM è¿”å›ç©ºæ‘˜è¦ï¼Œå¯ç”¨å…œåº•æ–‡æœ¬ã€‚")
                return text[:fallback_length]
        except Exception as e:
            print(f"âš ï¸ ChatGLM æ‘˜è¦ç”Ÿæˆå¤±è´¥: {e}ï¼Œå¯ç”¨å…œåº•æ–‡æœ¬ã€‚")
            return text[:fallback_length]

def generate_block_documents(
    block_tree,
    page_url="unknown.html",
    summary_model=None,
    summary_tokenizer=None,
    time_value=""
):
    """
    ç”Ÿæˆæ–‡æ¡£å—çš„å…ƒä¿¡æ¯åˆ—è¡¨ï¼ˆdoc_metaï¼‰ï¼Œç”¨äºä¿å­˜ä¸º JSON æˆ–å…¥åº“ã€‚
    æ‰“å°å¤„ç†è¿›åº¦å’Œå†…å®¹æ‘˜è¦ä¿¡æ¯ã€‚
    """
    from utils.text_process_utils import extract_title_from_block, clean_invisible

    path_tags = [b[0] for b in block_tree]
    doc_meta = []

    print(f"ğŸ“¦ å…±æå–å—æ•°ï¼š{len(path_tags)}")

    for pidx, tag in enumerate(path_tags):
        print(f"\nğŸ§© æ­£åœ¨å¤„ç†ç¬¬ {pidx+1}/{len(path_tags)} ä¸ª block")
        text = tag.get_text().strip().replace("\x00", "")
        text = clean_invisible(text)
        if not text:
            print("âš ï¸ ç©ºå†…å®¹ï¼Œè·³è¿‡")
            continue

        preview = text[:80].replace('\n', ' ') + ("..." if len(text) > 80 else "")
        print(f"ğŸ“„ æ–‡æœ¬é¢„è§ˆï¼š{preview}")

        title = extract_title_from_block(tag)
        print(f"ğŸ·ï¸ æå–æ ‡é¢˜ï¼š{title[:128]}")

        page_name = os.path.splitext(os.path.basename(page_url))[0]
        summary = ""

        if summary_model and summary_tokenizer:
            summary = generate_summary_ChatGLM(text, summary_model, summary_tokenizer)
            print(f"âœ… æ‘˜è¦ç”ŸæˆæˆåŠŸï¼š{summary}")

        doc_meta.append({
            "chunk_idx": pidx,
            "page_name": page_name,
            "title": title[:64],
            "page_url": page_url,
            "summary": summary,
            "text": text,
            "time": time_value,
        })

    print(f"\nâœ… æ‰€æœ‰å—å¤„ç†å®Œæ¯•ï¼Œå…±ç”Ÿæˆ {len(doc_meta)} æ¡æœ‰æ•ˆæ–‡æ¡£å—")
    return doc_meta



def save_doc_meta_to_block_dir(doc_meta, html_path, html_root_dir, block_root_dir):
    """
    ä¿å­˜ JSON æ–‡ä»¶ï¼Œè·¯å¾„æ˜ å°„ï¼š
    html_path = a/b/c.html â†’ ä¿å­˜ä¸º a_blocks/b/c.json
    """
    # ç›¸å¯¹è·¯å¾„ï¼šb/c.html
    rel_path = os.path.relpath(html_path, html_root_dir)

    # è¾“å‡ºè·¯å¾„ï¼ša_blocks/b/c.json
    rel_json_path = os.path.splitext(rel_path)[0] + ".json"
    json_full_path = os.path.join(block_root_dir, rel_json_path)

    # åˆ›å»ºç›®æ ‡ç›®å½•
    os.makedirs(os.path.dirname(json_full_path), exist_ok=True)

    # å†™å…¥ JSON æ–‡ä»¶
    with open(json_full_path, "w", encoding="utf-8") as f:
        json.dump(doc_meta, f, ensure_ascii=False, indent=2)

    print(f"âœ… JSON å·²ä¿å­˜ï¼š{json_full_path}")
    return json_full_path


