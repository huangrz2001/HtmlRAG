import os
import re
import jieba
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import json
from datetime import datetime
import torch
import jieba.analyse

# å…³é—­å¹¶è¡ŒåŒ–è­¦å‘Šï¼Œé¿å…æ§åˆ¶å°å†—ä½™ä¿¡æ¯
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# åˆå§‹åŒ–è‡ªå®šä¹‰åˆ†è¯å™¨ï¼Œä¸ä½¿ç”¨å…¨å±€å½±å“çš„ jieba åŠ è½½è¯å…¸
pure_tokenizer = jieba.Tokenizer(dictionary=jieba.DEFAULT_DICT)
jieba.load_userdict("./user_dict.txt")


# ======================== æ–‡æœ¬å¤„ç†å·¥å…·å‡½æ•° ========================

def clean_text(text: str) -> str:
    """ç§»é™¤æ–‡æœ¬ä¸­çš„ç‰¹æ®Šå­—ç¬¦ï¼Œä»…ä¿ç•™ä¸­è‹±æ–‡ä¸æ•°å­—"""
    return "".join(re.findall(r"[\u4e00-\u9fa5a-zA-Z0-9]+", text))


def jieba_cut_clean(text: str) -> list:
    """ç»“åˆ clean_text ä¸è‡ªå®šä¹‰åˆ†è¯å™¨è¿›è¡Œåˆ†è¯å¤„ç†"""
    text = clean_text(text)
    return list(pure_tokenizer.cut(text, HMM=False))


def clean_invisible(text):
    # å»é™¤æ‰€æœ‰ Unicode æ§åˆ¶å­—ç¬¦
    return re.sub(r'[\u200b-\u200f\u202a-\u202e\u2060-\u206f]', '', text)

# ======================== æ–‡æ¡£å—æ ‡é¢˜æå–å‡½æ•° ========================

def extract_title_from_block(tag) -> str:
    """
    ä» HTML tag ä¸­æå–ç¬¬ä¸€ä¸ª heading æ ‡ç­¾ï¼ˆh1~h6ï¼‰ä½œä¸ºæ ‡é¢˜
    è‹¥ä¸å­˜åœ¨ï¼Œåˆ™å›é€€ä¸ºç¬¬ä¸€ä¸ªéç©ºæ–‡æœ¬
    """
    from bs4 import Tag

    for descendant in tag.descendants:
        if isinstance(descendant, Tag) and descendant.name and descendant.name.lower().startswith("h"):
            return descendant.get_text(separator="", strip=True)[:48]

    for t in tag.stripped_strings:
        if t.strip():
            return t.strip()[:48]
    return ""


# ======================== Elasticsearch å»é‡ ========================

def is_duplicate_in_es(
    es,
    index_name,
    text,
    page_name,
    threshold_content=0.9,
    threshold_page_name=0.6,
    top_k=5,
) -> bool:
    """
    åŸºäº ES æŸ¥è¯¢ç›¸ä¼¼å†…å®¹å—ï¼Œåˆ¤æ–­æ˜¯å¦é‡å¤ï¼š
    - å†…å®¹ç›¸ä¼¼åº¦ >= é˜ˆå€¼ ä¸”
    - é¡µé¢åç§°ç›¸ä¼¼åº¦ >= é˜ˆå€¼
    """
    query = {"query": {"match": {"content": text}}}

    try:
        resp = es.search(index=index_name, body=query, size=top_k)
    except Exception as e:
        print(f"âš ï¸ Elasticsearch æŸ¥è¯¢å¤±è´¥: {e}")
        return False

    text_cleaned = clean_text(text)
    page_name_cleaned = clean_text(page_name)

    for hit in resp["hits"]["hits"]:
        content_existing = hit["_source"].get("content", "")
        page_name_existing = hit["_source"].get("page_name", "")

        try:
            vectorizer = TfidfVectorizer(tokenizer=jieba_cut_clean)
            content_sim = cosine_similarity(
                vectorizer.fit_transform([text_cleaned, clean_text(content_existing)])
            )[0, 1]
            title_sim = cosine_similarity(
                vectorizer.fit_transform([page_name_cleaned, clean_text(page_name_existing)])
            )[0, 1]
        except Exception as e:
            print(f"âš ï¸ ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
            continue

        if content_sim >= threshold_content and title_sim >= threshold_page_name:
            print(f"\nâ›”ï¸ å†…å®¹é‡å¤åº¦ {content_sim:.3f}ï¼Œæ ‡é¢˜é‡å¤åº¦ {title_sim:.3f}ï¼Œåˆ¤ä¸ºé‡å¤")
            print("ğŸ‘‰ å½“å‰æ–‡æœ¬ï¼š", text_cleaned[:300] + ("..." if len(text_cleaned) > 300 else ""))
            print("ğŸ‘‰ ç›¸ä¼¼ ES æ–‡æœ¬ï¼š", clean_text(content_existing)[:300] + ("..." if len(content_existing) > 300 else ""))
            print("ğŸ‘‰ å½“å‰æ ‡é¢˜ï¼š", page_name_cleaned)
            print("ğŸ‘‰ ES ä¸­æ ‡é¢˜ï¼š", clean_text(page_name_existing))
            print("=" * 80)
            return True

    return False


# ======================== æ–‡æ¡£å†…é‡å¤å—è¿‡æ»¤ ========================

def filter_duplicate_blocks(texts: list, threshold=0.9) -> list:
    """
    åŸºäº TF-IDF å‘é‡ä¸ cosine ç›¸ä¼¼åº¦ï¼Œè¿‡æ»¤é‡å¤æ–‡æœ¬å—
    è¿”å›ä¿ç•™çš„ç´¢å¼•åˆ—è¡¨
    """
    if len(texts) <= 1:
        return list(range(len(texts)))

    cleaned_texts = [clean_text(t) for t in texts]
    vectorizer = TfidfVectorizer(tokenizer=jieba_cut_clean)
    tfidf_matrix = vectorizer.fit_transform(cleaned_texts)
    similarity_matrix = cosine_similarity(tfidf_matrix)

    keep_indices = []
    seen = set()
    for i in range(len(cleaned_texts)):
        if i in seen:
            continue
        keep_indices.append(i)
        for j in range(i + 1, len(cleaned_texts)):
            if similarity_matrix[i][j] >= threshold:
                seen.add(j)

    return keep_indices





# ======================== ä¼˜åŒ–çš„ Jieba æŸ¥è¯¢æ„å»ºå‡½æ•° ========================
def build_optimal_jieba_query(
    jieba_keywords, fields_config, synonym_map=None, use_phrase=True, use_fuzzy=True
):
    """
    ç»¼åˆå¤šç§æŠ€æœ¯çš„ä¼˜åŒ–æŸ¥è¯¢ï¼Œå¢å¼ºåŒä¹‰è¯çš„ä½¿ç”¨

    :param jieba_keywords: jiebaåº“æ‰€æå–çš„å…³é”®è¯
    :param fields_config: {'title': {'boost':5, 'fuzzy':False}, ...}
    :param synonym_map: åŒä¹‰è¯è¯å…¸ï¼Œæ ¼å¼: {'å…³é”®è¯': ['åŒä¹‰è¯1', 'åŒä¹‰è¯2']}
    """
    should_clauses = []

    for word in jieba_keywords:
        # è·å–å…³é”®è¯åŠå…¶åŒä¹‰è¯
        synonyms = synonym_map.get(word, [word]) if synonym_map else [word]

        for field, config in fields_config.items():
            boost = config.get("boost", 1)

            # 1. ä¸ºæ¯ä¸ªå…³é”®è¯åŠå…¶åŒä¹‰è¯æ„å»ºORæŸ¥è¯¢
            synonym_queries = []

            # ç²¾ç¡®åŒ¹é…ï¼ˆä½¿ç”¨termsæŸ¥è¯¢æ›¿ä»£å¤šä¸ªtermæŸ¥è¯¢ï¼‰
            if len(synonyms) > 0:
                synonym_queries.append(
                    {"terms": {f"{field}.keyword": synonyms, "boost": boost * 1.2}}
                )

            # æ¨¡ç³ŠåŒ¹é…
            if use_fuzzy and config.get("fuzzy", True):
                for syn in synonyms:
                    synonym_queries.append(
                        {
                            "match": {
                                field: {
                                    "query": syn,
                                    "fuzziness": "AUTO",
                                    "boost": boost * 0.5,
                                }
                            }
                        }
                    )

            # çŸ­è¯­åŒ¹é…
            if use_phrase and len(word) > 1:
                for syn in synonyms:
                    synonym_queries.append(
                        {
                            "match_phrase": {
                                field: {"query": syn, "slop": 2,
                                        "boost": boost * 0.8}
                            }
                        }
                    )

            # å°†æ‰€æœ‰åŒä¹‰è¯ç›¸å…³çš„æŸ¥è¯¢ç»„åˆåˆ°ä¸€ä¸ªboolæŸ¥è¯¢ä¸­
            if synonym_queries:
                should_clauses.append(
                    {"bool": {"should": synonym_queries, "minimum_should_match": 1}}
                )

    return {
        "query": {"bool": {"should": should_clauses, "minimum_should_match": "30%"}},
        "highlight": {
            "fields": {
                "*": {
                    "pre_tags": ["<em>"],
                    "post_tags": ["</em>"],
                }  # æ·»åŠ ç®€å•çš„é«˜äº®æ ‡ç­¾
            }
        },
    }


# ======================== æ£€ç´¢ç»“æœå»é‡å‡½æ•°ï¼ˆé€‚ç”¨äº Milvus/ESï¼‰ ========================
from datetime import datetime
from difflib import SequenceMatcher

from datetime import datetime
from difflib import SequenceMatcher

from datetime import datetime
from difflib import SequenceMatcher

def deduplicate_ranked_blocks(docs: list,
                              threshold_content=0.9,
                              threshold_page_name=0.6,
                              window: int = 3) -> list:
    """
    å¤šçª—å£æ»‘åŠ¨å»é‡é€»è¾‘ï¼ˆå¸¦è¯¦ç»†æ‰“å°ï¼‰ï¼š
    - è‹¥åç»­ window ä¸ªå—ä¸­å­˜åœ¨é‡å¤ï¼Œåˆ™ç”¨æ—¶é—´æ›´æ–°æœ€æ–°é¡¹ï¼Œç»§ç»­æ»‘åŠ¨æ¯”è¾ƒ
    - ç›´åˆ°æ— é‡å¤ï¼Œä¿ç•™è¯¥å—å¹¶ç»§ç»­ä¸‹ä¸€ä¸ª
    """
    def parse_time(t: str) -> datetime:
        try:
            return datetime.strptime(t, "%Y-%m-%d %H:%M:%S")
        except Exception:
            return datetime.min

    def str_sim(a: str, b: str) -> float:
        return SequenceMatcher(None, a, b).ratio()

    seen = set()
    keep = []
    i = 0

    while i < len(docs):
        if i in seen:
            i += 1
            continue

        base = docs[i]
        base_text = clean_text(base.get("text", ""))
        base_name = clean_text(base.get("page_name", ""))
        base_time = parse_time(base.get("time", ""))
        best_doc = base
        best_time = base_time

        # print(f"\nğŸŸ© å½“å‰åŸºå‡†å— i={i}ï¼š")
        # print(f"ğŸ”¹æ ‡é¢˜: {base.get('page_name', '')}")
        # print(f"ğŸ”¹æ—¶é—´: {base.get('time', '')}")
        # print(f"ğŸ”¹å†…å®¹å‰50å­—: {base.get('text', '')[:50]}")

        for j in range(i + 1, min(i + 1 + window, len(docs))):
            if j in seen:
                continue

            comp = docs[j]
            sim_text = str_sim(base_text, clean_text(comp.get("text", "")))
            sim_name = str_sim(base_name, clean_text(comp.get("page_name", "")))

            if sim_text >= threshold_content and sim_name >= threshold_page_name:
                comp_time = parse_time(comp.get("time", ""))
                seen.add(j)

                # print(f"\nâš ï¸ å‘ç°é‡å¤å— j={j}ï¼š")
                # print(f"   - æ ‡é¢˜ç›¸ä¼¼åº¦: {sim_name:.3f}ï¼Œå†…å®¹ç›¸ä¼¼åº¦: {sim_text:.3f}")
                # print(f"   - æ ‡é¢˜: {comp.get('page_name', '')}")
                # print(f"   - æ—¶é—´: {comp.get('time', '')}")
                # print(f"   - å†…å®¹å‰50å­—: {comp.get('text', '')[:50]}")

                if comp_time > best_time:
                    seen.add(i)
                    best_doc = comp
                    best_time = comp_time
                    # print("âœ… å½“å‰å—è¢«æ›¿æ¢ä¸ºè¾ƒæ–°çš„é‡å¤å—")

        keep.append(best_doc)
        i += 1

    print(f"\nâœ… å»é‡å®Œæˆï¼ŒåŸå§‹ {len(docs)} ä¸ªå—ï¼Œä¿ç•™ {len(keep)} ä¸ªå—\n")
    return keep


# ======================== æ–‡æ¡£å—åˆ†ç±»å‡½æ•° ========================
def infer_chunk_category(page_url):
    if any(k in page_url for k in ["è§„åˆ™", "åˆ¶åº¦", "æ³•å¾‹", "å®¡æ ¸"]):
        return "è§„åˆ™ç±»"
    elif any(k in page_url for k in ["ä½¿ç”¨", "æŒ‡å—", "å¸®åŠ©", "æ“ä½œ", "åŠŸèƒ½"]):
        return "æ“ä½œç±»"
    elif any(k in page_url for k in ["ç”Ÿæ€", "è§’è‰²", "ç­–ç•¥", "æ¨å¹¿", "å¹³å°ä¿¡æ¯"]):
        return "ä¿¡æ¯ç±»"
    else:
        return "æ³›ç”¨ç±»"


# ======================== ChatGLM æ‘˜è¦ç”Ÿæˆå‡½æ•° ========================
def generate_summary_ChatGLM(
    text,
    page_url,
    model,
    tokenizer,
    max_new_tokens=150,
):
    if len(text) < max_new_tokens * 2:
        print("âš ï¸ æ–‡æœ¬é•¿åº¦ä¸è¶³ï¼Œä½¿ç”¨åŸæ–‡æœ¬")
        return text[:max_new_tokens]

    category = infer_chunk_category(page_url)
    text = text.strip().replace("\x00", "")

    prompt = (
            f"ä½ æ­£åœ¨å¤„ç†ä¸€ç¯‡ç”µå•†å¹³å°çš„çŸ¥è¯†å†…å®¹ï¼Œå±äºâ€œ{category}â€ç±»ã€‚\n"
            f"è¯·ä½ æ ¹æ®ä¸‹æ–¹å†…å®¹æç‚¼å…¶ä¸»è¦ä¿¡æ¯ï¼Œè¦æ±‚å¦‚ä¸‹ï¼š\n"
            f"1. æ¦‚æ‹¬è¦ç‚¹ï¼Œä¸è¦é‡å¤åŸæ–‡åŸå¥ï¼›\n"
            f"2. æ€»é•¿åº¦ä¸è¶…è¿‡{max_new_tokens}å­—ï¼Œä½¿ç”¨ç®€ä½“ä¸­æ–‡ï¼›\n"
            f"3. è¾“å‡ºæ ¼å¼ä¸ºå®Œæ•´ä¸€å¥è¯ã€‚\n"
            f"ğŸ“‚ æ¥æºè·¯å¾„ï¼š{page_url}\n"
            f"ğŸ“„ å†…å®¹ï¼š\n{text}"
        )

    try:
        messages = [{"role": "user", "content": prompt}]
        inputs = tokenizer.apply_chat_template(
            messages,
            add_generation_prompt=True,
            return_tensors="pt",
            return_dict=True,
        ).to(model.device)

        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                do_sample=True,
                top_p=0.8,
                temperature=0.4
            )
        # è£å‰ªæ‰ prompt éƒ¨åˆ†
        generated_ids = outputs[:, inputs["input_ids"].shape[1]:]
        response = tokenizer.decode(generated_ids[0], skip_special_tokens=True).strip()
        return response if response else text[:max_new_tokens]
    except Exception as e:
        print(f"âš ï¸ ChatGLM æ‘˜è¦ç”Ÿæˆå¤±è´¥: {e}ï¼Œä½¿ç”¨ fallback")
        return text[:max_new_tokens]



# ======================== ChatGLM é—®é¢˜ç”Ÿæˆå‡½æ•° ========================
def generate_question_ChatGLM(
    text,
    page_url,
    model,
    tokenizer,
    max_new_tokens=64,
    fallback_question="è¯¥å†…å®¹å¯æ„é€ ç›¸å…³ä¸šåŠ¡é—®é¢˜"
):

    category = infer_chunk_category(page_url)
    text = text.strip().replace("\x00", "")

    if category == "è§„åˆ™ç±»":
        hint = "å¹³å°æ˜¯å¦å…è®¸ã€è§„åˆ™çº¦æŸã€è¿è§„å¤„ç†"
    elif category == "æ“ä½œç±»":
        hint = "å¦‚ä½•æ“ä½œã€æ˜¯å¦å¯ç”¨ã€ä½¿ç”¨æ–¹æ³•"
    elif category == "ä¿¡æ¯ç±»":
        hint = "å¹³å°èƒŒæ™¯ã€äº§å“å®šä½ã€ç­–ç•¥è®¾è®¡"
    else:
        hint = "ç”¨æˆ·å®é™…å¯èƒ½ä¼šé—®çš„é—®é¢˜"

    prompt = (
        f"ä½ æ˜¯ä¸€ä¸ªç”µå•†å¹³å°çŸ¥è¯†é—®ç­”æ„å»ºåŠ©æ‰‹ï¼Œè¯·æ ¹æ®ä»¥ä¸‹å†…å®¹ç”Ÿæˆä¸€ä¸ªæœ‰å®é™…ä»·å€¼çš„ç”¨æˆ·é—®é¢˜ã€‚\n"
        f"è¦æ±‚ï¼š\n"
        f"- é—®é¢˜åº”ä½“ç°â€œ{hint}â€ï¼›\n"
        f"- ç¦æ­¢å¤è¿°åŸæ–‡ï¼Œåº”æç‚¼æ“ä½œã€åˆ¤æ–­æˆ–å’¨è¯¢ç‚¹ï¼›\n"
        f"- åªè¾“å‡ºä¸€ä¸ªç®€ä½“ä¸­æ–‡é—®é¢˜å¥ï¼Œä¸åŠ è¯´æ˜ã€‚\n"
        f"ğŸ“‚ æ¥æºè·¯å¾„ï¼š{page_url}\n"
        f"ğŸ“„ å†…å®¹ï¼š\n{text}"
    )

    try:
        messages = [{"role": "user", "content": prompt}]
        inputs = tokenizer.apply_chat_template(
            messages,
            add_generation_prompt=True,
            return_tensors="pt",
            return_dict=True,
        ).to(model.device)

        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                do_sample=True,
                top_p=0.9,
                temperature=0.7
            )
        generated_ids = outputs[:, inputs["input_ids"].shape[1]:]
        response = tokenizer.decode(generated_ids[0], skip_special_tokens=True).strip()
        return response if response else fallback_question
    except Exception as e:
        print(f"âš ï¸ ChatGLM é—®é¢˜ç”Ÿæˆå¤±è´¥: {e}ï¼Œä½¿ç”¨ fallback")
        return fallback_question




# ======================== æ–‡æ¡£å—ç”Ÿæˆå‡½æ•° ========================
def _generate_block_documents(
    block_tree,
    max_node_words,
    page_url="unknown.html",
    summary_model=None,
    summary_tokenizer=None,
    time_value=""
):
    """
    ç”Ÿæˆæ–‡æ¡£å—çš„å…ƒä¿¡æ¯åˆ—è¡¨ï¼ˆdoc_metaï¼‰ï¼Œç”¨äºä¿å­˜ä¸º JSON æˆ–å…¥åº“ã€‚
    æ‰“å°å¤„ç†è¿›åº¦å’Œå†…å®¹æ‘˜è¦ä¿¡æ¯ã€‚
    """
    from utils.text_process_utils import extract_title_from_block, clean_invisible

    path_tags = [b[0] for b in block_tree]
    doc_meta = []

    print(f"ğŸ“¦ å…±æå–å—æ•°ï¼š{len(path_tags)}")

    for pidx, tag in enumerate(path_tags):
        print(f"\nğŸ§© æ­£åœ¨å¤„ç†ç¬¬ {pidx+1}/{len(path_tags)} ä¸ª block")
        text = tag.get_text().strip().replace("\x00", "")
        text = clean_invisible(text)
        if not text:
            print("âš ï¸ ç©ºå†…å®¹ï¼Œè·³è¿‡")
            continue

        preview = text[:80].replace('\n', ' ') + ("..." if len(text) > 80 else "")
        print(f"ğŸ“„ æ–‡æœ¬é¢„è§ˆï¼š{preview}")

        title = extract_title_from_block(tag)
        print(f"ğŸ·ï¸ æå–æ ‡é¢˜ï¼š{title[:128]}")

        page_name = os.path.splitext(os.path.basename(page_url))[0]
        summary = ""

        if summary_model and summary_tokenizer:
            summary = generate_summary_ChatGLM(text, page_url, summary_model, summary_tokenizer)
            print(f"âœ… æ‘˜è¦ç”ŸæˆæˆåŠŸï¼š{summary}")
            question = generate_question_ChatGLM(text, page_url, summary_model, summary_tokenizer)
            print(f"âœ… é—®é¢˜ç”ŸæˆæˆåŠŸï¼š{question}")

        doc_meta.append({
            "chunk_idx": pidx,
            "page_name": page_name,
            "title": title[:128],
            "page_url": page_url,
            "summary": summary,
            "question": question,
            "text": text,
            "time": time_value,
        })

    print(f"\nâœ… æ‰€æœ‰å—å¤„ç†å®Œæ¯•ï¼Œå…±ç”Ÿæˆ {len(doc_meta)} æ¡æœ‰æ•ˆæ–‡æ¡£å—")
    return doc_meta


def generate_block_documents(
    block_tree,
    max_node_words,
    page_url="unknown.html",
    summary_model=None,
    summary_tokenizer=None,
    time_value=""
):
    """
    ç”Ÿæˆç»“æ„åŒ–æ–‡æ¡£å—ï¼Œæ”¯æŒè¡¨æ ¼è‡ªåŠ¨åˆ‡åˆ†ï¼Œç»Ÿä¸€ç”Ÿæˆ summary å’Œ questionã€‚
    """
    from utils.text_process_utils import extract_title_from_block, clean_invisible
    import os

    path_tags = [b[0] for b in block_tree]
    doc_meta = []
    chunk_idx = 0

    print(f"ğŸ“¦ å…±æå–å—æ•°ï¼š{len(path_tags)}")

    for pidx, tag in enumerate(path_tags):
        print(f"\nğŸ§© æ­£åœ¨å¤„ç†ç¬¬ {pidx+1}/{len(path_tags)} ä¸ª block")

        page_name = os.path.splitext(os.path.basename(page_url))[0]
        title = extract_title_from_block(tag)
        print(f"ğŸ·ï¸ æå–æ ‡é¢˜ï¼š{title[:128]}")

        is_table_block = (tag.name == "table") or tag.find("table") is not None

        if is_table_block:
            print("ğŸ“Š è¡¨æ ¼ç±»å‹ï¼Œæ‰§è¡ŒæŒ‰è¡Œæ‹¼æ¥åˆ‡åˆ†")
            table = tag.find("table") if tag.name != "table" else tag
            rows = table.find_all("tr")
            print(f"ğŸ“Š è¡¨æ ¼è¡Œæ•°ï¼š{len(rows)}")
            if not rows:
                continue

            def row_to_text(row):
                return " ".join(cell.strip() for cell in row.stripped_strings) + "\n"

            header_text = row_to_text(rows[0])
            current_text = header_text
            current_words = len(re.findall(r"[\u4e00-\u9fa5a-zA-Z0-9]", header_text))
            start_row = 1  # header æ˜¯ç¬¬1è¡Œ
            row_range_start = 1

            for idx, row in enumerate(rows[1:], start=2):
                row_text = row_to_text(row)
                row_words = len(re.findall(r"[\u4e00-\u9fa5a-zA-Z0-9]", row_text))

                if current_words + row_words > max_node_words:
                    # âœ… æäº¤å½“å‰å—
                    text = clean_invisible(current_text.strip())
                    if text:
                        summary, question = "", ""
                        if summary_model and summary_tokenizer:
                            summary = generate_summary_ChatGLM(text, page_url, summary_model, summary_tokenizer)
                            question = generate_question_ChatGLM(text, page_url, summary_model, summary_tokenizer)

                        title_with_range = f"{title[:96]} è¡¨æ ¼è¡Œ{row_range_start}-{idx-1}"
                        doc_meta.append({
                            "chunk_idx": chunk_idx,
                            "page_name": page_name,
                            "title": title_with_range,
                            "page_url": page_url,
                            "summary": summary,
                            "question": question,
                            "text": text,
                            "time": time_value,
                        })
                        chunk_idx += 1

                    # âœ… é‡ç½®
                    current_text = header_text + row_text
                    current_words = len(re.findall(r"[\u4e00-\u9fa5a-zA-Z0-9]", current_text))
                    row_range_start = idx
                else:
                    current_text += row_text
                    current_words += row_words

            # âœ… æäº¤æœ€åä¸€ä¸ªå—
            text = clean_invisible(current_text.strip())
            if text:
                summary, question = "", ""
                if summary_model and summary_tokenizer:
                    summary = generate_summary_ChatGLM(text, page_url, summary_model, summary_tokenizer)
                    question = generate_question_ChatGLM(text, page_url, summary_model, summary_tokenizer)

                title_with_range = f"{title[:96]} è¡¨æ ¼è¡Œ{row_range_start}-{len(rows)}"
                doc_meta.append({
                    "chunk_idx": chunk_idx,
                    "page_name": page_name,
                    "title": title_with_range,
                    "page_url": page_url,
                    "summary": summary,
                    "question": question,
                    "text": text,
                    "time": time_value,
                })
                chunk_idx += 1

        else:
            # text = tag.get_text().strip().replace("\x00", "")
            text = tag.get_text().replace("\x00", "")  # ä¸ strip()ï¼Œä¿ç•™æ¢è¡Œ

            text = clean_invisible(text)
            if not text:
                print("âš ï¸ ç©ºå†…å®¹ï¼Œè·³è¿‡")
                continue

            preview = text[:80].replace('\n', ' ') + ("..." if len(text) > 80 else "")
            print(f"ğŸ“„ æ–‡æœ¬é¢„è§ˆï¼š{preview}")

            summary, question = "", ""
            if summary_model and summary_tokenizer:
                summary = generate_summary_ChatGLM(text, page_url, summary_model, summary_tokenizer)
                question = generate_question_ChatGLM(text, page_url, summary_model, summary_tokenizer)

            doc_meta.append({
                "chunk_idx": chunk_idx,
                "page_name": page_name,
                "title": title[:128],
                "page_url": page_url,
                "summary": summary,
                "question": question,
                "text": text,
                "time": time_value,
            })
            chunk_idx += 1

    print(f"\nâœ… æ‰€æœ‰å—å¤„ç†å®Œæ¯•ï¼Œå…±ç”Ÿæˆ {len(doc_meta)} æ¡æœ‰æ•ˆæ–‡æ¡£å—")
    return doc_meta



def save_doc_meta_to_block_dir(doc_meta, html_path, html_root_dir, block_root_dir):
    """
    ä¿å­˜ JSON æ–‡ä»¶ï¼Œè·¯å¾„æ˜ å°„ï¼š
    html_path = a/b/c.html â†’ ä¿å­˜ä¸º a_blocks/b/c.json
    """
    # ç›¸å¯¹è·¯å¾„ï¼šb/c.html
    rel_path = os.path.relpath(html_path, html_root_dir)

    # è¾“å‡ºè·¯å¾„ï¼ša_blocks/b/c.json
    rel_json_path = os.path.splitext(rel_path)[0] + ".json"
    json_full_path = os.path.join(block_root_dir, rel_json_path)

    # åˆ›å»ºç›®æ ‡ç›®å½•
    os.makedirs(os.path.dirname(json_full_path), exist_ok=True)

    # å†™å…¥ JSON æ–‡ä»¶
    with open(json_full_path, "w", encoding="utf-8") as f:
        json.dump(doc_meta, f, ensure_ascii=False, indent=2)

    print(f"âœ… JSON å·²ä¿å­˜ï¼š{json_full_path}")
    return json_full_path


