
# -*- coding: utf-8 -*-
"""
æ–‡æœ¬å¤„ç†ä¸æ–‡æ¡£å—ç”Ÿæˆæ¨¡å—ï¼ˆç”¨äº HTML-RAG çŸ¥è¯†åº“æ„å»ºï¼‰

æœ¬æ¨¡å—è´Ÿè´£ä» HTML æ¸…æ´—ç»“æœä¸­æå–ç»“æ„åŒ–æ–‡æ¡£å—ï¼Œç”Ÿæˆæ‘˜è¦ä¸é—®å¥ï¼Œå¹¶æä¾›æŸ¥è¯¢æ„å»ºã€æ–‡æœ¬æ¸…æ´—ä¸å»é‡èƒ½åŠ›ï¼Œ
æ˜¯æ™ºèƒ½å®¢æœ/æ–‡æ¡£é—®ç­”ç­‰ RAG ç³»ç»Ÿä¸­â€œåˆ‡å—-æ‘˜è¦-å‘é‡åŒ–â€æµç¨‹çš„æ ¸å¿ƒç»„ä»¶ã€‚

æ ¸å¿ƒåŠŸèƒ½ç»“æ„ï¼š
------------------------------------------------
1. æ–‡æœ¬æ¸…æ´—ä¸åˆ†è¯ï¼š
   - `clean_text`: ç§»é™¤æ‰€æœ‰éä¸­è‹±æ–‡ä¸æ•°å­—å­—ç¬¦ã€‚
   - `jieba_cut_clean`: æ¸…æ´—åè¿›è¡Œè‡ªå®šä¹‰åˆ†è¯ï¼ˆæ”¯æŒå¤–éƒ¨ user_dictï¼‰ã€‚
   - `clean_invisible`: æ¸…é™¤é›¶å®½ä¸æ§åˆ¶ç±»ä¸å¯è§å­—ç¬¦ã€‚

2. è¯­ä¹‰å—æ ‡é¢˜æå–ï¼š
   - `extract_title_from_block`: æå– HTML å—ä¸­çš„ç¬¬ä¸€ä¸ªæ ‡é¢˜æˆ–éç©ºæ–‡æœ¬ä½œä¸º chunk titleã€‚

3. æŸ¥è¯¢æ„å»ºï¼ˆç”¨äº ES å€’æ’æ£€ç´¢ï¼‰ï¼š
   - `build_optimal_jieba_query`: ç»¼åˆç²¾ç¡®åŒ¹é…ã€æ¨¡ç³ŠæŸ¥è¯¢ã€çŸ­è¯­åŒ¹é…ä¸åŒä¹‰è¯æ‰©å±•æ„å»ºç»“æ„åŒ– bool æŸ¥è¯¢ã€‚

4. ç›¸ä¼¼å†…å®¹å»é‡ï¼š
   - `deduplicate_ranked_blocks_pal`: åŸºäº TF-IDF å’Œ cosine ç›¸ä¼¼åº¦è®¡ç®—æ–‡æœ¬å’Œé¡µé¢åçš„ç›¸ä¼¼æ€§ï¼ŒæŒ‰æ—¶é—´ä¼˜å…ˆä¿ç•™æœ€ä¼˜ç‰ˆæœ¬ã€‚

5. æ–‡æ¡£å—ç”Ÿæˆï¼š
   - `generate_block_documents`: å°†ç»“æ„åŒ– HTML èŠ‚ç‚¹ç”Ÿæˆå¸¦ metadata çš„ chunk åˆ—è¡¨ï¼Œæ”¯æŒè¡¨æ ¼è¡Œåˆ‡åˆ†ã€æ‘˜è¦ç”Ÿæˆã€é—®å¥æ„é€ ã€‚
     - æ”¯æŒæ‘˜è¦ç”Ÿæˆæ–¹å¼ï¼š
       - `generate_summary_ChatGLM`ï¼ˆè°ƒç”¨ ChatGLM æ¥å£ï¼‰
       - `generate_summary_vllm`ï¼ˆä½¿ç”¨ vLLM HTTP æ¥å£ï¼‰
     - å¯é€‰ç”Ÿæˆé—®å¥ `generate_question_ChatGLM`

6. å—æ•°æ®æŒä¹…åŒ–ï¼š
   - `save_doc_meta_to_block_dir`: å°† HTML å—çš„ç»“æ„åŒ–ä¿¡æ¯ä»¥ JSON æ ¼å¼å†™å…¥æŒ‡å®šè·¯å¾„ï¼Œè·¯å¾„ç»“æ„ä¸åŸå§‹ HTML ä¿æŒä¸€è‡´ã€‚

   """

import os
import re
import jieba
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import json
from datetime import datetime
import torch
from difflib import SequenceMatcher
from utils.llm_api import generate_summary_ChatGLM, generate_question_ChatGLM, generate_summary_vllm, generate_summary_vllm_async
import numpy as np
from collections import defaultdict
import aiohttp
import asyncio
import time


# å…³é—­å¹¶è¡ŒåŒ–è­¦å‘Šï¼Œé¿å…æ§åˆ¶å°å†—ä½™ä¿¡æ¯
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# åˆå§‹åŒ–è‡ªå®šä¹‰åˆ†è¯å™¨ï¼Œä¸ä½¿ç”¨å…¨å±€å½±å“çš„ jieba åŠ è½½è¯å…¸
pure_tokenizer = jieba.Tokenizer(dictionary=jieba.DEFAULT_DICT)
jieba.load_userdict("./user_dict.txt")


# ======================== æ–‡æœ¬å¤„ç†å·¥å…·å‡½æ•° ========================

def clean_text(text: str) -> str:
    """ç§»é™¤æ–‡æœ¬ä¸­çš„ç‰¹æ®Šå­—ç¬¦ï¼Œä»…ä¿ç•™ä¸­è‹±æ–‡ä¸æ•°å­—"""
    return "".join(re.findall(r"[\u4e00-\u9fa5a-zA-Z0-9]+", text))


def jieba_cut_clean(text: str) -> list:
    """ç»“åˆ clean_text ä¸è‡ªå®šä¹‰åˆ†è¯å™¨è¿›è¡Œåˆ†è¯å¤„ç†"""
    text = clean_text(text)
    return list(pure_tokenizer.cut(text, HMM=False))


def clean_invisible(text):
    # å»é™¤æ‰€æœ‰ Unicode æ§åˆ¶å­—ç¬¦
    return re.sub(r'[\u200b-\u200f\u202a-\u202e\u2060-\u206f]', '', text)

# ======================== æ–‡æ¡£å—æ ‡é¢˜æå–å‡½æ•° ========================

def extract_title_from_block(tag) -> str:
    """
    ä» HTML tag ä¸­æå–ç¬¬ä¸€ä¸ª heading æ ‡ç­¾ï¼ˆh1~h6ï¼‰ä½œä¸ºæ ‡é¢˜
    è‹¥ä¸å­˜åœ¨ï¼Œåˆ™å›é€€ä¸ºç¬¬ä¸€ä¸ªéç©ºæ–‡æœ¬
    """
    from bs4 import Tag

    for descendant in tag.descendants:
        if isinstance(descendant, Tag) and descendant.name and descendant.name.lower().startswith("h"):
            return descendant.get_text(separator="", strip=True)[:48]

    for t in tag.stripped_strings:
        if t.strip():
            return t.strip()[:48]
    return ""



# ======================== ä¼˜åŒ–çš„ Jieba æŸ¥è¯¢æ„å»ºå‡½æ•° ========================
def build_optimal_jieba_query(
    jieba_keywords, fields_config, synonym_map=None, use_phrase=True, use_fuzzy=True
):
    """
    ç»¼åˆå¤šç§æŠ€æœ¯çš„ä¼˜åŒ–æŸ¥è¯¢ï¼Œå¢å¼ºåŒä¹‰è¯çš„ä½¿ç”¨

    :param jieba_keywords: jiebaåº“æ‰€æå–çš„å…³é”®è¯
    :param fields_config: {'title': {'boost':5, 'fuzzy':False}, ...}
    :param synonym_map: åŒä¹‰è¯è¯å…¸ï¼Œæ ¼å¼: {'å…³é”®è¯': ['åŒä¹‰è¯1', 'åŒä¹‰è¯2']}
    """
    should_clauses = []

    for word in jieba_keywords:
        # è·å–å…³é”®è¯åŠå…¶åŒä¹‰è¯
        synonyms = synonym_map.get(word, [word]) if synonym_map else [word]

        for field, config in fields_config.items():
            boost = config.get("boost", 1)

            # 1. ä¸ºæ¯ä¸ªå…³é”®è¯åŠå…¶åŒä¹‰è¯æ„å»ºORæŸ¥è¯¢
            synonym_queries = []

            # ç²¾ç¡®åŒ¹é…ï¼ˆä½¿ç”¨termsæŸ¥è¯¢æ›¿ä»£å¤šä¸ªtermæŸ¥è¯¢ï¼‰
            if len(synonyms) > 0:
                synonym_queries.append(
                    {"terms": {f"{field}.keyword": synonyms, "boost": boost * 1.2}}
                )

            # æ¨¡ç³ŠåŒ¹é…
            if use_fuzzy and config.get("fuzzy", True):
                for syn in synonyms:
                    synonym_queries.append(
                        {
                            "match": {
                                field: {
                                    "query": syn,
                                    "fuzziness": "AUTO",
                                    "boost": boost * 0.5,
                                }
                            }
                        }
                    )

            # çŸ­è¯­åŒ¹é…
            if use_phrase and len(word) > 1:
                for syn in synonyms:
                    synonym_queries.append(
                        {
                            "match_phrase": {
                                field: {"query": syn, "slop": 2,
                                        "boost": boost * 0.8}
                            }
                        }
                    )

            # å°†æ‰€æœ‰åŒä¹‰è¯ç›¸å…³çš„æŸ¥è¯¢ç»„åˆåˆ°ä¸€ä¸ªboolæŸ¥è¯¢ä¸­
            if synonym_queries:
                should_clauses.append(
                    {"bool": {"should": synonym_queries, "minimum_should_match": 1}}
                )

    return {
        "query": {"bool": {"should": should_clauses, "minimum_should_match": "30%"}},
        "highlight": {
            "fields": {
                "*": {
                    "pre_tags": ["<em>"],
                    "post_tags": ["</em>"],
                }  # æ·»åŠ ç®€å•çš„é«˜äº®æ ‡ç­¾
            }
        },
    }


# ======================== æ£€ç´¢ç»“æœå»é‡å‡½æ•°ï¼ˆé€‚ç”¨äº Milvus/ESï¼‰ ========================
def parse_time(t: str) -> datetime:
    try:
        return datetime.strptime(t, "%Y-%m-%d %H:%M:%S")
    except Exception:
        return datetime.min

def str_sim(a: str, b: str) -> float:
    return SequenceMatcher(None, a, b).ratio()

def clean_text(text: str) -> str:
    """ç§»é™¤æ–‡æœ¬ä¸­çš„ç‰¹æ®Šå­—ç¬¦ï¼Œä»…ä¿ç•™ä¸­è‹±æ–‡ä¸æ•°å­—"""
    return "".join(re.findall(r"[\u4e00-\u9fa5a-zA-Z0-9]+", text))


def deduplicate_ranked_blocks_pal(docs, threshold_content=0.9, threshold_page_name=0.6):
    n = len(docs)
    if n <= 1:
        return docs

    texts = [clean_text(doc.get("text", "")) for doc in docs]
    names = [clean_text(doc.get("page_name", "")) for doc in docs]
    times = np.array([parse_time(doc.get("time", "")) for doc in docs])

    tfidf = TfidfVectorizer().fit(texts + names)
    text_vecs = tfidf.transform(texts)
    name_vecs = tfidf.transform(names)

    sim_text = cosine_similarity(text_vecs)
    sim_name = cosine_similarity(name_vecs)

    # ä¸Šä¸‰è§’é‡å¤å¯¹
    triu_idx = np.triu_indices(n, k=1)
    sim_mask = (sim_text[triu_idx] >= threshold_content) & (sim_name[triu_idx] >= threshold_page_name)
    dup_pairs = list(zip(triu_idx[0][sim_mask], triu_idx[1][sim_mask]))

    # æ„å»ºé‡å¤ç°‡ï¼šç”¨å›¾è¡¨ç¤º
    graph = defaultdict(set)
    for i, j in dup_pairs:
        graph[i].add(j)
        graph[j].add(i)

    visited = set()
    keep = set()

    def dfs(node, group):
        visited.add(node)
        group.append(node)
        for neighbor in graph[node]:
            if neighbor not in visited:
                dfs(neighbor, group)

    for i in range(n):
        if i not in visited:
            group = []
            dfs(i, group)
            if len(group) == 1:
                keep.add(group[0])
            else:
                latest = max(group, key=lambda x: times[x])
                keep.add(latest)

    kept = sorted(list(keep))
    print(f"âœ… åŸå§‹ {n} ä¸ªå—ï¼Œé‡å¤å¯¹ {len(dup_pairs)}ï¼Œå»é‡åä¿ç•™ {len(kept)}")
    return [docs[i] for i in kept]

def save_doc_meta_to_block_dir(doc_meta, html_path, html_root_dir, block_root_dir):
    """
    ä¿å­˜ JSON æ–‡ä»¶ï¼Œè·¯å¾„æ˜ å°„ï¼š
    html_path = a/b/c.html â†’ ä¿å­˜ä¸º a_blocks/b/c.json
    """
    # ç›¸å¯¹è·¯å¾„ï¼šb/c.html
    rel_path = os.path.relpath(html_path, html_root_dir)

    # è¾“å‡ºè·¯å¾„ï¼ša_blocks/b/c.json
    rel_json_path = os.path.splitext(rel_path)[0] + ".json"
    json_full_path = os.path.join(block_root_dir, rel_json_path)

    # åˆ›å»ºç›®æ ‡ç›®å½•
    os.makedirs(os.path.dirname(json_full_path), exist_ok=True)

    # å†™å…¥ JSON æ–‡ä»¶
    with open(json_full_path, "w", encoding="utf-8") as f:
        json.dump(doc_meta, f, ensure_ascii=False, indent=2)

    print(f"âœ… JSON å·²ä¿å­˜ï¼š{json_full_path}")
    return json_full_path




# ======================== æ–‡æ¡£å—ç”Ÿæˆå‡½æ•° ========================
def generate_block_documents(
    block_tree,
    max_node_words,
    page_url="unknown.html",
    summary_model=None,
    summary_tokenizer=None,
    time_value="",
    gen_question=False,
    use_vllm=True,
):
    """
    ç”Ÿæˆç»“æ„åŒ–æ–‡æ¡£å—ï¼Œæ”¯æŒè¡¨æ ¼è‡ªåŠ¨åˆ‡åˆ†ï¼Œç»Ÿä¸€ç”Ÿæˆ summary å’Œ questionã€‚
    """

    path_tags = [b[0] for b in block_tree]
    doc_meta = []
    chunk_idx = 0

    print(f"ğŸ“¦ å…±æå–å—æ•°ï¼š{len(path_tags)}")

    for pidx, tag in enumerate(path_tags):
        print(f"\nğŸ§© æ­£åœ¨å¤„ç†ç¬¬ {pidx+1}/{len(path_tags)} ä¸ª block")

        page_name = os.path.splitext(os.path.basename(page_url))[0]
        title = extract_title_from_block(tag)
        print(f"ğŸ·ï¸ æå–æ ‡é¢˜ï¼š{title[:128]}")

        is_table_block = (tag.name == "table") or tag.find("table") is not None

        if is_table_block:
            print("ğŸ“Š è¡¨æ ¼ç±»å‹ï¼Œæ‰§è¡ŒæŒ‰è¡Œæ‹¼æ¥åˆ‡åˆ†")
            table = tag.find("table") if tag.name != "table" else tag
            rows = table.find_all("tr")
            print(f"ğŸ“Š è¡¨æ ¼è¡Œæ•°ï¼š{len(rows)}")
            if not rows:
                continue

            def row_to_text(row):
                return " ".join(cell.strip() for cell in row.stripped_strings) + "\n"

            header_text = row_to_text(rows[0])
            current_text = header_text
            current_words = len(re.findall(r"[\u4e00-\u9fa5a-zA-Z0-9]", header_text))
            start_row = 1  # header æ˜¯ç¬¬1è¡Œ
            row_range_start = 1

            for idx, row in enumerate(rows[1:], start=2):
                row_text = row_to_text(row)
                row_words = len(re.findall(r"[\u4e00-\u9fa5a-zA-Z0-9]", row_text))

                if current_words + row_words > max_node_words:
                    # âœ… æäº¤å½“å‰å—
                    text = clean_invisible(current_text.strip())
                    if text:
                        summary, question = "", ""
                        if use_vllm:
                            generate_summary_vllm(text, page_url)
                        elif summary_model and summary_tokenizer:
                            summary = generate_summary_ChatGLM(text, page_url, summary_model, summary_tokenizer)
                            if gen_question:
                                question = generate_question_ChatGLM(text, page_url, summary_model, summary_tokenizer)

                        title_with_range = f"{title[:96]} è¡¨æ ¼è¡Œ{row_range_start}-{idx-1}"
                        doc_meta.append({
                            "chunk_idx": chunk_idx,
                            "page_name": page_name,
                            "title": title_with_range,
                            "page_url": page_url,
                            "summary": summary,
                            "question": question,
                            "text": text,
                            "time": time_value,
                        })
                        chunk_idx += 1

                    # âœ… é‡ç½®
                    current_text = header_text + row_text
                    current_words = len(re.findall(r"[\u4e00-\u9fa5a-zA-Z0-9]", current_text))
                    row_range_start = idx
                else:
                    current_text += row_text
                    current_words += row_words

            # âœ… æäº¤æœ€åä¸€ä¸ªå—
            text = clean_invisible(current_text.strip())
            if text:
                summary, question = "", ""
                if use_vllm:
                    generate_summary_vllm(text, page_url)
                elif summary_model and summary_tokenizer:
                    summary = generate_summary_ChatGLM(text, page_url, summary_model, summary_tokenizer)
                    if gen_question:
                        question = generate_question_ChatGLM(text, page_url, summary_model, summary_tokenizer)
                title_with_range = f"{title[:96]} è¡¨æ ¼è¡Œ{row_range_start}-{len(rows)}"
                doc_meta.append({
                    "chunk_idx": chunk_idx,
                    "page_name": page_name,
                    "title": title_with_range,
                    "page_url": page_url,
                    "summary": summary,
                    "question": question,
                    "text": text,
                    "time": time_value,
                })
                chunk_idx += 1

        else:
            # text = tag.get_text().strip().replace("\x00", "")
            text = tag.get_text().replace("\x00", "")  # ä¸ strip()ï¼Œä¿ç•™æ¢è¡Œ

            text = clean_invisible(text)
            if not text:
                print("âš ï¸ ç©ºå†…å®¹ï¼Œè·³è¿‡")
                continue

            preview = text[:80].replace('\n', ' ') + ("..." if len(text) > 80 else "")
            print(f"ğŸ“„ æ–‡æœ¬é¢„è§ˆï¼š{preview}")

            summary, question = "", ""
            if use_vllm:
                generate_summary_vllm(text, page_url)
            elif summary_model and summary_tokenizer:
                summary = generate_summary_ChatGLM(text, page_url, summary_model, summary_tokenizer)
                if gen_question:
                    question = generate_question_ChatGLM(text, page_url, summary_model, summary_tokenizer)
            doc_meta.append({
                "chunk_idx": chunk_idx,
                "page_name": page_name,
                "title": title[:128],
                "page_url": page_url,
                "summary": summary,
                "question": question,
                "text": text,
                "time": time_value,
            })
            chunk_idx += 1

    print(f"\nâœ… æ‰€æœ‰å—å¤„ç†å®Œæ¯•ï¼Œå…±ç”Ÿæˆ {len(doc_meta)} æ¡æœ‰æ•ˆæ–‡æ¡£å—")
    return doc_meta



async def generate_block_documents_async(
    block_tree,
    max_node_words,
    page_url="unknown.html",
    summary_model=None,
    summary_tokenizer=None,
    time_value="",
    gen_question=False,
    use_vllm=True,
    batch_size=32
):
    path_tags = [b[0] for b in block_tree]
    doc_meta, chunk_idx, tasks = [], 0, []
    page_name = os.path.splitext(os.path.basename(page_url))[0]

    def row_to_text(row):
        return " ".join(cell.strip() for cell in row.stripped_strings) + "\n"

    for tag in path_tags:
        title = extract_title_from_block(tag)
        is_table_block = (tag.name == "table") or tag.find("table") is not None

        if is_table_block:
            table = tag.find("table") if tag.name != "table" else tag
            rows = table.find_all("tr")
            if not rows:
                continue

            header_text = row_to_text(rows[0])
            current_text = header_text
            current_words = len(re.findall(r"[\u4e00-\u9fa5a-zA-Z0-9]", header_text))
            row_range_start = 1

            for idx, row in enumerate(rows[1:], start=2):
                row_text = row_to_text(row)
                row_words = len(re.findall(r"[\u4e00-\u9fa5a-zA-Z0-9]", row_text))

                if current_words + row_words > max_node_words:
                    text = clean_invisible(current_text.strip())
                    if text:
                        doc_meta.append({
                            "chunk_idx": chunk_idx,
                            "page_name": page_name,
                            "title": f"{title[:96]} è¡¨æ ¼è¡Œ{row_range_start}-{idx-1}",
                            "page_url": page_url,
                            "summary": "",
                            "question": "",
                            "text": text,
                            "time": time_value,
                        })
                        tasks.append((chunk_idx, text, page_url))
                        chunk_idx += 1
                    current_text = header_text + row_text
                    current_words = len(re.findall(r"[\u4e00-\u9fa5a-zA-Z0-9]", current_text))
                    row_range_start = idx
                else:
                    current_text += row_text
                    current_words += row_words

            text = clean_invisible(current_text.strip())
            if text:
                doc_meta.append({
                    "chunk_idx": chunk_idx,
                    "page_name": page_name,
                    "title": f"{title[:96]} è¡¨æ ¼è¡Œ{row_range_start}-{len(rows)}",
                    "page_url": page_url,
                    "summary": "",
                    "question": "",
                    "text": text,
                    "time": time_value,
                })
                tasks.append((chunk_idx, text, page_url))
                chunk_idx += 1

        else:
            text = clean_invisible(tag.get_text().replace("\x00", ""))
            if not text:
                continue
            doc_meta.append({
                "chunk_idx": chunk_idx,
                "page_name": page_name,
                "title": title[:128],
                "page_url": page_url,
                "summary": "",
                "question": "",
                "text": text,
                "time": time_value,
            })
            tasks.append((chunk_idx, text, page_url))
            chunk_idx += 1

    print(f"\nğŸš€ å¼€å§‹åˆ†æ‰¹å¹¶å‘ç”Ÿæˆ {len(tasks)} ä¸ªæ‘˜è¦ ...")
    start = time.time()

    for i in range(0, len(tasks), batch_size):
        batch = tasks[i:i + batch_size]
        summaries = await asyncio.gather(*[
            generate_summary_vllm_async(text, url) for _, text, url in batch
        ])
        for j, (chunk_idx_i, _, _) in enumerate(batch):
            doc_meta[chunk_idx_i]["summary"] = summaries[j]

    print(f"âœ… æ‘˜è¦ç”Ÿæˆå®Œæˆï¼ˆè€—æ—¶ {time.time() - start:.2f}sï¼‰")
    return doc_meta

