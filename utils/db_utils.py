# -*- coding: utf-8 -*-
"""
HTML ÊñáÊ°£Á¥¢Âºï‰∏éÂ§öÊ®°ÊÄÅÊ£ÄÁ¥¢Ê†∏ÂøÉÊ®°Âùó

Êú¨Ê®°ÂùóÊèê‰æõÈù¢ÂêëÁü•ËØÜÂûãÈóÆÁ≠îÁ≥ªÁªüÔºàÂ¶Ç RAGÔºâÁöÑ‰∏Ä‰ΩìÂåñÂêëÈáèÁ¥¢ÂºïÁÆ°ÁêÜ‰∏éÊü•ËØ¢ËÉΩÂäõÔºåÊîØÊåÅÂü∫‰∫é Milvus Âíå Elasticsearch ÁöÑÂèåÊ®°Ê£ÄÁ¥¢ÊñπÊ°àÔºå
Ê∂µÁõñÂêëÈáèÁ¥¢ÂºïÊûÑÂª∫„ÄÅÊñáÊ°£ÂùóÊèíÂÖ•‰∏éÂà†Èô§„ÄÅÁ¥¢ÂºïÈáçÂª∫„ÄÅÊñáÊú¨ÊëòË¶Å‰∏éËØ≠‰πâÂéªÈáç„ÄÅÂ§öÊ∫êËûçÂêàÊ£ÄÁ¥¢‰∏é reranker Á≤æÊéíÁ≠âÂäüËÉΩ„ÄÇ

Ê®°ÂùóÂäüËÉΩÊ¶ÇËßàÔºö
------------------------------------------------
1. ÂêëÈáèÂ∫ìÔºàMilvusÔºâÁÆ°ÁêÜÔºö
   - `reset_milvus`: ÈáçÂª∫ Milvus collectionÔºåÊîØÊåÅ‰∏ªÈîÆËá™Â¢û‰∏éÂ§öÂ≠óÊÆµÁªìÊûÑ„ÄÇ
   - `insert_block_to_milvus`: ÂêëÈáèÂùóÊèíÂÖ•ÔºåÊîØÊåÅÊâπÈáèÂÜôÂÖ•‰∏éÂµåÂÖ•ÁîüÊàê„ÄÇ
   - `delete_blocks_from_milvus`: Êåâ document_index Âà†Èô§ Milvus ÂêëÈáè„ÄÇ
   - `query_milvus_blocks`: Âü∫‰∫éËØ≠‰πâÂêëÈáèËøõË°å ANN Ê£ÄÁ¥¢ÔºåÊîØÊåÅ reranker Á≤æÊéí„ÄÇ

2. ÂÖ≥ÈîÆËØçÂ∫ìÔºàElasticsearchÔºâÁÆ°ÁêÜÔºö
   - `reset_es`: ÈáçÂª∫ Elasticsearch Á¥¢ÂºïÁªìÊûÑÔºå‰ΩøÁî® IK ÂàÜËØçÂô®ËøõË°å‰∏≠Êñá‰ºòÂåñ„ÄÇ
   - `insert_block_to_es`: ÊâπÈáèÊèíÂÖ•ÊñáÊ°£ÂùóÊñáÊú¨Ëá≥ Elasticsearch„ÄÇ
   - `delete_blocks_from_es`: Êåâ document_index Âà†Èô§ ES ÊñáÊ°£Âùó„ÄÇ
   - `query_es_blocks`: Âü∫‰∫éÂÖ≥ÈîÆËØçÊäΩÂèñÊûÑÂª∫Êü•ËØ¢ËØ≠Âè•ÔºåÊâßË°åÂÄíÊéíÊ£ÄÁ¥¢„ÄÇ

3. Â§öÊ∫êËûçÂêàÊ£ÄÁ¥¢‰∏éÂéªÈáçÔºö
   - `query_blocks`: ÂêåÊó∂‰ªé Milvus ‰∏é ES Ê£ÄÁ¥¢ÊñáÊ°£ÂùóÔºåÊîØÊåÅÊó∂Èó¥‰ºòÂÖàÁöÑÂéªÈáçÁ≠ñÁï•‰∏é reranker Á≤æÊéíÈÄªËæë„ÄÇ

4. reranker Ê®°ÂûãÁ≤æÊéíÔºö
   - `Reranker`: ‰ΩøÁî® transformer Ê®°ÂûãÂØπÂÄôÈÄâÂùóËøõË°å query-passage Á≤æÊéí„ÄÇ
   - `rerank_results`: ÂØπÂÄôÈÄâÊñáÊ°£ÂùóÊåâÁÖß relevance ÂàÜÊï∞ÈáçÊñ∞ÊéíÂ∫è„ÄÇ

‰æùËµñÁéØÂ¢É‰∏éÈÖçÁΩÆËØ¥ÊòéÔºö
------------------------------------------------
- Milvus >= 2.xÔºàÈúÄÈ¢ÑÂêØÂä®ÊúçÂä°ÔºåÁõëÂê¨ 19530 Á´ØÂè£Ôºâ
- Elasticsearch >= 7.xÔºàÈúÄÂêØÁî® IK ÂàÜËØçÂô®Ôºâ
- LangChain„ÄÅPyMilvus„ÄÅtransformers„ÄÅtorch Á≠â
- Â§ñÈÉ®‰æùËµñÈÖçÁΩÆÈÄöËøá utils/config.py Ê≥®ÂÖ•ÔºöÂ¶Ç milvus_host, es_host, index_name

"""

import os
import jieba
import numpy as np
from elasticsearch import Elasticsearch
from elasticsearch.helpers import bulk
from langchain_core.documents import Document
from langchain_milvus import Milvus
from pymilvus import (
    connections,
    utility,
    CollectionSchema,
    FieldSchema,
    DataType,
    Collection,
)
import torch
from utils.text_process_utils import build_optimal_jieba_query, deduplicate_ranked_blocks_pal
from time import sleep
from langchain_community.vectorstores import Milvus
from langchain.schema import Document
from time import sleep
from elasticsearch import Elasticsearch, helpers
import jieba.analyse
from utils.config import CONFIG, logger
from utils.llm_api import get_embeddings_from_vllm_async, get_embedding_from_vllm
import numpy as np
from typing import List


# Âä†ËΩΩËá™ÂÆö‰πâËØçÂÖ∏ÔºàÈÄÇÁî®‰∫éÁîµÂïÜ/ËøêËê•Âú∫ÊôØÔºâ
jieba.load_userdict("./user_dict.txt")

# ÂÖ®Â±ÄËøûÊé•Ê±†
_es_clients = {}
_milvus_alias_map = {}  # Â≠òÂÇ®Â∑≤ËøûÊé•ÁöÑÂà´Âêç
_milvus_collection_cache = {}  # Â≠òÂÇ®Â∑≤ÂàõÂª∫Âπ∂Âä†ËΩΩÁöÑ Collection ÂØπË±°


def get_env_config(env=None):
    """Ëé∑ÂèñÊåáÂÆöÁéØÂ¢É‰∏ãÁöÑÂÆåÊï¥ÈÖçÁΩÆÈ°π"""
    env = env or os.getenv("RAG_ENV") or CONFIG.get("env_default", "dev")
    env_cfg = CONFIG.get("env_config", {}).get(env)
    if not env_cfg:
        raise ValueError(f"‚ùå Êú™ÊâæÂà∞ÁéØÂ¢ÉÈÖçÁΩÆ: {env}")
    return env, env_cfg

def get_es(env=None):
    env, env_cfg = get_env_config(env)
    
    if env not in _es_clients:
        logger.debug(f"üîå ÂàùÂßãÂåñ ES ËøûÊé• [{env}]Ôºö{env_cfg['es_host']}")
    if "es_user" in env_cfg and "es_password" in env_cfg:
        es = Elasticsearch(
            hosts=[f"http://{env_cfg['es_host']}:9200"],
            basic_auth=(env_cfg["es_user"], env_cfg["es_password"]),
            request_timeout=30
        )
        _es_clients[env] = es
    else:
        _es_clients[env] = Elasticsearch(f"http://{env_cfg['es_host']}:9200")


    return _es_clients[env]


# def get_milvus_collection(env=None):
#     """
#     Ëé∑ÂèñÊåáÂÆöÁéØÂ¢É‰∏ãÁöÑ Milvus collectionÔºå‰ΩøÁî®ÁºìÂ≠òÊú∫Âà∂ÈÅøÂÖçÈáçÂ§çÂàõÂª∫ÂíåÂä†ËΩΩ„ÄÇ
#     """
#     env, env_cfg = get_env_config(env)
#     alias = env  # ‰ΩøÁî®ÁéØÂ¢ÉÂêç‰Ωú‰∏∫ËøûÊé•Âà´Âêç
#     collection_name = env_cfg["collection_name"]
#     print(collection_name)
    
#     # ÊûÑÂª∫ÁºìÂ≠òÈîÆ
#     cache_key = f"{alias}_{collection_name}"
    
#     # Ê£ÄÊü•ËøûÊé•ÊòØÂê¶Â∑≤Â≠òÂú®Ôºå‰∏çÂ≠òÂú®ÂàôÂàõÂª∫
#     if alias not in _milvus_alias_map:
#         logger.debug(f"üîå ÂàùÂßãÂåñ Milvus ËøûÊé• [{env}]Ôºö{env_cfg['milvus_host']}")
#         connections.connect(alias=alias, host=env_cfg["milvus_host"], port="19530")
#         _milvus_alias_map[alias] = True
    
#     # Ê£ÄÊü• Collection ÊòØÂê¶Â∑≤ÁºìÂ≠òÔºå‰∏çÂ≠òÂú®ÂàôÂàõÂª∫Âπ∂Âä†ËΩΩ
#     if cache_key not in _milvus_collection_cache:
#         logger.debug(f"üìö Âä†ËΩΩ Milvus Collection: {collection_name}")
#         col = Collection(name=collection_name, using=alias)
#         col.load()
#         _milvus_collection_cache[cache_key] = col
    
#     return _milvus_collection_cache[cache_key]




def get_milvus_collection(env=None):
    env, env_cfg = get_env_config(env)
    alias = env
    collection_name = env_cfg["collection_name"]
    print(f"üìù Ê≠£Âú®Âä†ËΩΩ Collection: {collection_name}")
    cache_key = f"{alias}_{collection_name}"
    
    if alias not in _milvus_alias_map:
        logger.debug(f"üîå ÂàùÂßãÂåñ Milvus ËøûÊé• [{env}]Ôºö{env_cfg['milvus_host']}")
        connect_params = {
            "alias": alias,
            "host": env_cfg["milvus_host"],
            "port": "19530",
            "secure": False
        }
        if "milvus_user" in env_cfg and "milvus_password" in env_cfg:
            connect_params["user"] = env_cfg["milvus_user"]
            connect_params["password"] = env_cfg["milvus_password"]

        connections.connect(**connect_params)
        _milvus_alias_map[alias] = True

    if cache_key not in _milvus_collection_cache:
        logger.debug(f"üìö Âä†ËΩΩ Milvus Collection: {collection_name}")
        col = Collection(name=collection_name, using=alias)
        col.load()
        _milvus_collection_cache[cache_key] = col
    
    return _milvus_collection_cache[cache_key]



def get_index_name(env=None):
    """Ëé∑ÂèñÂΩìÂâçÁéØÂ¢ÉÁöÑ Elasticsearch Á¥¢ÂºïÂêç"""
    _, env_cfg = get_env_config(env)
    return env_cfg["index_name"]

# ======================== Ëé∑ÂèñÊúÄÂ§ßÂÖ®Â±ÄÁ¥¢Âºï(milvus) ========================
def get_max_global_idx_milvus(env="dev"):
    """Ëé∑ÂèñÂΩìÂâçÁéØÂ¢É‰∏ã Milvus ‰∏≠ global_chunk_idx ÁöÑÊúÄÂ§ßÂÄº"""
    try:
        _, cfg = get_env_config(env)
        alias = env
        host = cfg["milvus_host"]
        collection_name = cfg["collection_name"]

        # ÂàùÂßãÂåñËøûÊé•ÔºàÂ¶ÇÊûúÊú™ËøûÊé•Ôºâ
        if not connections.has_connection(alias):
            connections.connect(alias=alias, host=host, port="19530")

        collection = Collection(name=collection_name, using=alias)
        collection.load()

        results = collection.query(
            expr="global_chunk_idx >= 0",
            output_fields=["global_chunk_idx"],
        )
        if not results:
            return 0
        max_idx = max(item["global_chunk_idx"] for item in results)
        return max_idx + 1
    except Exception as e:
        print(f"‚ö†Ô∏è Êü•ËØ¢Â§±Ë¥•: {e}")
        return 0


# ======================== ES Á¥¢ÂºïÈáçÂª∫ ========================
def reset_es(env="dev"):
    """ÈáçÂª∫ Elasticsearch Á¥¢ÂºïÔºàÂê´ ik ÂàÜËØç‰∏éÂ≠óÊÆµÊò†Â∞ÑÔºâ"""
    _, cfg = get_env_config(env)
    index_name = cfg["index_name"]
    es = get_es(env)

    logger.info("Connected to Elasticsearch!" if es.ping() else "Connection failed.")

    if es.indices.exists(index=index_name):
        logger.info(f"‚ö†Ô∏è {env} ÁéØÂ¢ÉÁ¥¢Âºï {index_name} Â∑≤Â≠òÂú®ÔºåÂà†Èô§‰∏≠...")
        es.indices.delete(index=index_name, ignore_unavailable=True, request_timeout=20)

    # es.indices.create(
    #     index=index_name,
    #     body={
    #         "settings": {
    #             "analysis": {
    #                 "analyzer": {
    #                     "ik_max_word": {
    #                         "type": "custom",
    #                         "tokenizer": "ik_max_word"
    #                     }
    #                 }
    #             }
    #         },
    #         "mappings": {
    #             "properties": {
    #                 "document_index": { "type": "long" },
    #                 "chunk_idx": { "type": "integer" },
    #                 "text": {
    #                     "type": "text",
    #                     "analyzer": "ik_max_word",
    #                     "fields": { "keyword": { "type": "keyword" } }
    #                 },
    #                 "page_url": {
    #                     "type": "text",
    #                     "fields": { "keyword": { "type": "keyword" } }
    #                 },
    #                 "page_name": {
    #                     "type": "text",
    #                     "analyzer": "ik_max_word",
    #                     "fields": { "keyword": { "type": "keyword" } }
    #                 },
    #                 "title": {
    #                     "type": "text",
    #                     "analyzer": "ik_max_word",
    #                     "fields": { "keyword": { "type": "keyword" } }
    #                 },
    #                 "summary": {
    #                     "type": "text",
    #                     "analyzer": "ik_max_word",
    #                     "fields": { "keyword": { "type": "keyword" } }
    #                 },
    #                 "time": {
    #                     "type": "text",
    #                     "fields": { "keyword": { "type": "keyword" } }
    #                 },
    #                 "question": {
    #                     "type": "text",
    #                     "analyzer": "ik_max_word",
    #                     "fields": { "keyword": { "type": "keyword" } }
    #                 }
    #             }
    #         }
    #     },
    # )
    es.indices.create(
    index=index_name,
    body={
        "mappings": {
            "properties": {
                "document_index": { "type": "long" },
                "chunk_idx": { "type": "integer" },
                "text": {
                    "type": "text",
                    "fields": { "keyword": { "type": "keyword" } }
                },
                "page_url": {
                    "type": "text",
                    "fields": { "keyword": { "type": "keyword" } }
                },
                "page_name": {
                    "type": "text",
                    "fields": { "keyword": { "type": "keyword" } }
                },
                "title": {
                    "type": "text",
                    "fields": { "keyword": { "type": "keyword" } }
                },
                "summary": {
                    "type": "text",
                    "fields": { "keyword": { "type": "keyword" } }
                },
                "time": {
                    "type": "text",
                    "fields": { "keyword": { "type": "keyword" } }
                },
                "question": {
                    "type": "text",
                    "fields": { "keyword": { "type": "keyword" } }
                }
                }
            }
        },
    )

    logger.info(f"‚úÖ {env} ÁéØÂ¢É ES Á¥¢Âºï '{index_name}' Â∑≤ÊàêÂäüÂàõÂª∫")


# ======================== Milvus ÂêëÈáèÂ∫ìÈáçÂª∫ ========================
def reset_milvus(env="dev", dim=768):
    """ÈáçÂª∫ Milvus ÂêëÈáèÈõÜÂêàÔºàËá™Âä®Ëé∑Âèñ collection_nameÔºâ"""
    _, cfg = get_env_config(env)
    collection_name = cfg["collection_name"]
    host = cfg["milvus_host"]
    alias = env

    if not connections.has_connection(alias):
        connections.connect(alias=alias, host=host, port="19530")

    if utility.has_collection(collection_name, using=alias):
        logger.info(f"‚ö†Ô∏è {env} ÁéØÂ¢É Milvus ÈõÜÂêà '{collection_name}' Â∑≤Â≠òÂú®ÔºåÊ≠£Âú®Âà†Èô§...")
        utility.drop_collection(collection_name, using=alias)

    logger.info(f"üöÄ {env} ÁéØÂ¢ÉÊ≠£Âú®ÂàõÂª∫ Milvus collection: {collection_name}")
    fields = [
        FieldSchema(name="global_chunk_idx", dtype=DataType.INT64, is_primary=True, auto_id=True),
        FieldSchema(name="document_index", dtype=DataType.INT64),
        FieldSchema(name="chunk_idx", dtype=DataType.INT64),
        FieldSchema(name="vector", dtype=DataType.FLOAT_VECTOR, dim=dim),
        FieldSchema(name="text", dtype=DataType.VARCHAR, max_length=20000),
        FieldSchema(name="page_url", dtype=DataType.VARCHAR, max_length=1024),
        FieldSchema(name="page_name", dtype=DataType.VARCHAR, max_length=512),
        FieldSchema(name="title", dtype=DataType.VARCHAR, max_length=512),
        FieldSchema(name="summary", dtype=DataType.VARCHAR, max_length=4096),
        FieldSchema(name="time", dtype=DataType.VARCHAR, max_length=128),
        FieldSchema(name="question", dtype=DataType.VARCHAR, max_length=1024),
    ]
    schema = CollectionSchema(fields=fields, description="HTMLÂùóÂêëÈáèÁ¥¢Âºï")
    Collection(name=collection_name, schema=schema, using=alias)
    logger.info(f"‚úÖ {env} ÁéØÂ¢É Milvus collection '{collection_name}' Â∑≤ÂàõÂª∫")

# ======================== ÊèíÂÖ• Milvus ========================
def insert_block_to_milvus(doc_meta_list, embedder, env="dev", batch_size=2) -> int:
    """
    ÂêëÊåáÂÆöÁéØÂ¢ÉÁöÑ Milvus ÊèíÂÖ•ÊñáÊ°£ÂùóÔºåËá™Âä®Ëé∑Âèñ collection_name Âíå host
    """
    _, cfg = get_env_config(env)
    collection_name = cfg["collection_name"]
    host = cfg["milvus_host"]

    all_docs = []
    for doc in doc_meta_list:
        doc.setdefault("document_index", -1)
        node = Document(
            page_content=doc["text"],
            metadata={k: v for k, v in doc.items() if k != "text"}
        )
        all_docs.append(node)

    logger.debug(f"üß† Ê≠£Âú®ÊèíÂÖ•ÂêëÈáèÂà∞ MilvusÔºà{env}Ôºâcollection: {collection_name} ...")

    # ÂàùÂßãÂåñ Milvus ÂêëÈáèÂ∫ìÂØπË±°Ôºàfrom_documents ‰ºöËá™Âä®Âª∫Ë°®Ôºå‰ΩÜÊàë‰ª¨Âª∫ËÆÆÂÖà resetÔºâ
    milvus = Milvus.from_documents(
        [all_docs[0]],  # Áî®Á¨¨‰∏ÄÊù°ÂàùÂßãÂåñ collection
        embedder,
        collection_name=collection_name,
        connection_args={
                "host": host,
                "port": "19530",
                "user": cfg.get("milvus_user", ""),
                "password": cfg.get("milvus_password", ""),
                "secure": False  # Â¶ÇÊûú‰Ω†Ê≤°ÂêØÁî®TLSÔºå‰∏ÄÂÆöËÆæ‰∏∫ FalseÔºÅ
            },
        index_params={
            "metric_type": "COSINE",
            "index_type": "IVF_FLAT",
            "params": {"nlist": 64}
        },
    )
    


    inserted = 1
    for i in range(1, len(all_docs), batch_size):
        batch = all_docs[i:i + batch_size]
        try:
            milvus.add_documents(batch)
            inserted += len(batch)
            logger.debug(f"‚úÖ ÊèíÂÖ• batch {i // batch_size + 1}: {len(batch)} Êù°")
        except Exception as e:
            logger.error(f"‚ùå ÊèíÂÖ• batch Â§±Ë¥•: {e}")

    logger.debug(f"‚úÖ Â∑≤ÊèíÂÖ• MilvusÔºà{env}ÔºâÔºö{inserted} Êù°ÊñáÊ°£Âùó")
    return inserted


async def insert_blocks_to_milvus_vllm_async(
    doc_meta_list: List[dict],
    url: str,
    env: str = "dev",
    batch_size: int = 2,
):
    _, cfg = get_env_config(env)
    collection = get_milvus_collection(env)
    logger.debug(f"üß† ÂáÜÂ§áÊèíÂÖ• {len(doc_meta_list)} Êù°ÊñáÊ°£Âùó Âà∞ Milvus[{env}]Ôºö{cfg['collection_name']}")

    total_inserted = 0
    N = len(doc_meta_list)

    for start in range(0, N, batch_size):
        batch = doc_meta_list[start : start + batch_size]

        try:
            # ÂÖàÊî∂ÈõÜ‰∏ÄÊâπÁî®‰∫é VLLM Ëé∑Âèñ embedding ÁöÑÊñáÊú¨ÔºàÊØîÂ¶Ç title Êàñ questionÔºâ
            texts_for_embedding = [doc.get("text", "") for doc in batch]
            embeddings = await get_embeddings_from_vllm_async(texts_for_embedding, url)

            # ÊûÑÈÄ†ÊØèÂàó
            document_index =   [doc.get("document_index", -1) for doc in batch]
            chunk_idx =        [doc.get("chunk_idx", -1) for doc in batch]
            vector =           embeddings
            text =             [doc.get("text", "") for doc in batch]
            page_url =         [doc.get("page_url", "") for doc in batch]
            page_name =        [doc.get("page_name", "") for doc in batch]
            title =            [doc.get("title", "") for doc in batch]
            summary =          [doc.get("summary", "") for doc in batch]
            time =             [doc.get("time", "") for doc in batch]
            question =         [doc.get("question", "") for doc in batch]

            milvus_data = [
                document_index,
                chunk_idx,
                vector,
                text,
                page_url,
                page_name,
                title,
                summary,
                time,
                question,
            ]

            collection.insert(milvus_data)
            logger.debug(f"‚úÖ Batch {start//batch_size + 1} ÊèíÂÖ•ÊàêÂäüÔºö{len(batch)} Êù°")
            total_inserted += len(batch)

        except Exception as e:
            logger.error(f"‚ùå Batch {start//batch_size + 1} ÊèíÂÖ•Â§±Ë¥•Ôºö{e}")

    logger.info(f"üöÄ ÊèíÂÖ•ÂÆåÊàêÔºåÂÖ±ÊàêÂäüÊèíÂÖ• {total_inserted} Êù°")
    return total_inserted



def insert_blocks_to_milvus_vllm(
    doc_meta_list: List[dict],
    url: str,
    env: str = "dev",
):
    """Â∞ÜÊñáÊ°£ÂùóÈÄêÊù°ÊèíÂÖ• Milvus"""
    _, cfg = get_env_config(env)
    collection = get_milvus_collection(env)
    logger.debug(f"üß† ÂáÜÂ§áÊèíÂÖ• {len(doc_meta_list)} Êù°ÊñáÊ°£Âùó Âà∞ Milvus[{env}]Ôºö{cfg['collection_name']}")

    success_count = 0
    failed_count = 0
    failed_indices = []

    for idx, doc in enumerate(doc_meta_list):
        try:
            # Ëé∑ÂèñÊñáÊú¨ÂµåÂÖ•ÂêëÈáè
            text = doc.get("text", "")
            embeddings = get_embedding_from_vllm(text, url)  # ÂêåÊ≠•ÁâàÊú¨ÁöÑÂµåÂÖ•ÂáΩÊï∞
            
            # ÊûÑÈÄ†ÊèíÂÖ•Êï∞ÊçÆ
            milvus_data = [
                [doc.get("document_index", -1)],
                [doc.get("chunk_idx", -1)],
                [embeddings[0]],  # ÂèñÁ¨¨‰∏Ä‰∏™ÂµåÂÖ•ÂêëÈáè
                [text],
                [doc.get("page_url", "")],
                [doc.get("page_name", "")],
                [doc.get("title", "")],
                [doc.get("summary", "")],
                [doc.get("time", "")],
                [doc.get("question", "")],
            ]
            
            # ÂçïÊù°ÊèíÂÖ•
            collection.insert(milvus_data)
            success_count += 1
            logger.debug(f"‚úÖ ÊèíÂÖ•ÊàêÂäüÔºöÊñáÊ°£Á¥¢Âºï {doc.get('document_index', -1)}ÔºåÂùóÁ¥¢Âºï {doc.get('chunk_idx', -1)}")
            
        except Exception as e:
            failed_count += 1
            failed_indices.append(idx)
            logger.error(f"‚ùå ÊèíÂÖ•Â§±Ë¥•ÔºöÊñáÊ°£Á¥¢Âºï {doc.get('document_index', -1)}ÔºåÂùóÁ¥¢Âºï {doc.get('chunk_idx', -1)}ÔºåÈîôËØØÔºö{e}")
    
    logger.info(f"üöÄ ÊèíÂÖ•ÂÆåÊàêÔºåÊàêÂäü {success_count} Êù°ÔºåÂ§±Ë¥• {failed_count} Êù°")
    
    if failed_count > 0:
        logger.warning(f"‚ö†Ô∏è ‰ª•‰∏ãÁ¥¢ÂºïÁöÑÊñáÊ°£ÊèíÂÖ•Â§±Ë¥•Ôºö{failed_indices}")
    
    return success_count




# ======================== ÊèíÂÖ• ES ========================
def insert_block_to_es(doc_meta_list, env="dev") -> int:
    """
    ÂêëÊåáÂÆöÁéØÂ¢ÉÁöÑ Elasticsearch Á¥¢ÂºïÊèíÂÖ•ÊñáÊ°£ÂùóÔºåËá™Âä®Ëé∑Âèñ index_name
    """
    _, cfg = get_env_config(env)
    index_name = cfg["index_name"]
    es = get_es(env)

    logger.debug(f"üì• Ê≠£Âú®ÊèíÂÖ•ÊñáÊ°£Âà∞ Elasticsearch Á¥¢ÂºïÔºà{env}Ôºâ: {index_name} ...")

    actions = []
    for doc in doc_meta_list:
        if "document_index" not in doc:
            doc["document_index"] = -1
        actions.append({
            "_index": index_name,
            "_source": {
                "document_index": doc["document_index"],
                "chunk_idx": doc["chunk_idx"],
                "title": doc["title"],
                "summary": doc.get("summary", ""),
                "text": doc["text"],
                "page_url": doc["page_url"],
                "page_name": doc["page_name"],
                "time": doc.get("time", ""),
                "question": doc.get("question", "")
            }
        })

    try:
        resp = helpers.bulk(es, actions)
        logger.debug(f"‚úÖ Â∑≤ÊèíÂÖ• ESÔºà{env}ÔºâÔºö{resp[0]} Êù°ÊñáÊ°£Âùó")
        return resp[0]
    except Exception as e:
        logger.error(f"‚ùå ES ÊèíÂÖ•Â§±Ë¥•Ôºà{env}Ôºâ: {e}")
        return 0




# ======================== Âà†Èô§ Milvus ‰∏≠ÁöÑÊñáÊ°£Âùó ========================
def delete_blocks_from_milvus(document_index: int, env: str = "dev") -> int:
    """‰ªéÊåáÂÆöÁéØÂ¢É‰∏ãÁöÑ Milvus collection ‰∏≠Âà†Èô§Êüê‰∏™ document_index"""
    try:
        col = get_milvus_collection(env=env)
        expr = f"document_index == {document_index}"
        result = col.delete(expr)
        count = result.delete_count if hasattr(result, "delete_count") else 0
        logger.debug(f"üóëÔ∏è MilvusÔºà{env}Ôºâ: Â∑≤Âà†Èô§ document_index = {document_index} ÁöÑÊñáÊ°£ÂùóÔºåÂÖ± {count} Êù°")
        return count
    except Exception as e:
        logger.error(f"‚ùå Milvus Âà†Èô§Â§±Ë¥•Ôºà{env}Ôºâ: {e}")
        return 0


# ======================== Âà†Èô§ ES ‰∏≠ÁöÑÊñáÊ°£Âùó ========================
def delete_blocks_from_es(document_index: int, env: str = "dev") -> int:
    """‰ªéÊåáÂÆöÁéØÂ¢É‰∏ãÁöÑ Elasticsearch Á¥¢Âºï‰∏≠Âà†Èô§Êüê‰∏™ document_index"""
    try:
        _, cfg = get_env_config(env)
        index_name = cfg["index_name"]
        es = get_es(env)

        query = {
            "query": {
                "term": {
                    "document_index": document_index
                }
            }
        }

        hits = es.search(index=index_name, body=query, size=10000)["hits"]["hits"]
        if not hits:
            logger.debug(f"üì≠ ESÔºà{env}Ôºâ: Êú™ÊâæÂà∞ document_index = {document_index} ÁöÑÊñáÊ°£")
            return 0

        resp = helpers.bulk(
            client=es,
            actions=[
                {"_op_type": "delete", "_index": index_name, "_id": hit["_id"]}
                for hit in hits
            ]
        )
        logger.debug(f"üóëÔ∏è ESÔºà{env}Ôºâ: Â∑≤Âà†Èô§ document_index = {document_index} ÁöÑÊñáÊ°£ÂùóÔºåÂÖ± {resp[0]} Êù°")
        return resp[0]
    except Exception as e:
        logger.error(f"‚ùå ES Âà†Èô§Â§±Ë¥•Ôºà{env}Ôºâ: {e}")
        return 0



# ======================== Milvus Êü•ËØ¢ÂáΩÊï∞ ========================
def query_milvus_blocks(
    question,
    embedder,
    env="dev",
    top_k=10,
    rerank_top_k=5,
    reranker=None,
):
    _, cfg = get_env_config(env)
    collection_name = cfg["collection_name"]

    collection = get_milvus_collection(env=env)
    if not collection.has_index():
        print(f"‚öôÔ∏è Creating index on vector ...")
        collection.create_index(
            field_name="vector",
            index_params={
                "index_type": "IVF_FLAT",
                "metric_type": "COSINE",
                "params": {"nlist": 64},
            },
            index_name="default",
        )
        print("‚úÖ Index created.")
    collection.load()

    query_vec = embedder.embed_query(question)

    results = collection.search(
        data=[query_vec],
        anns_field="vector",
        param={"metric_type": "COSINE", "params": {"nprobe": 100}},
        limit=top_k,
        output_fields=[
            "text", "page_url", "chunk_idx", "page_name",
            "title", "summary", "time", "question", "document_index"
        ],
    )

    milvus_rank = []
    for hits in results:
        for hit in hits:
            milvus_rank.append({
                "chunk_idx": hit.entity.get("chunk_idx", -1),
                "page_url": hit.entity.get("page_url", "unknown"),
                "page_name": hit.entity.get("page_name", "none"),
                "title": hit.entity.get("title", "none"),
                "summary": hit.entity.get("summary", ""),
                "time": hit.entity.get("time", ""),
                "text": hit.entity.get("text", ""),
                "question": hit.entity.get("question", ""),
                "document_index": hit.entity.get("document_index", -1),
            })

    return milvus_rank



# ======================== ES Êü•ËØ¢ÂáΩÊï∞ ========================
def query_es_blocks(
    question,
    env="dev",
    top_k=10
):
    _, cfg = get_env_config(env)
    es = get_es(env)
    index_name = cfg["index_name"]

    keywords = jieba.analyse.extract_tags(question, topK=8, withWeight=True)
    keywords = [_[0] for _ in keywords if _[1] > 0.0]

    fields_config = {"text": {"boost": 1}, "title": {"boost": 2}}
    query = build_optimal_jieba_query(keywords, fields_config)
    es_response = es.search(index=index_name, query=query.get("query", {}), size=top_k)

    es_rank = [
        {
            "document_index": hit["_source"].get("document_index", -1),
            "chunk_idx": hit["_source"].get("chunk_idx", -1),
            "page_url": hit["_source"].get("page_url", "unknown"),
            "page_name": hit["_source"].get("page_name", "none"),
            "title": hit["_source"].get("title", "none"),
            "summary": hit["_source"].get("summary", ""),
            "time": hit["_source"].get("time", ""),
            "question": hit["_source"].get("question", ""),
            "text": hit["_source"].get("text", ""),
        } for hit in es_response["hits"]["hits"]
    ]

    return es_rank





# ======================== Â§öÊ∫êÊü•ËØ¢ÂáΩÊï∞ ========================
def query_blocks(
    question,
    embedder,
    env="dev",
    top_k=10,
    reranker=None,
    rerank_top_k=5
):
    # 1. Milvus Êü•ËØ¢
    milvus_raw = query_milvus_blocks(
        question=question,
        embedder=embedder,
        env=env,
        top_k=top_k,
        reranker=None,
        rerank_top_k=rerank_top_k
    )

    # 2. ES Êü•ËØ¢
    es_raw = query_es_blocks(
        question=question,
        env=env,
        top_k=top_k
    )

    print(f"üì• ÂêàÂπ∂Ââç: Milvus={len(milvus_raw)}, ES={len(es_raw)}")

    # 3. ÂêàÂπ∂ÂéªÈáçÔºàÁõÆÂâç‰ªÖÊãºÊé•ÔºåÂ¶ÇÈúÄÂèØÊõøÊç¢‰∏∫ deduplicate_ranked_blocks_palÔºâ
    final_blocks = milvus_raw + es_raw

    # 4. ÂèØÈÄâ rerank
    if reranker is not None:
        final_blocks = rerank_results(final_blocks, question, reranker, top_k=rerank_top_k)

    print(f"‚úÖ ÊúÄÁªàËøîÂõûÊñáÊ°£ÂùóÊï∞: {len(final_blocks)}")
    print("üì¶ ËøîÂõûÁöÑÊñáÊ°£ÂùóÁ§∫‰æã:")
    for i, doc in enumerate(final_blocks[:5]):
        print(f"  [#{i+1}] document_index={doc.get('document_index',-1)}  page_url={doc['page_url']:<30} chunk_idx={doc['chunk_idx']:<4} title={doc['title'][:30]:<30}")
    return final_blocks


# ======================== Reranker ÂáΩÊï∞ ========================
class Reranker:
    def __init__(self, model, tokenizer, device):
        self.model = model
        self.tokenizer = tokenizer
        self.device = device

    def compute_score_pairs(self, pairs):
        inputs = self.tokenizer(
            [q for q, a in pairs], [a for q, a in pairs],
            padding=True, truncation=True, return_tensors="pt"
        ).to(self.device)
        with torch.no_grad():
            scores = self.model(**inputs).logits.squeeze(-1)
        return scores.cpu().tolist()

def rerank_results(docs, query, reranker, top_k):
    """
    ‰ΩøÁî® reranker ÂØπÊ£ÄÁ¥¢ÁªìÊûúËøõË°åÁ≤æÊéí
    """
    print("üîÅ Running Reranker...")
    texts = [d["text"] for d in docs]
    pairs = [(query, t) for t in texts]
    scores = reranker.compute_score_pairs(pairs)

    print("üìä ÂéüÂßãÈ°∫Â∫èÂèäÂàÜÊï∞:")
    for i, (doc, score) in enumerate(zip(docs, scores)):
        print(f"  [#{i+1}] chunk_idx={doc['chunk_idx']:<4} summary={doc['summary'][:30]:<30} score={score:.4f}")

    reranked = sorted(zip(docs, scores), key=lambda x: x[1], reverse=True)
    reranked_docs = [x[0] for x in reranked[:top_k]]

    print("\nüèÜ Reranked Top Results:")
    for i, (doc, score) in enumerate(reranked[:top_k]):
        print(f"  [#{i+1}] chunk_idx={doc['chunk_idx']:<4} summary={doc['summary'][:30]:<30} score={score:.4f}")

    print(f"‚úÖ Á≤æÊéíÂÆåÊàêÔºåÈÄâÂèñÂâç {top_k} Êù°")
    return reranked_docs



