# -*- coding: utf-8 -*-
"""
HTML æ–‡æ¡£ç´¢å¼•ä¸å¤šæ¨¡æ€æ£€ç´¢æ ¸å¿ƒæ¨¡å—

æœ¬æ¨¡å—æä¾›é¢å‘çŸ¥è¯†å‹é—®ç­”ç³»ç»Ÿï¼ˆå¦‚ RAGï¼‰çš„ä¸€ä½“åŒ–å‘é‡ç´¢å¼•ç®¡ç†ä¸æŸ¥è¯¢èƒ½åŠ›ï¼Œæ”¯æŒåŸºäº Milvus å’Œ Elasticsearch çš„åŒæ¨¡æ£€ç´¢æ–¹æ¡ˆï¼Œ
æ¶µç›–å‘é‡ç´¢å¼•æ„å»ºã€æ–‡æ¡£å—æ’å…¥ä¸åˆ é™¤ã€ç´¢å¼•é‡å»ºã€æ–‡æœ¬æ‘˜è¦ä¸è¯­ä¹‰å»é‡ã€å¤šæºèåˆæ£€ç´¢ä¸ reranker ç²¾æ’ç­‰åŠŸèƒ½ã€‚

æ¨¡å—åŠŸèƒ½æ¦‚è§ˆï¼š
------------------------------------------------
1. å‘é‡åº“ï¼ˆMilvusï¼‰ç®¡ç†ï¼š
   - `reset_milvus`: é‡å»º Milvus collectionï¼Œæ”¯æŒä¸»é”®è‡ªå¢ä¸å¤šå­—æ®µç»“æ„ã€‚
   - `insert_block_to_milvus`: å‘é‡å—æ’å…¥ï¼Œæ”¯æŒæ‰¹é‡å†™å…¥ä¸åµŒå…¥ç”Ÿæˆã€‚
   - `delete_blocks_from_milvus`: æŒ‰ file_idx åˆ é™¤ Milvus å‘é‡ã€‚
   - `query_milvus_blocks`: åŸºäºè¯­ä¹‰å‘é‡è¿›è¡Œ ANN æ£€ç´¢ï¼Œæ”¯æŒ reranker ç²¾æ’ã€‚

2. å…³é”®è¯åº“ï¼ˆElasticsearchï¼‰ç®¡ç†ï¼š
   - `reset_es`: é‡å»º Elasticsearch ç´¢å¼•ç»“æ„ï¼Œä½¿ç”¨ IK åˆ†è¯å™¨è¿›è¡Œä¸­æ–‡ä¼˜åŒ–ã€‚
   - `insert_block_to_es`: æ‰¹é‡æ’å…¥æ–‡æ¡£å—æ–‡æœ¬è‡³ Elasticsearchã€‚
   - `delete_blocks_from_es`: æŒ‰ file_idx åˆ é™¤ ES æ–‡æ¡£å—ã€‚
   - `query_es_blocks`: åŸºäºå…³é”®è¯æŠ½å–æ„å»ºæŸ¥è¯¢è¯­å¥ï¼Œæ‰§è¡Œå€’æ’æ£€ç´¢ã€‚

3. å¤šæºèåˆæ£€ç´¢ä¸å»é‡ï¼š
   - `query_blocks`: åŒæ—¶ä» Milvus ä¸ ES æ£€ç´¢æ–‡æ¡£å—ï¼Œæ”¯æŒæ—¶é—´ä¼˜å…ˆçš„å»é‡ç­–ç•¥ä¸ reranker ç²¾æ’é€»è¾‘ã€‚

4. reranker æ¨¡å‹ç²¾æ’ï¼š
   - `Reranker`: ä½¿ç”¨ transformer æ¨¡å‹å¯¹å€™é€‰å—è¿›è¡Œ query-passage ç²¾æ’ã€‚
   - `rerank_results`: å¯¹å€™é€‰æ–‡æ¡£å—æŒ‰ç…§ relevance åˆ†æ•°é‡æ–°æ’åºã€‚

ä¾èµ–ç¯å¢ƒä¸é…ç½®è¯´æ˜ï¼š
------------------------------------------------
- Milvus >= 2.xï¼ˆéœ€é¢„å¯åŠ¨æœåŠ¡ï¼Œç›‘å¬ 19530 ç«¯å£ï¼‰
- Elasticsearch >= 7.xï¼ˆéœ€å¯ç”¨ IK åˆ†è¯å™¨ï¼‰
- LangChainã€PyMilvusã€transformersã€torch ç­‰
- å¤–éƒ¨ä¾èµ–é…ç½®é€šè¿‡ utils/config.py æ³¨å…¥ï¼šå¦‚ milvus_host, es_host, index_name

"""

import os
import jieba
import numpy as np
from elasticsearch import Elasticsearch
from elasticsearch.helpers import bulk
from langchain_core.documents import Document
from langchain_milvus import Milvus
from pymilvus import (
    connections,
    utility,
    CollectionSchema,
    FieldSchema,
    DataType,
    Collection,
)
import torch
from utils.text_process_utils import build_optimal_jieba_query, deduplicate_ranked_blocks_pal
from time import sleep
from langchain_community.vectorstores import Milvus
from langchain.schema import Document
from time import sleep
from elasticsearch import Elasticsearch, helpers
import jieba.analyse
from utils.config import CONFIG

# åŠ è½½è‡ªå®šä¹‰è¯å…¸ï¼ˆé€‚ç”¨äºç”µå•†/è¿è¥åœºæ™¯ï¼‰
jieba.load_userdict("./user_dict.txt")


index_name = CONFIG.get("index_name", "curd_env")
Milvus_host = CONFIG.get('milvus_host', "192.168.7.247")
ES_host = CONFIG.get('es_host', "192.168.7.247")


# å…¨å±€ Elasticsearch å’Œ Milvus å®¢æˆ·ç«¯ï¼ˆå†…å»ºè¿æ¥æ± ï¼‰
es_client = Elasticsearch(f"http://{ES_host}:9200")
connections.connect(alias="default", host=Milvus_host, port="19530")


def get_es():
    """è¿”å›å…¨å±€ ES å®¢æˆ·ç«¯"""
    return es_client

def get_milvus_collection(collection_name):
    """è¿”å›å·²è¿æ¥çš„ Milvus collection å®ä¾‹"""
    col = Collection(name=collection_name)
    col.load()
    return col


# ======================== è·å–æœ€å¤§å…¨å±€ç´¢å¼•(milvus) ========================
def get_max_global_idx_milvus(host, collection_name):
    try:
        connections.connect(alias="default", host=host, port="19530")
        collection = Collection(name=collection_name)
        collection.load()
        results = collection.query(
            expr="global_chunk_idx >= 0",
            output_fields=["global_chunk_idx"],
        )
        if not results:
            return 0
        # æå–æ‰€æœ‰ idxï¼Œå–æœ€å¤§å€¼
        max_idx = max(item["global_chunk_idx"] for item in results)
        return max_idx + 1
    except Exception as e:
        print(f"âš ï¸ æŸ¥è¯¢å¤±è´¥: {e}")
        return 0


# ======================== ES ç´¢å¼•é‡å»º ========================
def reset_es(index_name=index_name):
    """é‡å»º Elasticsearch ç´¢å¼•ï¼Œä½¿ç”¨ IK åˆ†è¯å™¨ï¼Œå»é™¤ global_chunk_idxï¼Œå¢åŠ  file_idx"""
    es = get_es()
    print("Connected to ElasticSearch!" if es.ping() else "Connection failed.")

    if es.indices.exists(index=index_name):
        print(f"âš ï¸ ç´¢å¼• {index_name} å·²å­˜åœ¨ï¼Œåˆ é™¤ä¸­...")
        es.indices.delete(index=index_name, ignore_unavailable=True, request_timeout=20)

    es.indices.create(
        index=index_name,
        body={
            "settings": {
                "analysis": {
                    "analyzer": {
                        "ik_max_word": {
                            "type": "custom",
                            "tokenizer": "ik_max_word"
                        }
                    }
                }
            },
            "mappings": {
                "properties": {
                    "file_idx": { "type": "long" },
                    "chunk_idx": { "type": "integer" },
                    "text": {
                        "type": "text",
                        "analyzer": "ik_max_word",
                        "fields": { "keyword": { "type": "keyword" } }
                    },
                    "page_url": {
                        "type": "text",
                        "fields": { "keyword": { "type": "keyword" } }
                    },
                    "page_name": {
                        "type": "text",
                        "analyzer": "ik_max_word",
                        "fields": { "keyword": { "type": "keyword" } }
                    },
                    "title": {
                        "type": "text",
                        "analyzer": "ik_max_word",
                        "fields": { "keyword": { "type": "keyword" } }
                    },
                    "summary": {
                        "type": "text",
                        "analyzer": "ik_max_word",
                        "fields": { "keyword": { "type": "keyword" } }
                    },
                    "time": {
                        "type": "text",
                        "fields": { "keyword": { "type": "keyword" } }
                    },
                    "question": {
                        "type": "text",
                        "analyzer": "ik_max_word",
                        "fields": { "keyword": { "type": "keyword" } }
                    }
                }
            }
        },
    )
    print(f"âœ… ES ç´¢å¼• '{index_name}' å·²æˆåŠŸåˆ›å»ºï¼ˆå« file_idxï¼‰")

# ======================== Milvus å‘é‡åº“é‡å»º ========================
def reset_milvus(collection_name=index_name, dim=768):
    """é‡å»º Milvus å‘é‡é›†åˆï¼Œå«ä¸»é”®è‡ªå¢å’Œ file_idx å­—æ®µ"""
    # âš ï¸ æ³¨æ„ï¼šè¿æ¥åˆå§‹åŒ–å·²åœ¨ connections.py æ‰§è¡Œ
    if utility.has_collection(collection_name):
        print(f"âš ï¸ Milvus é›†åˆ '{collection_name}' å·²å­˜åœ¨ï¼Œæ­£åœ¨åˆ é™¤...")
        utility.drop_collection(collection_name)

    print(f"ğŸš€ æ­£åœ¨åˆ›å»º Milvus collection: {collection_name}")
    fields = [
        FieldSchema(name="global_chunk_idx", dtype=DataType.INT64, is_primary=True, auto_id=True),
        FieldSchema(name="file_idx", dtype=DataType.INT64),
        FieldSchema(name="chunk_idx", dtype=DataType.INT64),
        FieldSchema(name="vector", dtype=DataType.FLOAT_VECTOR, dim=dim),
        FieldSchema(name="text", dtype=DataType.VARCHAR, max_length=20000),
        FieldSchema(name="page_url", dtype=DataType.VARCHAR, max_length=1024),
        FieldSchema(name="page_name", dtype=DataType.VARCHAR, max_length=512),
        FieldSchema(name="title", dtype=DataType.VARCHAR, max_length=512),
        FieldSchema(name="summary", dtype=DataType.VARCHAR, max_length=4096),
        FieldSchema(name="time", dtype=DataType.VARCHAR, max_length=128),
        FieldSchema(name="question", dtype=DataType.VARCHAR, max_length=1024),
    ]
    schema = CollectionSchema(fields=fields, description="HTMLå—å‘é‡ç´¢å¼•")
    Collection(name=collection_name, schema=schema)
    print(f"âœ… Milvus collection '{collection_name}' å·²åˆ›å»ºï¼ˆå«ä¸»é”®è‡ªå¢ + file_idxï¼‰")

# ======================== æ’å…¥ Milvus ========================
def insert_block_to_milvus(doc_meta_list, embedder, collection_name, batch_size=100) -> int:
    print(f"ğŸ§  æ­£åœ¨æ’å…¥å‘é‡åˆ° Milvus collection: {collection_name} ...")
    all_docs = []
    for doc in doc_meta_list:
        # doc.setdefault("file_idx", -1)
        node = Document(
            page_content=doc["text"],
            metadata={k: v for k, v in doc.items() if k != "text"}
        )
        all_docs.append(node)

    milvus = Milvus.from_documents(
        [all_docs[0]],
        embedder,
        collection_name=collection_name,
        connection_args={"host": CONFIG["milvus_host"], "port": "19530"},
        index_params={
            "metric_type": "COSINE",
            "index_type": "IVF_FLAT",
            "params": {"nlist": 64}
        },
    )

    inserted = 1
    for i in range(1, len(all_docs), batch_size):
        batch = all_docs[i:i + batch_size]
        try:
            milvus.add_documents(batch)
            inserted += len(batch)
            print(f"âœ… æ’å…¥ batch {i // batch_size + 1}: {len(batch)} æ¡")
        except Exception as e:
            print(f"âŒ æ’å…¥ batch å¤±è´¥: {e}")
    return inserted

# ======================== æ’å…¥ ES ========================
def insert_block_to_es(doc_meta_list, es_index_name) -> int:
    es = get_es()
    print(f"ğŸ“¥ æ­£åœ¨æ’å…¥æ–‡æ¡£åˆ° Elasticsearch ç´¢å¼•: {es_index_name} ...")

    actions = []
    for doc in doc_meta_list:
        doc.setdefault("file_idx", -1)
        actions.append({
            "_index": es_index_name,
            "_source": {
                "file_idx": doc["file_idx"],
                "chunk_idx": doc["chunk_idx"],
                "title": doc["title"],
                "summary": doc.get("summary", ""),
                "text": doc["text"],
                "page_url": doc["page_url"],
                "page_name": doc["page_name"],
                "time": doc.get("time", ""),
                "question": doc.get("question", "")
            }
        })

    try:
        resp = helpers.bulk(es, actions)
        print(f"âœ… å·²æ’å…¥ ESï¼š{resp[0]} æ¡æ–‡æ¡£å—")
        return resp[0]  # æˆåŠŸæ•°
    except Exception as e:
        print(f"âŒ ES æ’å…¥å¤±è´¥: {e}")
        return 0

# ======================== åˆ é™¤ Milvus ä¸­çš„æ–‡æ¡£å— ========================
def delete_blocks_from_milvus(collection_name, file_idx) -> int:
    try:
        col = get_milvus_collection(collection_name)
        expr = f"file_idx == {file_idx}"
        count = col.num_entities  # åˆ é™¤å‰å®ä½“æ€»æ•°ï¼ˆå¯èƒ½éä¸¥æ ¼å¯¹åº”ï¼‰
        result = col.delete(expr)
        print(f"ğŸ—‘ï¸ Milvus: å·²åˆ é™¤ file_idx = {file_idx} çš„æ–‡æ¡£å—")
        return result.delete_count if hasattr(result, "delete_count") else 0
    except Exception as e:
        print(f"âŒ Milvus åˆ é™¤å¤±è´¥: {e}")
        return 0

# ======================== åˆ é™¤ ES ä¸­çš„æ–‡æ¡£å— ========================
def delete_blocks_from_es(index_name, file_idx) -> int:
    try:
        es = get_es()
        query = {
            "query": {
                "term": {
                    "file_idx": file_idx
                }
            }
        }

        hits = es.search(index=index_name, body=query, size=10000)["hits"]["hits"]
        if not hits:
            print(f"ğŸ“­ ES: æœªæ‰¾åˆ° file_idx = {file_idx} çš„æ–‡æ¡£")
            return 0

        resp = helpers.bulk(
            client=es,
            actions=[
                {"_op_type": "delete", "_index": index_name, "_id": hit["_id"]}
                for hit in hits
            ]
        )
        print(f"ğŸ—‘ï¸ ES: å·²åˆ é™¤ file_idx = {file_idx} çš„æ–‡æ¡£å—ï¼Œå…± {resp[0]} æ¡")
        return resp[0]
    except Exception as e:
        print(f"âŒ ES åˆ é™¤å¤±è´¥: {e}")
        return 0

# ======================== Milvus æŸ¥è¯¢å‡½æ•° ========================
def query_milvus_blocks(
    host,
    question,
    embedder,
    reranker=None,
    milvus_collection_name="jvliangqianchuan",
    top_k=10,
    rerank_top_k=5
):

    # print("ğŸ“¦ Connecting to Milvus ...")
    connections.connect(alias="default", host=host, port="19530")
    collection = Collection(name=milvus_collection_name)
    if not collection.has_index():
        print(f"âš™ï¸ Creating index on 'vector' ...")
        collection.create_index(
            field_name="vector",
            index_params={
                "index_type": "IVF_FLAT",
                "metric_type": "COSINE",
                "params": {"nlist": 64},
            },
            index_name="default",
        )
        print("âœ… Index created.")
    collection.load()
    query_vec = embedder.embed_query(question)
    # print(f"âœ… æŸ¥è¯¢å‘é‡ç»´åº¦: {len(query_vec)}, èŒƒæ•°: {np.linalg.norm(query_vec):.4f}")

    # print("ğŸ” Searching Milvus ...")
    results = collection.search(
        data=[query_vec],
        anns_field="vector",
        param={"metric_type": "COSINE", "params": {"nprobe": 100}},
        limit=top_k,
        output_fields=["text", "page_url", "chunk_idx", "page_name", "title", "summary", "time", "question", 'file_idx'],
    )
    # print(results)

    milvus_rank = []
    for hits in results:
        for hit in hits:
            milvus_rank.append({
                "chunk_idx": hit.entity.get("chunk_idx", -1),
                "page_url": hit.entity.get("page_url", "unknown"),
                "page_name": hit.entity.get("page_name", "none"),
                "title": hit.entity.get("title", "none"),
                "summary": hit.entity.get("summary", ""),
                "time": hit.entity.get("time", ""),
                "text": hit.entity.get("text", ""),
                "question": hit.entity.get("question", ""),
            })

    # print(f"ğŸ” Milvus åˆå§‹è¿”å›æ•°é‡: {len(milvus_rank)}")
    # milvus_rank = deduplicate_ranked_blocks(milvus_rank)
    # print(f"âœ… å»é‡åä¿ç•™æ•°é‡: {len(milvus_rank)}")

    # if reranker is not None:
    #     milvus_rank = rerank_results(milvus_rank, question, reranker, rerank_top_k)

    # print("=" * 60)
    # print("[Milvus] Top Results:")
    # for i, doc in enumerate(milvus_rank):
    #     print(f"#{i+1} ğŸ”¸ Chunk ID: {doc['chunk_idx']} | summary: {doc['summary']}")
    #     print("-" * 100)
    # print("=" * 60)

    return milvus_rank


# ======================== ES æŸ¥è¯¢å‡½æ•° ========================
def query_es_blocks(
    host,
    question,
    es_index_name="jvliangqianchuan",
    top_k=10
):
    es = Elasticsearch(f"http://{host}:9200")
    
    keywords = jieba.analyse.extract_tags(question, topK=8, withWeight=True)
    keywords = [_[0] for _ in keywords if _[1] > 0.0]
    # print("Keywords:", keywords)

    fields_config = {"text": {"boost": 1}, "title": {"boost": 2}}
    query = build_optimal_jieba_query(keywords, fields_config)
    es_response = es.search(index=es_index_name, query=query.get("query", {}), size=top_k)

    es_rank = [
        {
            "file_idx": hit["_source"].get("file_idx", -1),
            "chunk_idx": hit["_source"].get("chunk_idx", -1),
            "page_url": hit["_source"].get("page_url", "unknown"),
            "page_name": hit["_source"].get("page_name", "none"),
            "title": hit["_source"].get("title", "none"),
            "summary": hit["_source"].get("summary", ""),
            "time": hit["_source"].get("time", ""),
            "question": hit["_source"].get("question", ""),
            "text": hit["_source"].get("text", ""),
        } for hit in es_response["hits"]["hits"]
    ]
    # print(f"ğŸ” ES åˆå§‹è¿”å›æ•°é‡: {len(es_rank)}")
    # milvus_rank = deduplicate_ranked_blocks(es_rank)
    # print(f"âœ… å»é‡åä¿ç•™æ•°é‡: {len(es_rank)}")
    return es_rank



# ======================== å¤šæºæŸ¥è¯¢å‡½æ•° ========================
def query_blocks(
    question,
    embedder,
    host="localhost",
    milvus_collection_name="jvliangqianchuan",
    es_index_name="jvliangqianchuan",
    top_k=10,
    reranker=None,
    rerank_top_k=5
):
    # 1. æŸ¥è¯¢ Milvus
    milvus_raw = query_milvus_blocks(
        host=host,
        question=question,
        embedder=embedder,
        reranker=None,
        milvus_collection_name=milvus_collection_name,
        top_k=top_k,
        rerank_top_k=rerank_top_k
    )

    # 2. æŸ¥è¯¢ ES
    es_raw = query_es_blocks(
        host=host,
        question=question,
        es_index_name=es_index_name,
        top_k=top_k
    )

    print(f"ğŸ“¥ åˆå¹¶å‰: Milvus={len(milvus_raw)}, ES={len(es_raw)}")

    # 3. æ—¶é—´ä¼˜å…ˆå»é‡ï¼šå…ˆå†…éƒ¨å†äº¤å‰ï¼ˆMilvus against ESï¼‰
    # final_blocks = deduplicate_ranked_blocks_pal(milvus_raw + es_raw)
    final_blocks = milvus_raw + es_raw
    # 4. å¯é€‰é‡æ’åº
    if reranker is not None:
        final_blocks = rerank_results(final_blocks, question, reranker, top_k=rerank_top_k)

    print(f"âœ… æœ€ç»ˆè¿”å›æ–‡æ¡£å—æ•°: {len(final_blocks)}")
    print("ğŸ“¦ è¿”å›çš„æ–‡æ¡£å—ç¤ºä¾‹:")
    for i, doc in enumerate(final_blocks[:5]):
        # print(doc)
        print(f"  [#{i+1}] file_idx={doc.get('file_idx',-1)}  page_url={doc['page_url']:<30} chunk_idx={doc['chunk_idx']:<4} title={doc['title'][:30]:<30}")
    return final_blocks


# ======================== Reranker å‡½æ•° ========================
class Reranker:
    def __init__(self, model, tokenizer, device):
        self.model = model
        self.tokenizer = tokenizer
        self.device = device

    def compute_score_pairs(self, pairs):
        inputs = self.tokenizer(
            [q for q, a in pairs], [a for q, a in pairs],
            padding=True, truncation=True, return_tensors="pt"
        ).to(self.device)
        with torch.no_grad():
            scores = self.model(**inputs).logits.squeeze(-1)
        return scores.cpu().tolist()

def rerank_results(docs, query, reranker, top_k):
    """
    ä½¿ç”¨ reranker å¯¹æ£€ç´¢ç»“æœè¿›è¡Œç²¾æ’
    """
    print("ğŸ” Running Reranker...")
    texts = [d["text"] for d in docs]
    pairs = [(query, t) for t in texts]
    scores = reranker.compute_score_pairs(pairs)

    print("ğŸ“Š åŸå§‹é¡ºåºåŠåˆ†æ•°:")
    for i, (doc, score) in enumerate(zip(docs, scores)):
        print(f"  [#{i+1}] chunk_idx={doc['chunk_idx']:<4} summary={doc['summary'][:30]:<30} score={score:.4f}")

    reranked = sorted(zip(docs, scores), key=lambda x: x[1], reverse=True)
    reranked_docs = [x[0] for x in reranked[:top_k]]

    print("\nğŸ† Reranked Top Results:")
    for i, (doc, score) in enumerate(reranked[:top_k]):
        print(f"  [#{i+1}] chunk_idx={doc['chunk_idx']:<4} summary={doc['summary'][:30]:<30} score={score:.4f}")

    print(f"âœ… ç²¾æ’å®Œæˆï¼Œé€‰å–å‰ {top_k} æ¡")
    return reranked_docs



