# -*- coding: utf-8 -*-
"""
HTML æ–‡æ¡£ç´¢å¼•ä¸å¤šæ¨¡æ€æ£€ç´¢æ ¸å¿ƒæ¨¡å—

æœ¬æ¨¡å—æä¾›é¢å‘çŸ¥è¯†å‹é—®ç­”ç³»ç»Ÿï¼ˆå¦‚ RAGï¼‰çš„ä¸€ä½“åŒ–å‘é‡ç´¢å¼•ç®¡ç†ä¸æŸ¥è¯¢èƒ½åŠ›ï¼Œæ”¯æŒåŸºäº Milvus å’Œ Elasticsearch çš„åŒæ¨¡æ£€ç´¢æ–¹æ¡ˆï¼Œ
æ¶µç›–å‘é‡ç´¢å¼•æ„å»ºã€æ–‡æ¡£å—æ’å…¥ä¸åˆ é™¤ã€ç´¢å¼•é‡å»ºã€æ–‡æœ¬æ‘˜è¦ä¸è¯­ä¹‰å»é‡ã€å¤šæºèåˆæ£€ç´¢ä¸ reranker ç²¾æ’ç­‰åŠŸèƒ½ã€‚

æ¨¡å—åŠŸèƒ½æ¦‚è§ˆï¼š
------------------------------------------------
1. å‘é‡åº“ï¼ˆMilvusï¼‰ç®¡ç†ï¼š
   - `reset_milvus`: é‡å»º Milvus collectionï¼Œæ”¯æŒä¸»é”®è‡ªå¢ä¸å¤šå­—æ®µç»“æ„ã€‚
   - `insert_block_to_milvus`: å‘é‡å—æ’å…¥ï¼Œæ”¯æŒæ‰¹é‡å†™å…¥ä¸åµŒå…¥ç”Ÿæˆã€‚
   - `delete_blocks_from_milvus`: æŒ‰ document_index åˆ é™¤ Milvus å‘é‡ã€‚
   - `query_milvus_blocks`: åŸºäºè¯­ä¹‰å‘é‡è¿›è¡Œ ANN æ£€ç´¢ï¼Œæ”¯æŒ reranker ç²¾æ’ã€‚

2. å…³é”®è¯åº“ï¼ˆElasticsearchï¼‰ç®¡ç†ï¼š
   - `reset_es`: é‡å»º Elasticsearch ç´¢å¼•ç»“æ„ï¼Œä½¿ç”¨ IK åˆ†è¯å™¨è¿›è¡Œä¸­æ–‡ä¼˜åŒ–ã€‚
   - `insert_block_to_es`: æ‰¹é‡æ’å…¥æ–‡æ¡£å—æ–‡æœ¬è‡³ Elasticsearchã€‚
   - `delete_blocks_from_es`: æŒ‰ document_index åˆ é™¤ ES æ–‡æ¡£å—ã€‚
   - `query_es_blocks`: åŸºäºå…³é”®è¯æŠ½å–æ„å»ºæŸ¥è¯¢è¯­å¥ï¼Œæ‰§è¡Œå€’æ’æ£€ç´¢ã€‚

3. å¤šæºèåˆæ£€ç´¢ä¸å»é‡ï¼š
   - `query_blocks`: åŒæ—¶ä» Milvus ä¸ ES æ£€ç´¢æ–‡æ¡£å—ï¼Œæ”¯æŒæ—¶é—´ä¼˜å…ˆçš„å»é‡ç­–ç•¥ä¸ reranker ç²¾æ’é€»è¾‘ã€‚

4. reranker æ¨¡å‹ç²¾æ’ï¼š
   - `Reranker`: ä½¿ç”¨ transformer æ¨¡å‹å¯¹å€™é€‰å—è¿›è¡Œ query-passage ç²¾æ’ã€‚
   - `rerank_results`: å¯¹å€™é€‰æ–‡æ¡£å—æŒ‰ç…§ relevance åˆ†æ•°é‡æ–°æ’åºã€‚

ä¾èµ–ç¯å¢ƒä¸é…ç½®è¯´æ˜ï¼š
------------------------------------------------
- Milvus >= 2.xï¼ˆéœ€é¢„å¯åŠ¨æœåŠ¡ï¼Œç›‘å¬ 19530 ç«¯å£ï¼‰
- Elasticsearch >= 7.xï¼ˆéœ€å¯ç”¨ IK åˆ†è¯å™¨ï¼‰
- LangChainã€PyMilvusã€transformersã€torch ç­‰
- å¤–éƒ¨ä¾èµ–é…ç½®é€šè¿‡ utils/config.py æ³¨å…¥ï¼šå¦‚ milvus_host, es_host, index_name

"""

import os
import jieba
import numpy as np
from elasticsearch import Elasticsearch
from elasticsearch.helpers import bulk
from langchain_core.documents import Document
from langchain_milvus import Milvus
from pymilvus import (
    connections,
    utility,
    CollectionSchema,
    FieldSchema,
    DataType,
    Collection,
)
import torch
from utils.text_process_utils import build_optimal_jieba_query, deduplicate_ranked_blocks_pal
from time import sleep
from langchain_community.vectorstores import Milvus
from langchain.schema import Document
from time import sleep
from elasticsearch import Elasticsearch, helpers
import jieba.analyse
from utils.config import CONFIG, logger
from utils.llm_api import get_embeddings_from_vllm_async, get_embedding_from_vllm
import numpy as np
from typing import List


# åŠ è½½è‡ªå®šä¹‰è¯å…¸ï¼ˆé€‚ç”¨äºç”µå•†/è¿è¥åœºæ™¯ï¼‰
jieba.load_userdict("./user_dict.txt")

# å…¨å±€è¿æ¥æ± 
_es_clients = {}
_milvus_alias_map = {}  # å­˜å‚¨å·²è¿æ¥çš„åˆ«å
_milvus_collection_cache = {}  # å­˜å‚¨å·²åˆ›å»ºå¹¶åŠ è½½çš„ Collection å¯¹è±¡


def get_env_config(env=None):
    """è·å–æŒ‡å®šç¯å¢ƒä¸‹çš„å®Œæ•´é…ç½®é¡¹"""
    env = env or os.getenv("RAG_ENV") or CONFIG.get("env_default", "dev")
    env_cfg = CONFIG.get("env_config", {}).get(env)
    if not env_cfg:
        raise ValueError(f"âŒ æœªæ‰¾åˆ°ç¯å¢ƒé…ç½®: {env}")
    return env, env_cfg

def get_es(env=None):
    env, env_cfg = get_env_config(env)
    
    if env not in _es_clients:
        logger.debug(f"ğŸ”Œ åˆå§‹åŒ– ES è¿æ¥ [{env}]ï¼š{env_cfg['es_host']}")
    if "es_user" in env_cfg and "es_password" in env_cfg:
        es = Elasticsearch(
            hosts=[f"http://{env_cfg['es_host']}:9200"],
            basic_auth=(env_cfg["es_user"], env_cfg["es_password"]),
            request_timeout=30
        )
        _es_clients[env] = es
    else:
        _es_clients[env] = Elasticsearch(f"http://{env_cfg['es_host']}:9200")


    return _es_clients[env]


# def get_milvus_collection(env=None):
#     """
#     è·å–æŒ‡å®šç¯å¢ƒä¸‹çš„ Milvus collectionï¼Œä½¿ç”¨ç¼“å­˜æœºåˆ¶é¿å…é‡å¤åˆ›å»ºå’ŒåŠ è½½ã€‚
#     """
#     env, env_cfg = get_env_config(env)
#     alias = env  # ä½¿ç”¨ç¯å¢ƒåä½œä¸ºè¿æ¥åˆ«å
#     collection_name = env_cfg["collection_name"]
#     print(collection_name)
    
#     # æ„å»ºç¼“å­˜é”®
#     cache_key = f"{alias}_{collection_name}"
    
#     # æ£€æŸ¥è¿æ¥æ˜¯å¦å·²å­˜åœ¨ï¼Œä¸å­˜åœ¨åˆ™åˆ›å»º
#     if alias not in _milvus_alias_map:
#         logger.debug(f"ğŸ”Œ åˆå§‹åŒ– Milvus è¿æ¥ [{env}]ï¼š{env_cfg['milvus_host']}")
#         connections.connect(alias=alias, host=env_cfg["milvus_host"], port="19530")
#         _milvus_alias_map[alias] = True
    
#     # æ£€æŸ¥ Collection æ˜¯å¦å·²ç¼“å­˜ï¼Œä¸å­˜åœ¨åˆ™åˆ›å»ºå¹¶åŠ è½½
#     if cache_key not in _milvus_collection_cache:
#         logger.debug(f"ğŸ“š åŠ è½½ Milvus Collection: {collection_name}")
#         col = Collection(name=collection_name, using=alias)
#         col.load()
#         _milvus_collection_cache[cache_key] = col
    
#     return _milvus_collection_cache[cache_key]




def get_milvus_collection(env=None):
    env, env_cfg = get_env_config(env)
    alias = env
    collection_name = env_cfg["collection_name"]
    print(f"ğŸ“ æ­£åœ¨åŠ è½½ Collection: {collection_name}")
    cache_key = f"{alias}_{collection_name}"
    
    if alias not in _milvus_alias_map:
        logger.debug(f"ğŸ”Œ åˆå§‹åŒ– Milvus è¿æ¥ [{env}]ï¼š{env_cfg['milvus_host']}")
        connect_params = {
            "alias": alias,
            "host": env_cfg["milvus_host"],
            "port": "19530",
            "secure": False
        }
        if "milvus_user" in env_cfg and "milvus_password" in env_cfg:
            connect_params["user"] = env_cfg["milvus_user"]
            connect_params["password"] = env_cfg["milvus_password"]

        connections.connect(**connect_params)
        _milvus_alias_map[alias] = True

    if cache_key not in _milvus_collection_cache:
        logger.debug(f"ğŸ“š åŠ è½½ Milvus Collection: {collection_name}")
        col = Collection(name=collection_name, using=alias)
        col.load()
        _milvus_collection_cache[cache_key] = col
    
    return _milvus_collection_cache[cache_key]



def get_index_name(env=None):
    """è·å–å½“å‰ç¯å¢ƒçš„ Elasticsearch ç´¢å¼•å"""
    _, env_cfg = get_env_config(env)
    return env_cfg["index_name"]

# ======================== è·å–æœ€å¤§å…¨å±€ç´¢å¼•(milvus) ========================
def get_max_global_idx_milvus(env="dev"):
    """è·å–å½“å‰ç¯å¢ƒä¸‹ Milvus ä¸­ global_chunk_idx çš„æœ€å¤§å€¼"""
    try:
        _, cfg = get_env_config(env)
        alias = env
        host = cfg["milvus_host"]
        collection_name = cfg["collection_name"]

        # åˆå§‹åŒ–è¿æ¥ï¼ˆå¦‚æœæœªè¿æ¥ï¼‰
        if not connections.has_connection(alias):
            connections.connect(alias=alias, host=host, port="19530")

        collection = Collection(name=collection_name, using=alias)
        collection.load()

        results = collection.query(
            expr="global_chunk_idx >= 0",
            output_fields=["global_chunk_idx"],
        )
        if not results:
            return 0
        max_idx = max(item["global_chunk_idx"] for item in results)
        return max_idx + 1
    except Exception as e:
        print(f"âš ï¸ æŸ¥è¯¢å¤±è´¥: {e}")
        return 0


# ======================== ES ç´¢å¼•é‡å»º ========================
def reset_es(env="dev"):
    """é‡å»º Elasticsearch ç´¢å¼•ï¼ˆå« ik åˆ†è¯ä¸å­—æ®µæ˜ å°„ï¼‰"""
    _, cfg = get_env_config(env)
    index_name = cfg["index_name"]
    es = get_es(env)

    logger.info("Connected to Elasticsearch!" if es.ping() else "Connection failed.")

    if es.indices.exists(index=index_name):
        logger.info(f"âš ï¸ {env} ç¯å¢ƒç´¢å¼• {index_name} å·²å­˜åœ¨ï¼Œåˆ é™¤ä¸­...")
        es.indices.delete(index=index_name, ignore_unavailable=True, request_timeout=20)

    # es.indices.create(
    #     index=index_name,
    #     body={
    #         "settings": {
    #             "analysis": {
    #                 "analyzer": {
    #                     "ik_max_word": {
    #                         "type": "custom",
    #                         "tokenizer": "ik_max_word"
    #                     }
    #                 }
    #             }
    #         },
    #         "mappings": {
    #             "properties": {
    #                 "document_index": { "type": "long" },
    #                 "chunk_idx": { "type": "integer" },
    #                 "text": {
    #                     "type": "text",
    #                     "analyzer": "ik_max_word",
    #                     "fields": { "keyword": { "type": "keyword" } }
    #                 },
    #                 "page_url": {
    #                     "type": "text",
    #                     "fields": { "keyword": { "type": "keyword" } }
    #                 },
    #                 "page_name": {
    #                     "type": "text",
    #                     "analyzer": "ik_max_word",
    #                     "fields": { "keyword": { "type": "keyword" } }
    #                 },
    #                 "title": {
    #                     "type": "text",
    #                     "analyzer": "ik_max_word",
    #                     "fields": { "keyword": { "type": "keyword" } }
    #                 },
    #                 "summary": {
    #                     "type": "text",
    #                     "analyzer": "ik_max_word",
    #                     "fields": { "keyword": { "type": "keyword" } }
    #                 },
    #                 "time": {
    #                     "type": "text",
    #                     "fields": { "keyword": { "type": "keyword" } }
    #                 },
    #                 "question": {
    #                     "type": "text",
    #                     "analyzer": "ik_max_word",
    #                     "fields": { "keyword": { "type": "keyword" } }
    #                 }
    #             }
    #         }
    #     },
    # )
    es.indices.create(
    index=index_name,
    body={
        "mappings": {
            "properties": {
                "document_index": { "type": "long" },
                "chunk_idx": { "type": "integer" },
                "text": {
                    "type": "text",
                    "fields": { "keyword": { "type": "keyword" } }
                },
                "page_url": {
                    "type": "text",
                    "fields": { "keyword": { "type": "keyword" } }
                },
                "page_name": {
                    "type": "text",
                    "fields": { "keyword": { "type": "keyword" } }
                },
                "title": {
                    "type": "text",
                    "fields": { "keyword": { "type": "keyword" } }
                },
                "summary": {
                    "type": "text",
                    "fields": { "keyword": { "type": "keyword" } }
                },
                "time": {
                    "type": "text",
                    "fields": { "keyword": { "type": "keyword" } }
                },
                "question": {
                    "type": "text",
                    "fields": { "keyword": { "type": "keyword" } }
                }
                }
            }
        },
    )

    logger.info(f"âœ… {env} ç¯å¢ƒ ES ç´¢å¼• '{index_name}' å·²æˆåŠŸåˆ›å»º")


# ======================== Milvus å‘é‡åº“é‡å»º ========================
def reset_milvus(env="dev", dim=768):
    """é‡å»º Milvus å‘é‡é›†åˆï¼ˆè‡ªåŠ¨è·å– collection_nameï¼‰"""
    _, cfg = get_env_config(env)
    collection_name = cfg["collection_name"]
    host = cfg["milvus_host"]
    alias = env

    if not connections.has_connection(alias):
        connections.connect(alias=alias, host=host, port="19530")

    if utility.has_collection(collection_name, using=alias):
        logger.info(f"âš ï¸ {env} ç¯å¢ƒ Milvus é›†åˆ '{collection_name}' å·²å­˜åœ¨ï¼Œæ­£åœ¨åˆ é™¤...")
        utility.drop_collection(collection_name, using=alias)

    logger.info(f"ğŸš€ {env} ç¯å¢ƒæ­£åœ¨åˆ›å»º Milvus collection: {collection_name}")
    fields = [
        FieldSchema(name="global_chunk_idx", dtype=DataType.INT64, is_primary=True, auto_id=True),
        FieldSchema(name="document_index", dtype=DataType.INT64),
        FieldSchema(name="chunk_idx", dtype=DataType.INT64),
        FieldSchema(name="vector", dtype=DataType.FLOAT_VECTOR, dim=dim),
        FieldSchema(name="text", dtype=DataType.VARCHAR, max_length=20000),
        FieldSchema(name="page_url", dtype=DataType.VARCHAR, max_length=1024),
        FieldSchema(name="page_name", dtype=DataType.VARCHAR, max_length=512),
        FieldSchema(name="title", dtype=DataType.VARCHAR, max_length=512),
        FieldSchema(name="summary", dtype=DataType.VARCHAR, max_length=4096),
        FieldSchema(name="time", dtype=DataType.VARCHAR, max_length=128),
        FieldSchema(name="question", dtype=DataType.VARCHAR, max_length=1024),
    ]
    schema = CollectionSchema(fields=fields, description="HTMLå—å‘é‡ç´¢å¼•")
    Collection(name=collection_name, schema=schema, using=alias)
    logger.info(f"âœ… {env} ç¯å¢ƒ Milvus collection '{collection_name}' å·²åˆ›å»º")

# ======================== æ’å…¥ Milvus ========================
def insert_block_to_milvus(doc_meta_list, embedder, env="dev", batch_size=2) -> int:
    """
    å‘æŒ‡å®šç¯å¢ƒçš„ Milvus æ’å…¥æ–‡æ¡£å—ï¼Œè‡ªåŠ¨è·å– collection_name å’Œ host
    """
    _, cfg = get_env_config(env)
    collection_name = cfg["collection_name"]
    host = cfg["milvus_host"]

    all_docs = []
    for doc in doc_meta_list:
        doc.setdefault("document_index", -1)
        node = Document(
            page_content=doc["text"],
            metadata={k: v for k, v in doc.items() if k != "text"}
        )
        all_docs.append(node)

    logger.debug(f"ğŸ§  æ­£åœ¨æ’å…¥å‘é‡åˆ° Milvusï¼ˆ{env}ï¼‰collection: {collection_name} ...")

    # åˆå§‹åŒ– Milvus å‘é‡åº“å¯¹è±¡ï¼ˆfrom_documents ä¼šè‡ªåŠ¨å»ºè¡¨ï¼Œä½†æˆ‘ä»¬å»ºè®®å…ˆ resetï¼‰
    milvus = Milvus.from_documents(
        [all_docs[0]],  # ç”¨ç¬¬ä¸€æ¡åˆå§‹åŒ– collection
        embedder,
        collection_name=collection_name,
        connection_args={
                "host": host,
                "port": "19530",
                "user": cfg.get("milvus_user", ""),
                "password": cfg.get("milvus_password", ""),
                "secure": False  # å¦‚æœä½ æ²¡å¯ç”¨TLSï¼Œä¸€å®šè®¾ä¸º Falseï¼
            },
        index_params={
            "metric_type": "COSINE",
            "index_type": "IVF_FLAT",
            "params": {"nlist": 64}
        },
    )
    


    inserted = 1
    for i in range(1, len(all_docs), batch_size):
        batch = all_docs[i:i + batch_size]
        try:
            milvus.add_documents(batch)
            inserted += len(batch)
            logger.debug(f"âœ… æ’å…¥ batch {i // batch_size + 1}: {len(batch)} æ¡")
        except Exception as e:
            logger.error(f"âŒ æ’å…¥ batch å¤±è´¥: {e}")

    logger.debug(f"âœ… å·²æ’å…¥ Milvusï¼ˆ{env}ï¼‰ï¼š{inserted} æ¡æ–‡æ¡£å—")
    return inserted


async def insert_blocks_to_milvus_vllm_async(
    doc_meta_list: List[dict],
    url: str,
    env: str = "dev",
    batch_size: int = 2,
):
    _, cfg = get_env_config(env)
    collection = get_milvus_collection(env)
    logger.debug(f"ğŸ§  å‡†å¤‡æ’å…¥ {len(doc_meta_list)} æ¡æ–‡æ¡£å— åˆ° Milvus[{env}]ï¼š{cfg['collection_name']}")

    total_inserted = 0
    N = len(doc_meta_list)

    for start in range(0, N, batch_size):
        batch = doc_meta_list[start : start + batch_size]

        try:
            # å…ˆæ”¶é›†ä¸€æ‰¹ç”¨äº VLLM è·å– embedding çš„æ–‡æœ¬ï¼ˆæ¯”å¦‚ title æˆ– questionï¼‰
            texts_for_embedding = [doc.get("text", "") for doc in batch]
            embeddings = await get_embeddings_from_vllm_async(texts_for_embedding, url)

            # æ„é€ æ¯åˆ—
            document_index =   [doc.get("document_index", -1) for doc in batch]
            chunk_idx =        [doc.get("chunk_idx", -1) for doc in batch]
            vector =           embeddings
            text =             [doc.get("text", "") for doc in batch]
            page_url =         [doc.get("page_url", "") for doc in batch]
            page_name =        [doc.get("page_name", "") for doc in batch]
            title =            [doc.get("title", "") for doc in batch]
            summary =          [doc.get("summary", "") for doc in batch]
            time =             [doc.get("time", "") for doc in batch]
            question =         [doc.get("question", "") for doc in batch]

            milvus_data = [
                document_index,
                chunk_idx,
                vector,
                text,
                page_url,
                page_name,
                title,
                summary,
                time,
                question,
            ]

            collection.insert(milvus_data)
            logger.debug(f"âœ… Batch {start//batch_size + 1} æ’å…¥æˆåŠŸï¼š{len(batch)} æ¡")
            total_inserted += len(batch)

        except Exception as e:
            logger.error(f"âŒ Batch {start//batch_size + 1} æ’å…¥å¤±è´¥ï¼š{e}")

    logger.info(f"ğŸš€ æ’å…¥å®Œæˆï¼Œå…±æˆåŠŸæ’å…¥ {total_inserted} æ¡")
    return total_inserted



def insert_blocks_to_milvus_vllm(
    doc_meta_list: List[dict],
    url: str,
    env: str = "dev",
):
    """å°†æ–‡æ¡£å—é€æ¡æ’å…¥ Milvus"""
    _, cfg = get_env_config(env)
    collection = get_milvus_collection(env)
    logger.debug(f"ğŸ§  å‡†å¤‡æ’å…¥ {len(doc_meta_list)} æ¡æ–‡æ¡£å— åˆ° Milvus[{env}]ï¼š{cfg['collection_name']}")

    success_count = 0
    failed_count = 0
    failed_indices = []

    for idx, doc in enumerate(doc_meta_list):
        try:
            # è·å–æ–‡æœ¬åµŒå…¥å‘é‡
            text = doc.get("text", "")
            embeddings = get_embedding_from_vllm(text, url)  # åŒæ­¥ç‰ˆæœ¬çš„åµŒå…¥å‡½æ•°
            
            # æ„é€ æ’å…¥æ•°æ®
            milvus_data = [
                [doc.get("document_index", -1)],
                [doc.get("chunk_idx", -1)],
                [embeddings[0]],  # å–ç¬¬ä¸€ä¸ªåµŒå…¥å‘é‡
                [text],
                [doc.get("page_url", "")],
                [doc.get("page_name", "")],
                [doc.get("title", "")],
                [doc.get("summary", "")],
                [doc.get("time", "")],
                [doc.get("question", "")],
            ]
            
            # å•æ¡æ’å…¥
            collection.insert(milvus_data)
            success_count += 1
            logger.debug(f"âœ… æ’å…¥æˆåŠŸï¼šæ–‡æ¡£ç´¢å¼• {doc.get('document_index', -1)}ï¼Œå—ç´¢å¼• {doc.get('chunk_idx', -1)}")
            
        except Exception as e:
            failed_count += 1
            failed_indices.append(idx)
            logger.error(f"âŒ æ’å…¥å¤±è´¥ï¼šæ–‡æ¡£ç´¢å¼• {doc.get('document_index', -1)}ï¼Œå—ç´¢å¼• {doc.get('chunk_idx', -1)}ï¼Œé”™è¯¯ï¼š{e}")
    
    logger.info(f"ğŸš€ æ’å…¥å®Œæˆï¼ŒæˆåŠŸ {success_count} æ¡ï¼Œå¤±è´¥ {failed_count} æ¡")
    
    if failed_count > 0:
        logger.warning(f"âš ï¸ ä»¥ä¸‹ç´¢å¼•çš„æ–‡æ¡£æ’å…¥å¤±è´¥ï¼š{failed_indices}")
    
    return success_count




# ======================== æ’å…¥ ES ========================
def insert_block_to_es(doc_meta_list, env="dev") -> int:
    """
    å‘æŒ‡å®šç¯å¢ƒçš„ Elasticsearch ç´¢å¼•æ’å…¥æ–‡æ¡£å—ï¼Œè‡ªåŠ¨è·å– index_name
    """
    _, cfg = get_env_config(env)
    index_name = cfg["index_name"]
    es = get_es(env)

    logger.debug(f"ğŸ“¥ æ­£åœ¨æ’å…¥æ–‡æ¡£åˆ° Elasticsearch ç´¢å¼•ï¼ˆ{env}ï¼‰: {index_name} ...")

    actions = []
    for doc in doc_meta_list:
        if "document_index" not in doc:
            doc["document_index"] = -1
        actions.append({
            "_index": index_name,
            "_source": {
                "document_index": doc["document_index"],
                "chunk_idx": doc["chunk_idx"],
                "title": doc["title"],
                "summary": doc.get("summary", ""),
                "text": doc["text"],
                "page_url": doc["page_url"],
                "page_name": doc["page_name"],
                "time": doc.get("time", ""),
                "question": doc.get("question", "")
            }
        })

    try:
        resp = helpers.bulk(es, actions)
        logger.debug(f"âœ… å·²æ’å…¥ ESï¼ˆ{env}ï¼‰ï¼š{resp[0]} æ¡æ–‡æ¡£å—")
        return resp[0]
    except Exception as e:
        logger.error(f"âŒ ES æ’å…¥å¤±è´¥ï¼ˆ{env}ï¼‰: {e}")
        return 0




# ======================== åˆ é™¤ Milvus ä¸­çš„æ–‡æ¡£å— ========================
def delete_blocks_from_milvus(document_index: int, env: str = "dev") -> int:
    """ä»æŒ‡å®šç¯å¢ƒä¸‹çš„ Milvus collection ä¸­åˆ é™¤æŸä¸ª document_index"""
    try:
        col = get_milvus_collection(env=env)
        expr = f"document_index == {document_index}"
        result = col.delete(expr)
        count = result.delete_count if hasattr(result, "delete_count") else 0
        logger.debug(f"ğŸ—‘ï¸ Milvusï¼ˆ{env}ï¼‰: å·²åˆ é™¤ document_index = {document_index} çš„æ–‡æ¡£å—ï¼Œå…± {count} æ¡")
        return count
    except Exception as e:
        logger.error(f"âŒ Milvus åˆ é™¤å¤±è´¥ï¼ˆ{env}ï¼‰: {e}")
        return 0


# ======================== åˆ é™¤ ES ä¸­çš„æ–‡æ¡£å— ========================
def delete_blocks_from_es(document_index: int, env: str = "dev") -> int:
    """ä»æŒ‡å®šç¯å¢ƒä¸‹çš„ Elasticsearch ç´¢å¼•ä¸­åˆ é™¤æŸä¸ª document_index"""
    try:
        _, cfg = get_env_config(env)
        index_name = cfg["index_name"]
        es = get_es(env)

        query = {
            "query": {
                "term": {
                    "document_index": document_index
                }
            }
        }

        hits = es.search(index=index_name, body=query, size=10000)["hits"]["hits"]
        if not hits:
            logger.debug(f"ğŸ“­ ESï¼ˆ{env}ï¼‰: æœªæ‰¾åˆ° document_index = {document_index} çš„æ–‡æ¡£")
            return 0

        resp = helpers.bulk(
            client=es,
            actions=[
                {"_op_type": "delete", "_index": index_name, "_id": hit["_id"]}
                for hit in hits
            ]
        )
        logger.debug(f"ğŸ—‘ï¸ ESï¼ˆ{env}ï¼‰: å·²åˆ é™¤ document_index = {document_index} çš„æ–‡æ¡£å—ï¼Œå…± {resp[0]} æ¡")
        return resp[0]
    except Exception as e:
        logger.error(f"âŒ ES åˆ é™¤å¤±è´¥ï¼ˆ{env}ï¼‰: {e}")
        return 0



# ======================== Milvus æŸ¥è¯¢å‡½æ•° ========================
def query_milvus_blocks(
    question,
    embedder,
    env="dev",
    top_k=10,
    rerank_top_k=5,
    reranker=None,
):
    _, cfg = get_env_config(env)
    collection_name = cfg["collection_name"]

    collection = get_milvus_collection(env=env)
    if not collection.has_index():
        print(f"âš™ï¸ Creating index on vector ...")
        collection.create_index(
            field_name="vector",
            index_params={
                "index_type": "IVF_FLAT",
                "metric_type": "COSINE",
                "params": {"nlist": 64},
            },
            index_name="default",
        )
        print("âœ… Index created.")
    collection.load()

    query_vec = embedder.embed_query(question)

    results = collection.search(
        data=[query_vec],
        anns_field="vector",
        param={"metric_type": "COSINE", "params": {"nprobe": 100}},
        limit=top_k,
        output_fields=[
            "text", "page_url", "chunk_idx", "page_name",
            "title", "summary", "time", "question", "document_index"
        ],
    )

    milvus_rank = []
    for hits in results:
        for hit in hits:
            milvus_rank.append({
                "chunk_idx": hit.entity.get("chunk_idx", -1),
                "page_url": hit.entity.get("page_url", "unknown"),
                "page_name": hit.entity.get("page_name", "none"),
                "title": hit.entity.get("title", "none"),
                "summary": hit.entity.get("summary", ""),
                "time": hit.entity.get("time", ""),
                "text": hit.entity.get("text", ""),
                "question": hit.entity.get("question", ""),
                "document_index": hit.entity.get("document_index", -1),
            })

    return milvus_rank



# ======================== ES æŸ¥è¯¢å‡½æ•° ========================
def query_es_blocks(
    question,
    env="dev",
    top_k=10
):
    _, cfg = get_env_config(env)
    es = get_es(env)
    index_name = cfg["index_name"]

    keywords = jieba.analyse.extract_tags(question, topK=8, withWeight=True)
    keywords = [_[0] for _ in keywords if _[1] > 0.0]

    fields_config = {"text": {"boost": 1}, "title": {"boost": 2}}
    query = build_optimal_jieba_query(keywords, fields_config)
    es_response = es.search(index=index_name, query=query.get("query", {}), size=top_k)

    es_rank = [
        {
            "document_index": hit["_source"].get("document_index", -1),
            "chunk_idx": hit["_source"].get("chunk_idx", -1),
            "page_url": hit["_source"].get("page_url", "unknown"),
            "page_name": hit["_source"].get("page_name", "none"),
            "title": hit["_source"].get("title", "none"),
            "summary": hit["_source"].get("summary", ""),
            "time": hit["_source"].get("time", ""),
            "question": hit["_source"].get("question", ""),
            "text": hit["_source"].get("text", ""),
        } for hit in es_response["hits"]["hits"]
    ]

    return es_rank





# ======================== å¤šæºæŸ¥è¯¢å‡½æ•° ========================
def query_blocks(
    question,
    embedder,
    env="dev",
    top_k=10,
    reranker=None,
    rerank_top_k=5
):
    # 1. Milvus æŸ¥è¯¢
    milvus_raw = query_milvus_blocks(
        question=question,
        embedder=embedder,
        env=env,
        top_k=top_k,
        reranker=None,
        rerank_top_k=rerank_top_k
    )

    # 2. ES æŸ¥è¯¢
    es_raw = query_es_blocks(
        question=question,
        env=env,
        top_k=top_k
    )

    print(f"ğŸ“¥ åˆå¹¶å‰: Milvus={len(milvus_raw)}, ES={len(es_raw)}")

    # 3. åˆå¹¶å»é‡ï¼ˆç›®å‰ä»…æ‹¼æ¥ï¼Œå¦‚éœ€å¯æ›¿æ¢ä¸º deduplicate_ranked_blocks_palï¼‰
    final_blocks = milvus_raw + es_raw

    # 4. å¯é€‰ rerank
    if reranker is not None:
        final_blocks = rerank_results(final_blocks, question, reranker, top_k=rerank_top_k)

    print(f"âœ… æœ€ç»ˆè¿”å›æ–‡æ¡£å—æ•°: {len(final_blocks)}")
    print("ğŸ“¦ è¿”å›çš„æ–‡æ¡£å—ç¤ºä¾‹:")
    for i, doc in enumerate(final_blocks[:5]):
        print(f"  [#{i+1}] document_index={doc.get('document_index',-1)}  page_url={doc['page_url']:<30} chunk_idx={doc['chunk_idx']:<4} title={doc['title'][:30]:<30}")
    return final_blocks


# ======================== Reranker å‡½æ•° ========================
class Reranker:
    def __init__(self, model, tokenizer, device):
        self.model = model
        self.tokenizer = tokenizer
        self.device = device

    def compute_score_pairs(self, pairs):
        inputs = self.tokenizer(
            [q for q, a in pairs], [a for q, a in pairs],
            padding=True, truncation=True, return_tensors="pt"
        ).to(self.device)
        with torch.no_grad():
            scores = self.model(**inputs).logits.squeeze(-1)
        return scores.cpu().tolist()

def rerank_results(docs, query, reranker, top_k):
    """
    ä½¿ç”¨ reranker å¯¹æ£€ç´¢ç»“æœè¿›è¡Œç²¾æ’
    """
    print("ğŸ” Running Reranker...")
    texts = [d["text"] for d in docs]
    pairs = [(query, t) for t in texts]
    scores = reranker.compute_score_pairs(pairs)

    print("ğŸ“Š åŸå§‹é¡ºåºåŠåˆ†æ•°:")
    for i, (doc, score) in enumerate(zip(docs, scores)):
        print(f"  [#{i+1}] chunk_idx={doc['chunk_idx']:<4} summary={doc['summary'][:30]:<30} score={score:.4f}")

    reranked = sorted(zip(docs, scores), key=lambda x: x[1], reverse=True)
    reranked_docs = [x[0] for x in reranked[:top_k]]

    print("\nğŸ† Reranked Top Results:")
    for i, (doc, score) in enumerate(reranked[:top_k]):
        print(f"  [#{i+1}] chunk_idx={doc['chunk_idx']:<4} summary={doc['summary'][:30]:<30} score={score:.4f}")

    print(f"âœ… ç²¾æ’å®Œæˆï¼Œé€‰å–å‰ {top_k} æ¡")
    return reranked_docs



