# -*- coding: utf-8 -*-
"""
HTML æ–‡æœ¬é«˜é¢‘çŸ­è¯­æå–ä¸åˆ†è¯è¯å…¸ç”Ÿæˆè„šæœ¬

æœ¬æ¨¡å—ç”¨äºæ‰¹é‡æå–ä¸­æ–‡ HTML æ–‡æœ¬ä¸­çš„é«˜é¢‘çŸ­è¯­ï¼Œæ”¯æŒ n-gram ç»Ÿè®¡ã€å¥çº§æˆªæ–­ã€å†—ä½™è¿‡æ»¤ç­‰æ“ä½œï¼Œæœ€ç»ˆç”Ÿæˆé€‚é… `jieba` å’Œ IK Analyzer çš„è‡ªå®šä¹‰è¯å…¸æ–‡ä»¶ã€‚

æ ¸å¿ƒåŠŸèƒ½ï¼š
------------------------------------------------
1. æ–‡æœ¬æå–ä¸æ¸…æ´—ï¼š
   - `extract_text_from_html`: ä»å•ä¸ª HTML æ–‡ä»¶ä¸­æå–çº¯æ–‡æœ¬å†…å®¹ã€‚
   - `collect_all_html_texts`: éå†ç›®å½•ï¼Œæå–æ‰€æœ‰ HTML æ–‡ä»¶æ–‡æœ¬ã€‚

2. é«˜é¢‘çŸ­è¯­æå–ï¼š
   - `extract_phrases_by_frequency`: åŸºäºå¥çº§åˆ‡åˆ†å’Œæ»‘åŠ¨çª—å£æ–¹å¼æå– n-gram è¯ç»„ï¼Œæ”¯æŒé¢‘ç‡ç»Ÿè®¡ä¸é•¿åº¦æ§åˆ¶ã€‚
   - `split_sentences`: åŸºäºä¸­æ–‡æ ‡ç‚¹ç¬¦å·æ–­å¥ï¼Œé¿å…è·¨å¥æ‹¼è¯ã€‚

3. å†—ä½™è¯è¿‡æ»¤ç­–ç•¥ï¼š
   - `filter_keep_longest_only`: ä»…ä¿ç•™æœ€é•¿ä¸é‡å çŸ­è¯­ï¼ˆæŒ‰åŒ…å«å…³ç³»è¿‡æ»¤ï¼‰ã€‚
   - `filter_by_freq_ratio`: è‹¥çŸ­è¯æ˜¯æŸé•¿è¯å­ä¸²ï¼Œä¸”é¢‘ç‡ç›¸è¿‘ï¼Œåˆ™è§†ä¸ºå†—ä½™çŸ­è¯ï¼Œè¿‡æ»¤æ‰ã€‚

4. åˆ†è¯è¯å…¸æ„å»ºä¸ä¿å­˜ï¼š
   - `save_to_jieba_dict`: å°†æå–ç»“æœä¿å­˜ä¸º `jieba` è¯å…¸æ ¼å¼ï¼šè¯è¯­ é¢‘ç‡ è¯æ€§ï¼ˆnï¼‰ã€‚
   - è‡ªåŠ¨ç”Ÿæˆ IK Analyzer æ‰€éœ€çš„çº¯è¯æ¡è¯å…¸æ–‡ä»¶ `my_dict.dic`ã€‚

é…ç½®ä¸ç”¨æ³•ï¼š
------------------------------------------------
- é»˜è®¤å¤„ç†ç›®å½•ä¸º `"å·¨é‡åƒå·çŸ¥è¯†åº“all"`ï¼Œå¯åœ¨ `__main__` ä¸­è‡ªå®šä¹‰ã€‚
- æ”¯æŒæŒ‡å®š `ngram` èŒƒå›´ï¼ˆå¦‚ 2~12ï¼‰ä¸æå–æ•°é‡ä¸Šé™ `top_k`ã€‚
- æœ€ç»ˆè¯å…¸é»˜è®¤ä¿å­˜ä¸ºï¼š
  - `user_dict.txt`: jieba æ ¼å¼ï¼ˆå¯ç”¨äºä¸­æ–‡åˆ†è¯å¢å¼ºï¼‰
  - `my_dict.dic`: IK åˆ†è¯å™¨è¯æ¡å­—å…¸ï¼ˆä»…å«è¯é¡¹ï¼‰

"""



import os
import re
from collections import Counter
from bs4 import BeautifulSoup

def extract_text_from_html(html_path):
    try:
        with open(html_path, 'r', encoding='utf-8') as f:
            soup = BeautifulSoup(f, 'html.parser')
            text = soup.get_text()
            print(f"âœ”ï¸ æˆåŠŸæå–æ–‡æœ¬ï¼š{html_path}ï¼ˆ{len(text)} å­—ï¼‰")
            return text
    except Exception as e:
        print(f"âŒ è¯»å–å¤±è´¥: {html_path}, åŸå› : {e}")
        return ""

def collect_all_html_texts(root_dir):
    texts = []
    print(f"ğŸ” æ­£åœ¨éå†ç›®å½•: {root_dir}")
    for root, _, files in os.walk(root_dir):
        for file in files:
            if file.endswith(".html"):
                full_path = os.path.join(root, file)
                text = extract_text_from_html(full_path)
                if text:
                    texts.append(text)
    print(f"ğŸ“„ å…±æ”¶é›†åˆ° {len(texts)} ä¸ª HTML æ–‡æœ¬")
    return texts

def split_sentences(text):
    # ä¸­æ–‡åŠä¸­è‹±æ ‡ç‚¹æ–­å¥
    return re.split(r"[ã€‚ï¼ï¼Ÿï¼›ï¼Œ,.!?;:ã€\n\r]+", text)

def is_chinese_word(word):
    return bool(word) and all('\u4e00' <= ch <= '\u9fff' for ch in word)


def extract_phrases_by_frequency(texts, ngram_range=(2, 5), top_k=500):
    counter = Counter()
    print("ğŸ“Š æ­£åœ¨è¿›è¡Œ n-gram è¯é¢‘ç»Ÿè®¡ï¼ˆæŒ‰å¥æˆªæ–­ï¼Œçº¯ä¸­æ–‡ï¼‰...")

    for text in texts:
        sentences = split_sentences(text)
        for sentence in sentences:
            sentence = re.sub(r"[^\u4e00-\u9fff]", "", sentence)  # ä»…ä¿ç•™ä¸­æ–‡
            for n in range(ngram_range[0], ngram_range[1] + 1):
                for i in range(len(sentence) - n + 1):
                    gram = sentence[i:i + n]
                    if is_chinese_word(gram):
                        counter[gram] += 1

    top_phrases = counter.most_common(top_k)
    print(f"âœ… é«˜é¢‘çŸ­è¯­æå–å®Œæ¯•ï¼ˆåˆ†å¥åï¼‰ï¼Œå…±è¿”å›å‰ {len(top_phrases)} é¡¹")
    return top_phrases




def save_to_jieba_dict(phrases, output_path="user_dict.txt", default_freq=10000):
    print(f"ğŸ“ æ­£åœ¨ä¿å­˜è¯å…¸åˆ° {output_path} ...")
    with open(output_path, "w", encoding="utf-8") as f:
        for phrase, freq in phrases:
            f.write(f"{phrase} {freq or default_freq} n\n")
    print(f"âœ… è¯å…¸ä¿å­˜æˆåŠŸï¼Œå¯ç”¨äº jieba / IK åˆ†è¯å™¨")


def filter_keep_longest_only(phrases):
    print("ğŸ“Š æ­£åœ¨è¿‡æ»¤å†—ä½™è¯ ...")
    phrases_sorted = sorted(phrases, key=lambda x: (-len(x[0]), -x[1]))
    kept = []
    for phrase, freq in phrases_sorted:
        if any(phrase in longer for longer, _ in kept if phrase != longer):
            continue  # æ˜¯å·²ä¿ç•™è¯çš„å­ä¸²ï¼Œè·³è¿‡
        kept.append((phrase, freq))
    return kept

def filter_by_freq_ratio(phrases, threshold=0.8):
    print("ğŸ“Š æ­£åœ¨è¿‡æ»¤å†—ä½™è¯ ...")
    phrases_sorted = sorted(phrases, key=lambda x: -len(x[0]))  # å…ˆé•¿è¯åçŸ­è¯
    phrase_map = {phrase: freq for phrase, freq in phrases}
    filtered = {}

    for phrase in phrase_map:
        if any(
            phrase in longer and phrase != longer and
            phrase_map.get(longer, 0) >= phrase_map[phrase] * threshold
            for longer in phrase_map
        ):
            continue  # è¢«æŸä¸ªæ›´é•¿è¯å¸æ”¶ï¼Œè·³è¿‡
        filtered[phrase] = phrase_map[phrase]

    return list(filtered.items())



# ===== ä¸»æµç¨‹å…¥å£ =====
if __name__ == "__main__":
    html_root_dir = "å·¨é‡åƒå·çŸ¥è¯†åº“all"  # â— è¯·æ›¿æ¢ä¸ºå®é™… HTML ç›®å½•
    texts = collect_all_html_texts(html_root_dir)
    phrases = extract_phrases_by_frequency(texts, ngram_range=(2, 12), top_k=1000)
    filtered_phrases = filter_by_freq_ratio(phrases)
    # filtered_phrases = phrases
    save_to_jieba_dict(filtered_phrases, output_path="user_dict.txt")
    # é¢å¤–ç”Ÿæˆ IK åˆ†è¯å™¨ä½¿ç”¨çš„çº¯è¯æ¡è¯å…¸
    ik_dict_path = "my_dict.dic"
    with open("user_dict.txt", "r", encoding="utf-8") as fin, open(ik_dict_path, "w", encoding="utf-8") as fout:
        for line in fin:
            parts = line.strip().split()
            if parts:
                word = parts[0]
                fout.write(word + "\n")
    print(f"âœ… IK åˆ†è¯å™¨è¯å…¸å·²ç”Ÿæˆ: {ik_dict_path}ï¼ˆä»…åŒ…å«è¯æ¡ï¼‰")
