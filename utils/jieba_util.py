import os
import re
from collections import Counter
from bs4 import BeautifulSoup

def extract_text_from_html(html_path):
    try:
        with open(html_path, 'r', encoding='utf-8') as f:
            soup = BeautifulSoup(f, 'html.parser')
            text = soup.get_text()
            print(f"âœ”ï¸ æˆåŠŸæå–æ–‡æœ¬ï¼š{html_path}ï¼ˆ{len(text)} å­—ï¼‰")
            return text
    except Exception as e:
        print(f"âŒ è¯»å–å¤±è´¥: {html_path}, åŸå› : {e}")
        return ""

def collect_all_html_texts(root_dir):
    texts = []
    print(f"ğŸ” æ­£åœ¨éå†ç›®å½•: {root_dir}")
    for root, _, files in os.walk(root_dir):
        for file in files:
            if file.endswith(".html"):
                full_path = os.path.join(root, file)
                text = extract_text_from_html(full_path)
                if text:
                    texts.append(text)
    print(f"ğŸ“„ å…±æ”¶é›†åˆ° {len(texts)} ä¸ª HTML æ–‡æœ¬")
    return texts

def split_sentences(text):
    # ä¸­æ–‡åŠä¸­è‹±æ ‡ç‚¹æ–­å¥
    return re.split(r"[ã€‚ï¼ï¼Ÿï¼›ï¼Œ,.!?;:ã€\n\r]+", text)

def is_chinese_word(word):
    return bool(word) and all('\u4e00' <= ch <= '\u9fff' for ch in word)


def extract_phrases_by_frequency(texts, ngram_range=(2, 5), top_k=500):
    counter = Counter()
    print("ğŸ“Š æ­£åœ¨è¿›è¡Œ n-gram è¯é¢‘ç»Ÿè®¡ï¼ˆæŒ‰å¥æˆªæ–­ï¼Œçº¯ä¸­æ–‡ï¼‰...")

    for text in texts:
        sentences = split_sentences(text)
        for sentence in sentences:
            sentence = re.sub(r"[^\u4e00-\u9fff]", "", sentence)  # ä»…ä¿ç•™ä¸­æ–‡
            for n in range(ngram_range[0], ngram_range[1] + 1):
                for i in range(len(sentence) - n + 1):
                    gram = sentence[i:i + n]
                    if is_chinese_word(gram):
                        counter[gram] += 1

    top_phrases = counter.most_common(top_k)
    print(f"âœ… é«˜é¢‘çŸ­è¯­æå–å®Œæ¯•ï¼ˆåˆ†å¥åï¼‰ï¼Œå…±è¿”å›å‰ {len(top_phrases)} é¡¹")
    return top_phrases




def save_to_jieba_dict(phrases, output_path="user_dict.txt", default_freq=10000):
    print(f"ğŸ“ æ­£åœ¨ä¿å­˜è¯å…¸åˆ° {output_path} ...")
    with open(output_path, "w", encoding="utf-8") as f:
        for phrase, freq in phrases:
            f.write(f"{phrase} {freq or default_freq} n\n")
    print(f"âœ… è¯å…¸ä¿å­˜æˆåŠŸï¼Œå¯ç”¨äº jieba / IK åˆ†è¯å™¨")


def filter_keep_longest_only(phrases):
    print("ğŸ“Š æ­£åœ¨è¿‡æ»¤å†—ä½™è¯ ...")
    phrases_sorted = sorted(phrases, key=lambda x: (-len(x[0]), -x[1]))
    kept = []
    for phrase, freq in phrases_sorted:
        if any(phrase in longer for longer, _ in kept if phrase != longer):
            continue  # æ˜¯å·²ä¿ç•™è¯çš„å­ä¸²ï¼Œè·³è¿‡
        kept.append((phrase, freq))
    return kept

def filter_by_freq_ratio(phrases, threshold=0.8):
    print("ğŸ“Š æ­£åœ¨è¿‡æ»¤å†—ä½™è¯ ...")
    phrases_sorted = sorted(phrases, key=lambda x: -len(x[0]))  # å…ˆé•¿è¯åçŸ­è¯
    phrase_map = {phrase: freq for phrase, freq in phrases}
    filtered = {}

    for phrase in phrase_map:
        if any(
            phrase in longer and phrase != longer and
            phrase_map.get(longer, 0) >= phrase_map[phrase] * threshold
            for longer in phrase_map
        ):
            continue  # è¢«æŸä¸ªæ›´é•¿è¯å¸æ”¶ï¼Œè·³è¿‡
        filtered[phrase] = phrase_map[phrase]

    return list(filtered.items())



# ===== ä¸»æµç¨‹å…¥å£ =====
if __name__ == "__main__":
    html_root_dir = "å·¨é‡åƒå·çŸ¥è¯†åº“all"  # â— è¯·æ›¿æ¢ä¸ºå®é™… HTML ç›®å½•
    texts = collect_all_html_texts(html_root_dir)
    phrases = extract_phrases_by_frequency(texts, ngram_range=(2, 12), top_k=1000)
    filtered_phrases = filter_by_freq_ratio(phrases)
    # filtered_phrases = phrases
    save_to_jieba_dict(filtered_phrases, output_path="user_dict.txt")
    # é¢å¤–ç”Ÿæˆ IK åˆ†è¯å™¨ä½¿ç”¨çš„çº¯è¯æ¡è¯å…¸
    ik_dict_path = "my_dict.dic"
    with open("user_dict.txt", "r", encoding="utf-8") as fin, open(ik_dict_path, "w", encoding="utf-8") as fout:
        for line in fin:
            parts = line.strip().split()
            if parts:
                word = parts[0]
                fout.write(word + "\n")
    print(f"âœ… IK åˆ†è¯å™¨è¯å…¸å·²ç”Ÿæˆ: {ik_dict_path}ï¼ˆä»…åŒ…å«è¯æ¡ï¼‰")
