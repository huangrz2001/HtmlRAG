import os
import re
import jieba
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import json
from datetime import datetime
import torch
from difflib import SequenceMatcher
import requests
import time
import asyncio
from utils.config import CONFIG



# Semaphore æ§åˆ¶å¹¶å‘è´Ÿè½½
sem = asyncio.Semaphore(CONFIG.get("vllm_max_concurrent_requests", 32))
timeout = CONFIG.get("vllm_timeout", 60)
api_url = CONFIG.get("vllm_api_url", "http://localhost:8000/v1/chat/completions")


# ======================== æ–‡æ¡£å—åˆ†ç±»å‡½æ•° ========================
def infer_chunk_category(page_url):
    if any(k in page_url for k in ["è§„åˆ™", "åˆ¶åº¦", "æ³•å¾‹", "å®¡æ ¸"]):
        return "è§„åˆ™ç±»"
    elif any(k in page_url for k in ["ä½¿ç”¨", "æŒ‡å—", "å¸®åŠ©", "æ“ä½œ", "åŠŸèƒ½"]):
        return "æ“ä½œç±»"
    elif any(k in page_url for k in ["ç”Ÿæ€", "è§’è‰²", "ç­–ç•¥", "æ¨å¹¿", "å¹³å°ä¿¡æ¯"]):
        return "ä¿¡æ¯ç±»"
    else:
        return "æ³›ç”¨ç±»"



# ======================== vLLM æ‘˜è¦ç”Ÿæˆå‡½æ•° ========================
def generate_summary_vllm(text, page_url, max_new_tokens=150, model="glm") -> str:
    """ä½¿ç”¨ HTTP è°ƒç”¨ vLLM å¹¶å‘å—æ§æ‘˜è¦ç”Ÿæˆ"""
    if len(text) < max_new_tokens * 2:
        print("âš ï¸ æ–‡æœ¬é•¿åº¦ä¸è¶³ï¼Œä½¿ç”¨åŸæ–‡æœ¬")
        return text[:max_new_tokens]

    category = infer_chunk_category(page_url)
    text = text.strip().replace("\x00", "")

    prompt = (
        f"ä½ æ­£åœ¨å¤„ç†ä¸€ç¯‡ç”µå•†å¹³å°çš„çŸ¥è¯†å†…å®¹ï¼Œå±äºâ€œ{category}â€ç±»ã€‚\n"
        f"è¯·ä½ æ ¹æ®ä¸‹æ–¹å†…å®¹æç‚¼å…¶ä¸»è¦ä¿¡æ¯ï¼Œè¦æ±‚å¦‚ä¸‹ï¼š\n"
        f"1. æ¦‚æ‹¬è¦ç‚¹ï¼Œä¸è¦é‡å¤åŸæ–‡åŸå¥ï¼›\n"
        f"2. æ€»é•¿åº¦ä¸è¶…è¿‡{max_new_tokens}å­—ï¼Œä½¿ç”¨ç®€ä½“ä¸­æ–‡ï¼›\n"
        f"3. è¾“å‡ºæ ¼å¼ä¸ºå®Œæ•´ä¸€å¥è¯ã€‚\n"
        f"ğŸ“‚ æ¥æºè·¯å¾„ï¼š{page_url}\n"
        f"ğŸ“„ å†…å®¹ï¼š\n{text}"
    )

    payload = {
        "model": model,
        "messages": [{"role": "user", "content": prompt}],
        "max_tokens": max_new_tokens,
        "temperature": 0.4,
        "top_p": 0.8
    }

    api_url = CONFIG.get("vllm_api_url", "http://localhost:8000/v1/chat/completions")

    try:
        with sem:  # æ§åˆ¶å¹¶å‘
            start = time.time()
            response = requests.post(api_url, headers={"Content-Type": "application/json"}, json=payload, timeout=60)
            response.raise_for_status()
            result = response.json()["choices"][0]["message"]["content"].strip()
            duration = time.time() - start
            print(f"âœ… vLLMæ‘˜è¦æˆåŠŸ (è€—æ—¶ {duration:.2f}s)")
            return result or text[:max_new_tokens]
    except Exception as e:
        print(f"âš ï¸ vLLM æ‘˜è¦ç”Ÿæˆå¤±è´¥: {e}ï¼Œfallback åˆ°æˆªæ–­æ–‡æœ¬")
        return text[:max_new_tokens]



# ======================== ChatGLM æ‘˜è¦ç”Ÿæˆå‡½æ•° ========================
def generate_summary_ChatGLM(
    text,
    page_url,
    model,
    tokenizer,
    max_new_tokens=150,
):
    if len(text) < max_new_tokens * 2:
        print("âš ï¸ æ–‡æœ¬é•¿åº¦ä¸è¶³ï¼Œä½¿ç”¨åŸæ–‡æœ¬")
        return text[:max_new_tokens]

    category = infer_chunk_category(page_url)
    text = text.strip().replace("\x00", "")

    prompt = (
            f"ä½ æ­£åœ¨å¤„ç†ä¸€ç¯‡ç”µå•†å¹³å°çš„çŸ¥è¯†å†…å®¹ï¼Œå±äºâ€œ{category}â€ç±»ã€‚\n"
            f"è¯·ä½ æ ¹æ®ä¸‹æ–¹å†…å®¹æç‚¼å…¶ä¸»è¦ä¿¡æ¯ï¼Œè¦æ±‚å¦‚ä¸‹ï¼š\n"
            f"1. æ¦‚æ‹¬è¦ç‚¹ï¼Œä¸è¦é‡å¤åŸæ–‡åŸå¥ï¼›\n"
            f"2. æ€»é•¿åº¦ä¸è¶…è¿‡{max_new_tokens}å­—ï¼Œä½¿ç”¨ç®€ä½“ä¸­æ–‡ï¼›\n"
            f"3. è¾“å‡ºæ ¼å¼ä¸ºå®Œæ•´ä¸€å¥è¯ã€‚\n"
            f"ğŸ“‚ æ¥æºè·¯å¾„ï¼š{page_url}\n"
            f"ğŸ“„ å†…å®¹ï¼š\n{text}"
        )

    try:
        messages = [{"role": "user", "content": prompt}]
        inputs = tokenizer.apply_chat_template(
            messages,
            add_generation_prompt=True,
            return_tensors="pt",
            return_dict=True,
        ).to(model.device)

        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                do_sample=True,
                top_p=0.8,
                temperature=0.4
            )
        # è£å‰ªæ‰ prompt éƒ¨åˆ†
        generated_ids = outputs[:, inputs["input_ids"].shape[1]:]
        response = tokenizer.decode(generated_ids[0], skip_special_tokens=True).strip()
        return response if response else text[:max_new_tokens]
    except Exception as e:
        print(f"âš ï¸ ChatGLM æ‘˜è¦ç”Ÿæˆå¤±è´¥: {e}ï¼Œä½¿ç”¨ fallback")
        return text[:max_new_tokens]



# ======================== ChatGLM é—®é¢˜ç”Ÿæˆå‡½æ•° ========================
def generate_question_ChatGLM(
    text,
    page_url,
    model,
    tokenizer,
    max_new_tokens=64,
    fallback_question="è¯¥å†…å®¹å¯æ„é€ ç›¸å…³ä¸šåŠ¡é—®é¢˜"
):

    category = infer_chunk_category(page_url)
    text = text.strip().replace("\x00", "")

    if category == "è§„åˆ™ç±»":
        hint = "å¹³å°æ˜¯å¦å…è®¸ã€è§„åˆ™çº¦æŸã€è¿è§„å¤„ç†"
    elif category == "æ“ä½œç±»":
        hint = "å¦‚ä½•æ“ä½œã€æ˜¯å¦å¯ç”¨ã€ä½¿ç”¨æ–¹æ³•"
    elif category == "ä¿¡æ¯ç±»":
        hint = "å¹³å°èƒŒæ™¯ã€äº§å“å®šä½ã€ç­–ç•¥è®¾è®¡"
    else:
        hint = "ç”¨æˆ·å®é™…å¯èƒ½ä¼šé—®çš„é—®é¢˜"

    prompt = (
        f"ä½ æ˜¯ä¸€ä¸ªç”µå•†å¹³å°çŸ¥è¯†é—®ç­”æ„å»ºåŠ©æ‰‹ï¼Œè¯·æ ¹æ®ä»¥ä¸‹å†…å®¹ç”Ÿæˆä¸€ä¸ªæœ‰å®é™…ä»·å€¼çš„ç”¨æˆ·é—®é¢˜ã€‚\n"
        f"è¦æ±‚ï¼š\n"
        f"- é—®é¢˜åº”ä½“ç°â€œ{hint}â€ï¼›\n"
        f"- ç¦æ­¢å¤è¿°åŸæ–‡ï¼Œåº”æç‚¼æ“ä½œã€åˆ¤æ–­æˆ–å’¨è¯¢ç‚¹ï¼›\n"
        f"- åªè¾“å‡ºä¸€ä¸ªç®€ä½“ä¸­æ–‡é—®é¢˜å¥ï¼Œä¸åŠ è¯´æ˜ã€‚\n"
        f"ğŸ“‚ æ¥æºè·¯å¾„ï¼š{page_url}\n"
        f"ğŸ“„ å†…å®¹ï¼š\n{text}"
    )

    try:
        messages = [{"role": "user", "content": prompt}]
        inputs = tokenizer.apply_chat_template(
            messages,
            add_generation_prompt=True,
            return_tensors="pt",
            return_dict=True,
        ).to(model.device)

        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                do_sample=True,
                top_p=0.9,
                temperature=0.7
            )
        generated_ids = outputs[:, inputs["input_ids"].shape[1]:]
        response = tokenizer.decode(generated_ids[0], skip_special_tokens=True).strip()
        return response if response else fallback_question
    except Exception as e:
        print(f"âš ï¸ ChatGLM é—®é¢˜ç”Ÿæˆå¤±è´¥: {e}ï¼Œä½¿ç”¨ fallback")
        return fallback_question



# ======================== ChatGLM å¤šè½® Query Rewriting å‡½æ•° ========================
def rewrite_query_ChatGLM(
    dialogue: list,  # æ ¼å¼: [{"role": "user", "content": "..."}, {"role": "assistant", "content": "..."}]
    final_query: str,
    model,
    tokenizer,
    max_new_tokens=128,
):
    """
    åŸºäºå¯¹è¯å†å²å’Œå½“å‰æ¨¡ç³Šé—®é¢˜ï¼Œä½¿ç”¨ ChatGLM é‡å†™ä¸ºç‹¬ç«‹æ¸…æ™°é—®é¢˜
    """
    fallback_rewrite = final_query

    # æ„é€ æç¤ºè¯
    prompt = (
        "ä½ æ˜¯ä¸€ä¸ªç”µå•†å¹³å°æ™ºèƒ½å®¢æœçš„å¯¹è¯æ¸…æ™°åŒ–åŠ©æ‰‹ã€‚\n"
        "ç”¨æˆ·æå‡ºçš„é—®é¢˜å¯èƒ½å­˜åœ¨å¤æ‚æŒ‡ä»£ã€ä¸Šä¸‹æ–‡ä¾èµ–æˆ–è¡¨è¾¾æ¨¡ç³Šç­‰é—®é¢˜ã€‚\n"
        "ä½ éœ€è¦æ ¹æ®å¤šè½®å†å²å¯¹è¯ï¼Œä¸°å¯Œæ¶¦è‰²ç”¨æˆ·çš„å½“å‰é—®é¢˜ï¼Œä½¿å…¶æˆä¸ºä¸€ä¸ªæ¸…æ™°ã€å®Œæ•´çš„ç‹¬ç«‹é—®é¢˜ã€‚\n\n"
        "ä¸‹é¢æ˜¯è¦æ±‚ï¼š\n"
        "- å‡†ç¡®è§£æç”¨æˆ·çœŸå®æ„å›¾ï¼Œä½¿å¾—è¿™ä¸ªç‹¬ç«‹é—®é¢˜å°½é‡å®Œæ•´ï¼Œå°½å¯èƒ½åŒ…å«æ‰€æœ‰ä¿¡æ¯ï¼›\n"
        "- é—®é¢˜è¶Šä¸°å¯Œè¶Šå¥½ï¼Œç‰¹åˆ«æ˜¯è¦æ•æ‰åˆ°å…³é”®çš„æŒ‡ä»£ï¼Œåœºæ™¯ï¼Œç‰¹åˆ«é’ˆå¯¹çš„é—®é¢˜å’Œä¾‹å­ç­‰ç­‰ï¼›\n"
        "- ä¸å¯ä»¥æé€ ä¸å­˜åœ¨çš„ä¿¡æ¯ï¼›\n"
        "- ä¸æ·»åŠ è§£é‡Šã€æ³¨é‡Šã€å¼•å¯¼è¯­ç­‰ï¼Œåªè¾“å‡ºæ¶¦è‰²åçš„é—®é¢˜å¥ã€‚\n\n"
        "ä¸‹é¢æ˜¯å†å²å¯¹è¯ï¼š\n"
    )

    for turn in dialogue:
        role = "ç”¨æˆ·" if turn["speaker"] == "user" else "ç³»ç»Ÿ"
        content = turn["text"].replace("\n", " ").strip()
        prompt += f"{role}ï¼š{content}\n"

    prompt += f"ç”¨æˆ·å½“å‰é—®é¢˜æ˜¯ï¼š{final_query.strip()}\nè¯·ä½ éµå¾ªè¦æ±‚æ¶¦è‰²ä¸ºä¸€ä¸ªæ¸…æ™°ã€å®Œæ•´çš„ç‹¬ç«‹é—®é¢˜ï¼š"
    # print(f"ğŸ” é‡å†™æç¤ºè¯ï¼š\n{prompt[:256]}...")  # ä»…æ‰“å°å‰256å­—ç¬¦

    try:
        messages = [{"role": "user", "content": prompt}]
        inputs = tokenizer.apply_chat_template(
            messages,
            add_generation_prompt=True,
            return_tensors="pt",
            return_dict=True,
        ).to(model.device)

        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                do_sample=True,
                top_p=0.8,
                temperature=0.4
            )

        generated_ids = outputs[:, inputs["input_ids"].shape[1]:]
        response = tokenizer.decode(generated_ids[0], skip_special_tokens=True).strip()
        return response if response else fallback_rewrite
    except Exception as e:
        print(f"âš ï¸ ChatGLM é‡å†™å¤±è´¥: {e}ï¼Œè¿”å›åŸé—®é¢˜")
        return fallback_rewrite


def rewrite_query_vllm(
    dialogue: list,
    final_query: str,
    model: str = "glm",
    max_new_tokens: int = 128
) -> str:
    """
    åŸºäºå¯¹è¯å†å²å’Œå½“å‰æ¨¡ç³Šé—®é¢˜ï¼Œä½¿ç”¨ vLLM Chat æ¥å£é‡å†™ä¸ºç‹¬ç«‹æ¸…æ™°é—®é¢˜ï¼ˆå¸¦å¹¶å‘æ§åˆ¶ï¼‰
    """
    fallback_rewrite = final_query
    # æ„é€  prompt
    prompt = (
        "ä½ æ˜¯ä¸€ä¸ªç”µå•†å¹³å°æ™ºèƒ½å®¢æœçš„å¯¹è¯æ¸…æ™°åŒ–åŠ©æ‰‹ã€‚\n"
        "ç”¨æˆ·æå‡ºçš„é—®é¢˜å¯èƒ½å­˜åœ¨å¤æ‚æŒ‡ä»£ã€ä¸Šä¸‹æ–‡ä¾èµ–æˆ–è¡¨è¾¾æ¨¡ç³Šç­‰é—®é¢˜ã€‚\n"
        "ä½ éœ€è¦æ ¹æ®å¤šè½®å†å²å¯¹è¯ï¼Œä¸°å¯Œæ¶¦è‰²ç”¨æˆ·çš„å½“å‰é—®é¢˜ï¼Œä½¿å…¶æˆä¸ºä¸€ä¸ªæ¸…æ™°ã€å®Œæ•´çš„ç‹¬ç«‹é—®é¢˜ã€‚\n\n"
        "ä¸‹é¢æ˜¯è¦æ±‚ï¼š\n"
        "- å‡†ç¡®è§£æç”¨æˆ·çœŸå®æ„å›¾ï¼Œä½¿å¾—è¿™ä¸ªç‹¬ç«‹é—®é¢˜å°½é‡å®Œæ•´ï¼Œå°½å¯èƒ½åŒ…å«æ‰€æœ‰ä¿¡æ¯ï¼›\n"
        "- é—®é¢˜è¶Šå…·ä½“è¶Šå¥½ï¼Œç‰¹åˆ«æ˜¯è¦æ•æ‰åˆ°å…³é”®çš„æŒ‡ä»£ï¼Œåœºæ™¯ï¼Œç‰¹åˆ«é’ˆå¯¹çš„é—®é¢˜å’Œä¾‹å­ç­‰ç­‰ï¼›\n"
        "- ä¸å¯ä»¥æé€ ä¸å­˜åœ¨çš„ä¿¡æ¯ï¼ŒåŒæ—¶è¿‡æ»¤æ‰ä¸çœŸå®æ„å›¾æ— å…³çš„ä¿¡æ¯ï¼›\n"
        "- ä¸æ·»åŠ è§£é‡Šã€æ³¨é‡Šã€å¼•å¯¼è¯­ç­‰ï¼Œåªè¾“å‡ºæ¶¦è‰²åçš„é—®é¢˜å¥ã€‚\n\n"
        "ä¸‹é¢æ˜¯å†å²å¯¹è¯ï¼š\n"
    )

    for turn in dialogue:
        role = "ç”¨æˆ·" if turn.get("speaker") == "user" else "ç³»ç»Ÿ"
        content = turn.get("text", "").replace("\n", " ").strip()
        prompt += f"{role}ï¼š{content}\n"

    prompt += f"ç”¨æˆ·å½“å‰é—®é¢˜æ˜¯ï¼š{final_query.strip()}\nè¯·ä½ éµå¾ªè¦æ±‚æ¶¦è‰²ä¸ºä¸€ä¸ªæ¸…æ™°ã€å®Œæ•´çš„ç‹¬ç«‹é—®é¢˜ï¼š"

    payload = {
        "model": model,
        "messages": [{"role": "user", "content": prompt}],
        "max_tokens": max_new_tokens,
        "temperature": 0.4,
        "top_p": 0.8
    }

    try:
        # with sem:  # é™åˆ¶å¹¶å‘è¯·æ±‚
        start = time.time()
        response = requests.post(api_url, headers={"Content-Type": "application/json"}, json=payload, timeout=timeout)
        response.raise_for_status()
        result = response.json()["choices"][0]["message"]["content"].strip()
        duration = time.time() - start
        print(f"âœ… é‡å†™æˆåŠŸï¼ˆè€—æ—¶ {duration:.2f}sï¼‰")
        return result if result else fallback_rewrite
    except Exception as e:
        print(f"âš ï¸ vLLM é‡å†™å¤±è´¥: {e}")
        return fallback_rewrite
