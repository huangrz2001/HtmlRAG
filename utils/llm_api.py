# -*- coding: utf-8 -*-
"""
å¤§æ¨¡å‹æ‘˜è¦ç”Ÿæˆä¸å¤šè½®é—®ç­”é‡å†™æ¨¡å—ï¼ˆChatGLM / vLLM æ”¯æŒï¼‰

æœ¬æ¨¡å—å°è£…äº†ä½¿ç”¨ ChatGLM ä¸ vLLM æ¥å£å®Œæˆä»¥ä¸‹ä»»åŠ¡çš„èƒ½åŠ›ï¼š
1. æ–‡æ¡£å—æ‘˜è¦ç”Ÿæˆï¼ˆæ”¯æŒå¹¶å‘æ§åˆ¶å’Œç±»å‹è‡ªé€‚åº”ï¼‰
2. é—®é¢˜ç”Ÿæˆï¼ˆæ ¹æ®æ–‡æ¡£å—è‡ªåŠ¨ç”Ÿæˆæ½œåœ¨ç”¨æˆ·é—®é¢˜ï¼‰
3. å¤šè½®å¯¹è¯é‡å†™ï¼ˆå°†ç”¨æˆ·æ¨¡ç³Šæé—®é‡å†™ä¸ºç‹¬ç«‹æ¸…æ™°é—®é¢˜ï¼‰
4. å‘é‡ç”Ÿæˆï¼ˆåŸºäº vLLM Embedding æ¥å£ï¼‰

æ¨¡å—äº®ç‚¹ï¼š
------------------------------------------------
- æ”¯æŒ ChatGLM æœ¬åœ°æ¨¡å‹ä¸ vLLM éƒ¨ç½²æœåŠ¡ä¸¤ç§è°ƒç”¨æ–¹å¼
- æ‘˜è¦/é—®å¥ç”Ÿæˆæ ¹æ® URL è·¯å¾„æ™ºèƒ½åˆ†ç±»ï¼ˆè§„åˆ™ç±» / æ“ä½œç±» / ä¿¡æ¯ç±» / æ³›ç”¨ç±»ï¼‰
- å¤šè½® query é‡å†™ä»»åŠ¡æ”¯æŒä¸Šä¸‹æ–‡èåˆï¼Œæç¤ºè¯ç»è¿‡ prompt tuning ä¼˜åŒ–
- æä¾› vLLM å¹¶å‘æ§åˆ¶æœºåˆ¶ï¼ˆé€šè¿‡ asyncio.Semaphore å®ç°è¯·æ±‚é€Ÿç‡è°ƒæ§ï¼‰
- æ”¯æŒ vLLM åµŒå…¥æ¥å£ï¼Œå¯ç”¨äºåç»­å‘é‡åŒ–æœç´¢

ä¸»è¦å‡½æ•°è¯´æ˜ï¼š
------------------------------------------------
1. æ‘˜è¦ç”Ÿæˆï¼š
   - `generate_summary_vllm`: ä½¿ç”¨ vLLM æ¥å£ç”Ÿæˆæ‘˜è¦ï¼Œæ”¯æŒè¶…æ—¶ä¸å¹¶å‘é™åˆ¶
   - `generate_summary_ChatGLM`: ä½¿ç”¨æœ¬åœ° ChatGLM æ¨¡å‹ç”Ÿæˆæ‘˜è¦

2. é—®é¢˜ç”Ÿæˆï¼š
   - `generate_question_ChatGLM`: æ ¹æ®æ–‡æ¡£å†…å®¹å’Œç±»åˆ«ç”Ÿæˆä¸€ä¸ªä»£è¡¨æ€§é—®é¢˜

3. å¤šè½®å¯¹è¯é‡å†™ï¼š
   - `rewrite_query_ChatGLM`: ä½¿ç”¨ ChatGLM å¯¹è¯æ¨¡æ¿æ”¹å†™ç”¨æˆ·æ¨¡ç³Šé—®é¢˜
   - `rewrite_query_vllm`: ä½¿ç”¨ vLLM Chat API æ”¹å†™ç”¨æˆ·æ¨¡ç³Šé—®é¢˜

4. å…¶ä»–è¾…åŠ©ï¼š
   - `infer_chunk_category`: æ ¹æ® page_url åˆ†ç±»æ–‡æ¡£å†…å®¹ï¼ˆè§„åˆ™/æ“ä½œ/ä¿¡æ¯/æ³›ç”¨ï¼‰
   - `get_embedding_from_vllm`: è°ƒç”¨ vLLM embedding æ¥å£è·å–æ–‡æœ¬å‘é‡ï¼ˆé€‚é… BCE æ¨¡å‹ï¼‰

é…ç½®ä¾èµ–é¡¹ï¼š
------------------------------------------------
- `CONFIG` ä¸­å¯é…ç½®ï¼š
  - `"vllm_api_url"`ï¼švLLM æ¨ç†æœåŠ¡åœ°å€
  - `"vllm_max_concurrent_requests"`ï¼šæœ€å¤§å¹¶å‘æ•°
  - `"vllm_timeout"`ï¼šè¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰

"""


import httpx
import torch
import requests
import time
import asyncio
from utils.config import CONFIG, get_aiohttp_session, close_aiohttp_session, sem, logger
import aiohttp
from typing import List, Dict, Optional, Set
import random



timeout = CONFIG.get("vllm_timeout", 60)
# api_url = CONFIG.get("vllm_api_url", "http://localhost:8011/v1/chat/completions")
VLLM_SERVERS = CONFIG.get("vllm_api_servers", [])
VLLM_TIMEOUT = CONFIG.get("vllm_timeout", 60)



# ======================== æ–‡æ¡£å—åˆ†ç±»å‡½æ•° ========================
def infer_chunk_category(page_url):
    if any(k in page_url for k in ["è§„åˆ™", "åˆ¶åº¦", "æ³•å¾‹", "å®¡æ ¸"]):
        return "è§„åˆ™ç±»"
    elif any(k in page_url for k in ["ä½¿ç”¨", "æŒ‡å—", "å¸®åŠ©", "æ“ä½œ", "åŠŸèƒ½"]):
        return "æ“ä½œç±»"
    elif any(k in page_url for k in ["ç”Ÿæ€", "è§’è‰²", "ç­–ç•¥", "æ¨å¹¿", "å¹³å°ä¿¡æ¯"]):
        return "ä¿¡æ¯ç±»"
    else:
        return "æ³›ç”¨ç±»"




def get_embedding_from_vllm(text: str) -> list[float]:
    url = "http://0.0.0.0:8010/v1/embeddings"
    payload = {
        "model": "/home/algo/AD_agent/models/bce-embedding-base_v1",
        "input": [text],    # ä¸€å®šè¦æ˜¯åˆ—è¡¨
    }
    resp = requests.post(url, json=payload, timeout=5)
    resp.raise_for_status()
    data = resp.json()
    return data["data"][0]["embedding"]
    


# ======================== vLLM æ‘˜è¦ç”Ÿæˆå‡½æ•° ========================
def generate_summary_vllm(text, page_url, max_new_tokens=150, model="glm") -> str:
    """ä½¿ç”¨ HTTP è°ƒç”¨ vLLM å¹¶å‘å—æ§æ‘˜è¦ç”Ÿæˆ"""
    if len(text) < max_new_tokens * 2:
        print("âš ï¸ æ–‡æœ¬é•¿åº¦ä¸è¶³ï¼Œä½¿ç”¨åŸæ–‡æœ¬")
        return text[:max_new_tokens]

    category = infer_chunk_category(page_url)
    text = text.strip().replace("\x00", "")

    prompt = (
        f"ä½ æ­£åœ¨å¤„ç†ä¸€ç¯‡ç”µå•†å¹³å°çš„çŸ¥è¯†å†…å®¹ï¼Œå±äºâ€œ{category}â€ç±»ã€‚\n"
        f"è¯·ä½ æ ¹æ®ä¸‹æ–¹å†…å®¹æç‚¼å…¶ä¸»è¦ä¿¡æ¯ï¼Œè¦æ±‚å¦‚ä¸‹ï¼š\n"
        f"1. æ¦‚æ‹¬è¦ç‚¹ï¼Œä¸è¦é‡å¤åŸæ–‡åŸå¥ï¼›\n"
        f"2. æ€»é•¿åº¦ä¸è¶…è¿‡{max_new_tokens}å­—ï¼Œä½¿ç”¨ç®€ä½“ä¸­æ–‡ï¼›\n"
        f"3. è¾“å‡ºæ ¼å¼ä¸ºå®Œæ•´ä¸€å¥è¯ã€‚\n"
        f"ğŸ“‚ æ¥æºè·¯å¾„ï¼š{page_url}\n"
        f"ğŸ“„ å†…å®¹ï¼š\n{text}"
    )

    payload = {
        "model": model,
        "messages": [{"role": "user", "content": prompt}],
        "max_tokens": max_new_tokens,
        "temperature": 0.4,
        "top_p": 0.8
    }

    api_url = CONFIG.get("vllm_api_url", "http://localhost:8000/v1/chat/completions")

    try:
        start = time.time()
        response = requests.post(api_url, headers={"Content-Type": "application/json"}, json=payload, timeout=60)
        response.raise_for_status()
        result = response.json()["choices"][0]["message"]["content"].strip()
        duration = time.time() - start
        print(f"âœ… vLLMæ‘˜è¦æˆåŠŸ (è€—æ—¶ {duration:.2f}s)")
        return result or text[:max_new_tokens]
    except Exception as e:
        print(f"âš ï¸ vLLM æ‘˜è¦ç”Ÿæˆå¤±è´¥: {e}ï¼Œfallback åˆ°æˆªæ–­æ–‡æœ¬")
        return text[:max_new_tokens]



async def generate_summary_vllm_async(text, page_url, model="glm", max_new_tokens=150):
    """
    çœŸæ­£å¼‚æ­¥å¹¶å‘è°ƒç”¨ vLLM æ¥å£ç”Ÿæˆæ‘˜è¦
    """
    category = infer_chunk_category(page_url)
    text = text.strip().replace("\x00", "")

    prompt = (
        f"ä½ æ­£åœ¨å¤„ç†ä¸€ç¯‡ç”µå•†å¹³å°çš„çŸ¥è¯†å†…å®¹ï¼Œå±äºâ€œ{category}â€ç±»ã€‚\n"
        f"è¯·ä½ æ ¹æ®ä¸‹æ–¹å†…å®¹æç‚¼å…¶ä¸»è¦ä¿¡æ¯ï¼Œè¦æ±‚å¦‚ä¸‹ï¼š\n"
        f"1. æ¦‚æ‹¬è¦ç‚¹ï¼Œä¸è¦é‡å¤åŸæ–‡åŸå¥ï¼›\n"
        f"2. æ€»é•¿åº¦ä¸è¶…è¿‡{max_new_tokens}å­—ï¼Œä½¿ç”¨ç®€ä½“ä¸­æ–‡ï¼›\n"
        f"3. è¾“å‡ºæ ¼å¼ä¸ºå®Œæ•´ä¸€å¥è¯ã€‚\n"
        f"ğŸ“‚ æ¥æºè·¯å¾„ï¼š{page_url}\n"
        f"ğŸ“„ å†…å®¹ï¼š\n{text}"
    )

    payload = {
        "model": model,
        "messages": [{"role": "user", "content": prompt}],
        "max_tokens": max_new_tokens,
        "temperature": 0.4,
        "top_p": 0.8,
    }

    url = CONFIG.get("vllm_api_url", "http://localhost:8000/v1/chat/completions")
    headers = {"Content-Type": "application/json"}

    try:
        async with aiohttp.ClientSession(headers=headers) as session:
            async with session.post(url, json=payload, timeout=CONFIG.get("vllm_timeout", 60)) as resp:
                resp.raise_for_status()
                result = await resp.json()
                return result["choices"][0]["message"]["content"].strip()
    except Exception as e:
        print(f"âš ï¸ vLLM å¼‚æ­¥æ‘˜è¦å¤±è´¥: {e}ï¼Œè¿”å›æˆªæ–­æ–‡æœ¬")
        return text[:max_new_tokens]


# ======================== ChatGLM æ‘˜è¦ç”Ÿæˆå‡½æ•° ========================
def generate_summary_ChatGLM(
    text,
    page_url,
    model,
    tokenizer,
    max_new_tokens=150,
):
    if len(text) < max_new_tokens * 2:
        print("âš ï¸ æ–‡æœ¬é•¿åº¦ä¸è¶³ï¼Œä½¿ç”¨åŸæ–‡æœ¬")
        return text[:max_new_tokens]

    category = infer_chunk_category(page_url)
    text = text.strip().replace("\x00", "")

    prompt = (
            f"ä½ æ­£åœ¨å¤„ç†ä¸€ç¯‡ç”µå•†å¹³å°çš„çŸ¥è¯†å†…å®¹ï¼Œå±äºâ€œ{category}â€ç±»ã€‚\n"
            f"è¯·ä½ æ ¹æ®ä¸‹æ–¹å†…å®¹æç‚¼å…¶ä¸»è¦ä¿¡æ¯ï¼Œè¦æ±‚å¦‚ä¸‹ï¼š\n"
            f"1. æ¦‚æ‹¬è¦ç‚¹ï¼Œä¸è¦é‡å¤åŸæ–‡åŸå¥ï¼›\n"
            f"2. æ€»é•¿åº¦ä¸è¶…è¿‡{max_new_tokens}å­—ï¼Œä½¿ç”¨ç®€ä½“ä¸­æ–‡ï¼›\n"
            f"3. è¾“å‡ºæ ¼å¼ä¸ºå®Œæ•´ä¸€å¥è¯ã€‚\n"
            f"ğŸ“‚ æ¥æºè·¯å¾„ï¼š{page_url}\n"
            f"ğŸ“„ å†…å®¹ï¼š\n{text}"
        )

    try:
        messages = [{"role": "user", "content": prompt}]
        inputs = tokenizer.apply_chat_template(
            messages,
            add_generation_prompt=True,
            return_tensors="pt",
            return_dict=True,
        ).to(model.device)

        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                do_sample=True,
                top_p=0.8,
                temperature=0.4
            )
        # è£å‰ªæ‰ prompt éƒ¨åˆ†
        generated_ids = outputs[:, inputs["input_ids"].shape[1]:]
        response = tokenizer.decode(generated_ids[0], skip_special_tokens=True).strip()
        return response if response else text[:max_new_tokens]
    except Exception as e:
        print(f"âš ï¸ ChatGLM æ‘˜è¦ç”Ÿæˆå¤±è´¥: {e}ï¼Œä½¿ç”¨ fallback")
        return text[:max_new_tokens]



# ======================== ChatGLM é—®é¢˜ç”Ÿæˆå‡½æ•° ========================
def generate_question_ChatGLM(
    text,
    page_url,
    model,
    tokenizer,
    max_new_tokens=64,
    fallback_question="è¯¥å†…å®¹å¯æ„é€ ç›¸å…³ä¸šåŠ¡é—®é¢˜"
):

    category = infer_chunk_category(page_url)
    text = text.strip().replace("\x00", "")

    if category == "è§„åˆ™ç±»":
        hint = "å¹³å°æ˜¯å¦å…è®¸ã€è§„åˆ™çº¦æŸã€è¿è§„å¤„ç†"
    elif category == "æ“ä½œç±»":
        hint = "å¦‚ä½•æ“ä½œã€æ˜¯å¦å¯ç”¨ã€ä½¿ç”¨æ–¹æ³•"
    elif category == "ä¿¡æ¯ç±»":
        hint = "å¹³å°èƒŒæ™¯ã€äº§å“å®šä½ã€ç­–ç•¥è®¾è®¡"
    else:
        hint = "ç”¨æˆ·å®é™…å¯èƒ½ä¼šé—®çš„é—®é¢˜"

    prompt = (
        f"ä½ æ˜¯ä¸€ä¸ªç”µå•†å¹³å°çŸ¥è¯†é—®ç­”æ„å»ºåŠ©æ‰‹ï¼Œè¯·æ ¹æ®ä»¥ä¸‹å†…å®¹ç”Ÿæˆä¸€ä¸ªæœ‰å®é™…ä»·å€¼çš„ç”¨æˆ·é—®é¢˜ã€‚\n"
        f"è¦æ±‚ï¼š\n"
        f"- é—®é¢˜åº”ä½“ç°â€œ{hint}â€ï¼›\n"
        f"- ç¦æ­¢å¤è¿°åŸæ–‡ï¼Œåº”æç‚¼æ“ä½œã€åˆ¤æ–­æˆ–å’¨è¯¢ç‚¹ï¼›\n"
        f"- åªè¾“å‡ºä¸€ä¸ªç®€ä½“ä¸­æ–‡é—®é¢˜å¥ï¼Œä¸åŠ è¯´æ˜ã€‚\n"
        f"ğŸ“‚ æ¥æºè·¯å¾„ï¼š{page_url}\n"
        f"ğŸ“„ å†…å®¹ï¼š\n{text}"
    )

    try:
        messages = [{"role": "user", "content": prompt}]
        inputs = tokenizer.apply_chat_template(
            messages,
            add_generation_prompt=True,
            return_tensors="pt",
            return_dict=True,
        ).to(model.device)

        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                do_sample=True,
                top_p=0.9,
                temperature=0.7
            )
        generated_ids = outputs[:, inputs["input_ids"].shape[1]:]
        response = tokenizer.decode(generated_ids[0], skip_special_tokens=True).strip()
        return response if response else fallback_question
    except Exception as e:
        print(f"âš ï¸ ChatGLM é—®é¢˜ç”Ÿæˆå¤±è´¥: {e}ï¼Œä½¿ç”¨ fallback")
        return fallback_question



# ======================== ChatGLM å¤šè½® Query Rewriting å‡½æ•° ========================
def rewrite_query_ChatGLM(
    dialogue: list,  # æ ¼å¼: [{"role": "user", "content": "..."}, {"role": "assistant", "content": "..."}]
    final_query: str,
    model,
    tokenizer,
    max_new_tokens=128,
):
    """
    åŸºäºå¯¹è¯å†å²å’Œå½“å‰æ¨¡ç³Šé—®é¢˜ï¼Œä½¿ç”¨ ChatGLM é‡å†™ä¸ºç‹¬ç«‹æ¸…æ™°é—®é¢˜
    """
    fallback_rewrite = final_query

    # æ„é€ æç¤ºè¯
    prompt = (
        "ä½ æ˜¯ä¸€ä¸ªç”µå•†å¹³å°æ™ºèƒ½å®¢æœçš„å¯¹è¯æ¸…æ™°åŒ–åŠ©æ‰‹ã€‚\n"
        "ç”¨æˆ·æå‡ºçš„é—®é¢˜å¯èƒ½å­˜åœ¨å¤æ‚æŒ‡ä»£ã€ä¸Šä¸‹æ–‡ä¾èµ–æˆ–è¡¨è¾¾æ¨¡ç³Šç­‰é—®é¢˜ã€‚\n"
        "ä½ éœ€è¦æ ¹æ®å¤šè½®å†å²å¯¹è¯ï¼Œä¸°å¯Œæ¶¦è‰²ç”¨æˆ·çš„å½“å‰é—®é¢˜ï¼Œä½¿å…¶æˆä¸ºä¸€ä¸ªæ¸…æ™°ã€å®Œæ•´çš„ç‹¬ç«‹é—®é¢˜ã€‚\n\n"
        "ä¸‹é¢æ˜¯è¦æ±‚ï¼š\n"
        "- å‡†ç¡®è§£æç”¨æˆ·çœŸå®æ„å›¾ï¼Œä½¿å¾—è¿™ä¸ªç‹¬ç«‹é—®é¢˜å°½é‡å®Œæ•´ï¼Œå°½å¯èƒ½åŒ…å«æ‰€æœ‰ä¿¡æ¯ï¼›\n"
        "- é—®é¢˜è¶Šä¸°å¯Œè¶Šå¥½ï¼Œç‰¹åˆ«æ˜¯è¦æ•æ‰åˆ°å…³é”®çš„æŒ‡ä»£ï¼Œåœºæ™¯ï¼Œç‰¹åˆ«é’ˆå¯¹çš„é—®é¢˜å’Œä¾‹å­ç­‰ç­‰ï¼›\n"
        "- ä¸å¯ä»¥æé€ ä¸å­˜åœ¨çš„ä¿¡æ¯ï¼›\n"
        "- ä¸æ·»åŠ è§£é‡Šã€æ³¨é‡Šã€å¼•å¯¼è¯­ç­‰ï¼Œåªè¾“å‡ºæ¶¦è‰²åçš„é—®é¢˜å¥ã€‚\n\n"
        "ä¸‹é¢æ˜¯å†å²å¯¹è¯ï¼š\n"
    )

    for turn in dialogue:
        role = "ç”¨æˆ·" if turn["speaker"] == "user" else "ç³»ç»Ÿ"
        content = turn["text"].replace("\n", " ").strip()
        prompt += f"{role}ï¼š{content}\n"

    prompt += f"ç”¨æˆ·å½“å‰é—®é¢˜æ˜¯ï¼š{final_query.strip()}\nè¯·ä½ éµå¾ªè¦æ±‚æ¶¦è‰²ä¸ºä¸€ä¸ªæ¸…æ™°ã€å®Œæ•´çš„ç‹¬ç«‹é—®é¢˜ï¼š"
    # print(f"ğŸ” é‡å†™æç¤ºè¯ï¼š\n{prompt[:256]}...")  # ä»…æ‰“å°å‰256å­—ç¬¦

    try:
        messages = [{"role": "user", "content": prompt}]
        inputs = tokenizer.apply_chat_template(
            messages,
            add_generation_prompt=True,
            return_tensors="pt",
            return_dict=True,
        ).to(model.device)

        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                do_sample=True,
                top_p=0.8,
                temperature=0.4
            )

        generated_ids = outputs[:, inputs["input_ids"].shape[1]:]
        response = tokenizer.decode(generated_ids[0], skip_special_tokens=True).strip()
        return response if response else fallback_rewrite
    except Exception as e:
        print(f"âš ï¸ ChatGLM é‡å†™å¤±è´¥: {e}ï¼Œè¿”å›åŸé—®é¢˜")
        return fallback_rewrite


def rewrite_query_vllm(
    dialogue: list,
    final_query: str,
    model: str = "glm",
    max_new_tokens: int = 128
) -> str:
    """
    åŸºäºå¯¹è¯å†å²å’Œå½“å‰æ¨¡ç³Šé—®é¢˜ï¼Œä½¿ç”¨ vLLM Chat æ¥å£é‡å†™ä¸ºç‹¬ç«‹æ¸…æ™°é—®é¢˜ï¼ˆå¸¦å¹¶å‘æ§åˆ¶ï¼‰
    """
    fallback_rewrite = final_query
    # æ„é€  prompt
    prompt = (
        "ä½ æ˜¯ä¸€ä¸ªé—®é¢˜é‡å†™APIï¼Œåªä¼šé‡å†™ä¼˜åŒ–æˆ–å¤è¯»ç”¨æˆ·çš„é—®é¢˜ã€‚\n"
        "ç”¨æˆ·æå‡ºçš„é—®é¢˜å¯èƒ½å­˜åœ¨å¤æ‚æŒ‡ä»£ã€ä¸Šä¸‹æ–‡ä¾èµ–æˆ–è¡¨è¾¾æ¨¡ç³Šç­‰é—®é¢˜ã€‚\n"
        "ä½ éœ€è¦æ ¹æ®å¤šè½®å†å²å¯¹è¯ï¼Œä¸°å¯Œæ¶¦è‰²ç”¨æˆ·çš„å½“å‰é—®é¢˜ï¼Œä½¿å…¶æˆä¸ºä¸€ä¸ªæ¸…æ™°ã€å®Œæ•´çš„ç‹¬ç«‹é—®é¢˜ã€‚\n\n"
        "ä¸‹é¢æ˜¯è¦æ±‚ï¼š\n"
        "- å‡†ç¡®è§£æç”¨æˆ·çœŸå®æ„å›¾ï¼Œä»å†å²ä¸­å‘æ˜æŒ‡ä»£å’Œæ„å›¾ï¼Œä½¿å¾—è¿™ä¸ªç‹¬ç«‹é—®é¢˜å°½é‡å®Œæ•´ï¼Œå°½å¯èƒ½åŒ…å«æ‰€æœ‰ä¿¡æ¯å…³é”®è¯,ç‰¹åˆ«æ˜¯è¦è¡¥å……å…³é”®çš„åŠ¨æœºï¼ŒæŒ‡ä»£å’Œåœºæ™¯ï¼ˆæ¶¦è‰²åçš„é—®é¢˜è‡³å°‘æ¶‰åŠï¼šç”¨æˆ·é¢å¯¹ä»€ä¹ˆå‰ç½®æƒ…å†µï¼Œå¼ºè°ƒäº†ä»€ä¹ˆé™åˆ¶ï¼Œæœ‰ä»€ä¹ˆç–‘é—®ç­‰ï¼‰ï¼Œä½†ä¸å¯ä»¥æé€ ä¸å­˜åœ¨çš„ä¿¡æ¯ï¼›\n"
        "- å¦‚æœå½“å‰é—®é¢˜æœ¬èº«æ˜¯ 1.éé—®é¢˜ï¼ˆå¦‚â€œä½ å¥½â€ç­‰å¯’æš„ï¼Œâ€œé€€å‡ºç³»ç»Ÿâ€ç­‰æŒ‡ä»¤ï¼Œâ€œå‘µå‘µâ€ç­‰æ— æ„ä¹‰çŒæ°´ï¼‰ï¼Œ2. éæŠ€æœ¯æ€§é—®é¢˜ï¼ˆâ€œä½ æ˜¯è°â€ï¼Œâ€œä½ æ˜¯AIå—â€ç­‰èº«ä»½è¯¢é—®ï¼Œâ€œäººç”Ÿçš„æ„ä¹‰æ˜¯ä»€ä¹ˆâ€ç­‰æ— å…³æ€§é—®é¢˜ï¼‰ï¼Œä¸è¿›è¡Œæ¶¦è‰²ç›´æ¥è¿”å›åŸå¥å­ï¼›\n"
        "- ä¸€å®šä¸€å®šä¸å¯ä»¥å›ç­”ç”¨æˆ·é—®é¢˜ï¼Œä½ åªå…³æ³¨é—®é¢˜æœ¬èº«çš„æ¶¦è‰²ï¼›\n"
        "- ä¸æ·»åŠ è§£é‡Šã€æ³¨é‡Šã€å¼•å¯¼è¯­ç­‰ï¼Œåªè¾“å‡ºæ¶¦è‰²åçš„é—®é¢˜å¥ã€‚\n\n"
        "ä¸‹é¢æ˜¯å†å²å¯¹è¯ï¼š\n"
    )

    for turn in dialogue:
        role = "ç”¨æˆ·" if turn.get("speaker") == "user" else "ç³»ç»Ÿ"
        content = turn.get("text", "").replace("\n", " ").strip()
        prompt += f"{role}ï¼š{content}\n"

    prompt += f"ç”¨æˆ·å½“å‰é—®é¢˜æ˜¯ï¼š{final_query.strip()}\nè¯·ä½ éµå¾ªè¦æ±‚æ¶¦è‰²ä¸ºä¸€ä¸ªæ¸…æ™°ã€å®Œæ•´çš„ç‹¬ç«‹é—®é¢˜ï¼š"

    payload = {
        "model": model,
        "messages": [{"role": "user", "content": prompt}],
        "max_tokens": max_new_tokens,
        "temperature": 0.4,
        "top_p": 0.8
    }
    try:
        start = time.time()
        response = requests.post(api_url, headers={"Content-Type": "application/json"}, json=payload, timeout=timeout)
        response.raise_for_status()
        result = response.json()["choices"][0]["message"]["content"].strip()
        duration = time.time() - start
        print(f"âœ… é‡å†™æˆåŠŸï¼ˆè€—æ—¶ {duration:.2f}sï¼‰")
        return result if result else fallback_rewrite
    except Exception as e:
        print(f"âš ï¸ vLLM é‡å†™å¤±è´¥: {e}")
        return fallback_rewrite


# å‡è¡¡è´Ÿè½½handler
def weighted_sample_without_replacement(servers: List[Dict[str, any]], tried: Set[str]) -> Optional[str]:
    """åœ¨æœªå°è¯•æœåŠ¡å™¨ä¸­æŒ‰æƒé‡éšæœºé‡‡æ ·ä¸€ä¸ª"""
    candidates = [(s["url"], s.get("weight", 1)) for s in servers if s["url"] not in tried]
    if not candidates:
        return None
    urls, weights = zip(*candidates)
    return random.choices(urls, weights=weights, k=1)[0]


async def call_vllm_with_retry_weighted(payload: dict, timeout: int = 15, max_retries: Optional[int] = None) -> dict:
    """
    å¸¦æƒé‡çš„vLLMå¼‚æ­¥è°ƒç”¨ï¼Œå¤±è´¥è‡ªåŠ¨é‡è¯•ï¼ŒæŒ‰æƒé‡é‡‡æ ·ä½†ä¸é‡å¤ã€‚
    """
    tried_urls = set()
    retries = max_retries or len(VLLM_SERVERS)

    errors = []

    for _ in range(retries):
        api_url = weighted_sample_without_replacement(VLLM_SERVERS, tried_urls)
        if api_url is None:
            break
        tried_urls.add(api_url)

        try:
            logger.debug(f"ğŸš€ è¯·æ±‚ vLLM: {api_url}")
            async with aiohttp.ClientSession() as session:
                async with session.post(api_url, json=payload, timeout=timeout) as resp:
                    resp.raise_for_status()
                    result = await resp.json()
                    return result  # âœ… æˆåŠŸ
        except Exception as e:
            logger.warning(f"âš ï¸ vLLM è¯·æ±‚å¤±è´¥: {api_url} â†’ {e}")
            errors.append((api_url, str(e)))
            continue

    # æ‰€æœ‰å°è¯•éƒ½å¤±è´¥
    error_msg = "æ‰€æœ‰ vLLM å®ä¾‹è¯·æ±‚å¤±è´¥: " + "; ".join([f"{url}: {err}" for url, err in errors])
    raise RuntimeError(error_msg)



# max_new_tokens=1024ä¸»è¦é˜²æ­¢å¸¦æ€ç»´é“¾è¾“å‡ºæº¢å‡º
async def rewrite_query_vllm_async(dialogue, final_query, model="glm", max_new_tokens=1024):
    fallback_rewrite = final_query
    banned_phrases = [
        "ä½ å¥½", "æ‚¨å¥½", "hi", "hello", "å“ˆå–½", "åœ¨å—", "å–‚", "è¯·é—®åœ¨å—", "æœ‰äººå—", "helloï¼Ÿ",
        "ä½ æ˜¯è°", "ä½ æ˜¯äººå—", "ä½ æ˜¯æœºå™¨äººå—", "ä½ æ˜¯AIå—", "ä½ å«ä»€ä¹ˆ", "ä½ æ˜¯å®¢æœå—", "ä½ æ˜¯æ™ºèƒ½åŠ©æ‰‹å—", "ä½ æ˜¯äººå·¥çš„å—", "ä½ èƒ½å¬æ‡‚æˆ‘è¯´è¯å—",
        "å‘µå‘µ", "å“ˆå“ˆ", "å—¯", "å“¼", "é¢", "å¥½å§", "æ— è¯­", "ã€‚ã€‚ã€‚", "...", "===", "ä½ çŒœ", "éšä¾¿", "çœ‹ä½ å’‹è¯´",
        "æµ‹è¯•", "test", "just testing", "éšä¾¿é—®é—®", "è¿™æ˜¯ä¸ªæµ‹è¯•", "debug", "çœ‹çœ‹ä½ æ€ä¹ˆå›ç­”",
        "ä»Šå¤©æ˜¯å‡ å·", "æ—¶é—´", "å¤©æ°”", "åŒ—äº¬å¤©æ°”", "ä»Šå¤©å¤©æ°”", "è®²ä¸ªç¬‘è¯", "èƒŒé¦–è¯—", "ç»™æˆ‘å”±é¦–æ­Œ", "æ¥æ®µrap",
        "é‡å¯ä¸€ä¸‹", "æ¸…é™¤ç¼“å­˜", "é€€å‡ºç³»ç»Ÿ", "ä¿å­˜æ–‡ä»¶", "æ‰“å¼€æµè§ˆå™¨", "è¿è¡Œä»£ç ", "æ‰§è¡Œè„šæœ¬", "å›ç­”é—®é¢˜",
        "ä½ æ‰®æ¼”è°", "å‡è®¾ä½ æ˜¯", "ä½ æ˜¯äººç±»", "å¦‚æœä½ æ˜¯æˆ‘", "ä»ä½ çš„è§’åº¦çœ‹", "ä½ ä½œä¸ºä¸€ä¸ªAI",
        "å­˜åœ¨çš„æ„ä¹‰æ˜¯ä»€ä¹ˆ", "äººç”Ÿçš„æ„ä¹‰", "ä»€ä¹ˆæ˜¯çœŸå®", "ä½ æ€ä¹ˆçœ‹è¿™ä¸ªä¸–ç•Œ", "ä½ è§‰å¾—æˆ‘æ˜¯è°",
        "å•†å“", "æœåŠ¡", "å¹³å°", "æŠ–éŸ³", "å°çº¢ä¹¦", "è§†é¢‘", "è§„åˆ™", "æ”¿ç­–", "æŠ¥è¡¨"
    ]

    if any(phrase == final_query for phrase in banned_phrases):
        logger.debug(f"ğŸ” å‘½ä¸­è¿‡æ»¤è¯ Query é‡å†™è·³è¿‡ï¼š{final_query}")
        return fallback_rewrite

    if len(dialogue) < 2:
        logger.debug(f"ğŸ” å¯¹è¯å†å²è¿‡çŸ­ï¼Œè·³è¿‡ Query é‡å†™ï¼š{final_query}")
        return fallback_rewrite

    system_prompt = (
        "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é—®é¢˜é‡å†™æ¨¡å—ï¼Œä¸“é—¨ç”¨äºå¤šè½®å¯¹è¯åœºæ™¯ä¸‹çš„æŒ‡ä»£è¡¥å…¨ä¸è¯­ä¹‰è¿˜åŸä»»åŠ¡ã€‚\n"
        "è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹è§„åˆ™æ‰§è¡Œï¼š\n\n"
        "ã€ä»»åŠ¡ç›®æ ‡ã€‘\n"
        "1. ä½ çš„ç›®æ ‡æ˜¯å‡†ç¡®è§£æç”¨æˆ·çœŸå®æ„å›¾ï¼Œä»å†å²ä¸­å‘æ˜æŒ‡ä»£å’Œæ„å›¾ï¼Œä½¿å¾—è¿™ä¸ªç‹¬ç«‹é—®é¢˜å°½é‡å®Œæ•´ï¼Œå°½å¯èƒ½åŒ…å«æ‰€æœ‰ä¿¡æ¯å…³é”®è¯,ç‰¹åˆ«æ˜¯è¦è¡¥å……å…³é”®çš„åŠ¨æœºï¼ŒæŒ‡ä»£å’Œåœºæ™¯ï¼ˆæ¶¦è‰²åçš„é—®é¢˜è‡³å°‘æ¶‰åŠï¼šç”¨æˆ·é¢å¯¹çš„å‰ç½®æƒ…å†µï¼Œä»€ä¹ˆé™åˆ¶ï¼Œä»€ä¹ˆç–‘é—®ç­‰ï¼‰ã€‚\n"
        "2. ä»…å½“å†å²ä¿¡æ¯èƒ½å¤Ÿæä¾›æ˜ç¡®ä¸Šä¸‹æ–‡æ—¶æ‰è¿›è¡Œè¡¥å…¨ï¼Œå¦åˆ™ä¿æŒå½“å‰é—®é¢˜ä¸å˜ã€‚\n"
        "3. å¯¹äºæ˜æ˜¾ä¸éœ€è¦è¡¥å…¨çš„é—®é¢˜å¦‚ï¼šè¯­æ°”è¯ï¼š\"å‘µå‘µ\"ï¼Œå‘½ä»¤ï¼š\"å…³æœº\"ï¼Œæ— å…³å†…å®¹ï¼š\"äººç”Ÿçš„æ„ä¹‰\"ç­‰ï¼Œè¿”å›åŸå¥\n"
        "4. ä¸è¿›è¡Œä»»ä½•æ— å…³å‘æŒ¥ã€æ‰©å†™ã€æ¶¦è‰²ã€ä¿®è¾æ€§æè¿°ã€è§£é‡Šã€æ€»ç»“ã€æ„Ÿæƒ…è‰²å½©ã€‚\n"
        "5. ä¸ç¼–é€ ä»»ä½•ä¸å­˜åœ¨çš„å‡è®¾èƒŒæ™¯æˆ–æ–°ä¿¡æ¯ã€‚\n"
        "6. è¾“å‡ºæ ¼å¼å¿…é¡»ä¸¥æ ¼éµå¾ªï¼šä»…è¾“å‡ºæœ€ç»ˆé‡å†™ç»“æœæ–‡æœ¬ï¼Œä¸åŒ…å«ä»»ä½•å‰ç¼€ã€æç¤ºè¯ã€è¯´æ˜æ€§æ–‡å­—æˆ–æ¢è¡Œç¬¦ã€‚é‡å†™çš„ç–‘é—®å¥ä»¥ï¼š\"æˆ‘æƒ³çŸ¥é“\"å¼€å¤´ï¼Œéç–‘é—®å¥ä½ è‡ªè¡Œé€‚é…\n"
        "7. å½“æ— éœ€é‡å†™æ—¶ï¼Œç›´æ¥è¾“å‡ºåŸé—®é¢˜ã€‚\n\n"
        "ã€é‡å†™ç¤ºä¾‹ã€‘\n"
        "å†å²å¯¹è¯ï¼š\nç”¨æˆ·ï¼šå°çº¢é‹ä»€ä¹ˆæ—¶å€™æœ‰è´§ï¼Ÿ\nç³»ç»Ÿï¼šè¯·é—®æ‚¨æŒ‡çš„æ˜¯å“ªæ¬¾å°çº¢é‹ï¼Ÿ\nç”¨æˆ·ï¼šå°±æ˜¯ä¸Šæ¬¡ç¼ºè´§é‚£æ¬¾\nå½“å‰é—®é¢˜ï¼šå°±æ˜¯ä¸Šæ¬¡ç¼ºè´§é‚£æ¬¾\né‡å†™ç»“æœï¼šæˆ‘æƒ³çŸ¥é“ä¸Šæ¬¡ç¼ºè´§çš„é‚£æ¬¾å°çº¢é‹ä»€ä¹ˆæ—¶å€™æœ‰è´§ï¼Ÿ\n\n"
        "å†å²å¯¹è¯ï¼š\nç”¨æˆ·ï¼šæˆ‘ä»¬å‰å‡ å¤©è´¦å·è¢«é™æµäº†ï¼Œä¸çŸ¥é“ä»€ä¹ˆåŸå› \nç³»ç»Ÿï¼šé™æµå¯èƒ½æ˜¯å› ä¸ºç´ æè¿è§„æˆ–è´¦æˆ·è¡¨ç°ä¸ä½³ã€‚\nç”¨æˆ·ï¼šé‚£æˆ‘ä»¬è¿˜æœ‰å…¶ä»–è®¡åˆ’ï¼Œèƒ½æŠ•æ”¾å—ï¼Ÿ\nå½“å‰é—®é¢˜ï¼šé‚£æˆ‘ä»¬è¿˜æœ‰å…¶ä»–è®¡åˆ’ï¼Œèƒ½æŠ•æ”¾å—ï¼Ÿ\né‡å†™ç»“æœï¼šæˆ‘æƒ³çŸ¥é“å½“è´¦å·å¤„äºé™æµçŠ¶æ€æ—¶ï¼Œæ˜¯å¦å¯ä»¥ç»§ç»­å¼€å¯æ–°çš„å¹¿å‘ŠæŠ•æ”¾è®¡åˆ’ï¼Ÿ\n\n"
        "å†å²å¯¹è¯ï¼š\nç”¨æˆ·ï¼šä½ æ˜¯è°ï¼Ÿ\nå½“å‰é—®é¢˜ï¼šä½ æ˜¯è°ï¼Ÿ\né‡å†™ç»“æœï¼šä½ æ˜¯è°ï¼Ÿ\n\n"
        "å†å²å¯¹è¯ï¼š\nç”¨æˆ·ï¼šå¼€å¿ƒ\nå½“å‰é—®é¢˜ï¼šå¼€å¿ƒ\né‡å†™ç»“æœï¼šå¼€å¿ƒ\n\n"
        "å†å²å¯¹è¯ï¼š\nç”¨æˆ·ï¼šæˆ‘è¦ä¹°å®ƒ\nç³»ç»Ÿï¼šè¯·é—®æ‚¨æŒ‡çš„æ˜¯å“ªæ¬¾å•†å“ï¼Ÿ\nç”¨æˆ·ï¼šä¹‹å‰æ¨èçš„é‚£åŒ\nå½“å‰é—®é¢˜ï¼šä¹‹å‰æ¨èçš„é‚£åŒ\né‡å†™ç»“æœï¼šæˆ‘æƒ³è¦ä¹°ä½ ä¹‹å‰æ¨èçš„é‚£åŒé‹ã€‚\n\n"
        "æ³¨æ„ï¼šä¸¥æ ¼æŒ‰ç…§ä»¥ä¸Šé£æ ¼å·¥ä½œï¼Œç¦æ­¢ä»»ä½•é¢å¤–è¾“å‡ºã€‚"
    )

    history_content = ""
    for turn in dialogue:
        role = "ç”¨æˆ·" if turn.get("speaker") == "user" else "ç³»ç»Ÿ"
        content = turn.get("text", "").replace("\n", " ").strip()
        history_content += f"{role}ï¼š{content}\n"

    current_question = f"å½“å‰é—®é¢˜ï¼š{final_query.strip()}"

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": f"{history_content}\n{current_question}\nè¯·ä½ æ ¹æ®ä»»åŠ¡è§„åˆ™è¾“å‡ºé‡å†™ç»“æœã€‚"}
    ]
    payload = {
        "model": model,
        "messages": messages,
        "max_tokens": max_new_tokens,
        "temperature": 0.2,
        "top_p": 0.5,
        "top_k": -1,
    }

    try:
        async with sem:
            start = time.time()
            result = await call_vllm_with_retry_weighted(payload, timeout=CONFIG.get("vllm_timeout", 60))
            rewritten = result["choices"][0]["message"]["content"].strip()
            # å¦‚æœæœ‰æ€ç»´é“¾
            rewritten = rewritten.split("</think>")[-1].strip() if "</think>" in rewritten else rewritten.strip()
            logger.debug(f"{final_query} é‡å†™æˆåŠŸï¼Œç”¨æ—¶ {time.time() - start:.2f}sï¼Œç»“æœï¼š{rewritten}")
            return rewritten or fallback_rewrite
    except Exception as e:
        logger.error(f"âš ï¸ é‡å†™è¯·æ±‚å¤±è´¥ï¼Œè¿”å›åŸé—®é¢˜ï¼š{e}")
        return fallback_rewrite



async def generate_summary_vllm_async(text, page_url, model="glm", max_new_tokens=1024):
    """
    ä½¿ç”¨ vLLM å¼‚æ­¥æ¥å£ç”Ÿæˆæ‘˜è¦ï¼Œæ”¯æŒå¤šæœºè·¯ç”±ä¸å¤±è´¥é‡è¯•ã€‚
    """
    category = infer_chunk_category(page_url)
    text = text.strip().replace("\x00", "")
    fallback_summary = text[:max_new_tokens]

    # æ„å»º system + user æ ¼å¼çš„ prompt
    system_prompt = (
        "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½çš„å†…å®¹æ‘˜è¦åŠ©æ‰‹ï¼Œä¸“é—¨ç”¨äºæç‚¼ç”µå•†å¹³å°æ–‡æ¡£çš„ä¸»è¦ä¿¡æ¯ã€‚\n"
        "ä½ éœ€è¦æ ¹æ®ç»™å‡ºçš„æ–‡æ¡£å†…å®¹ï¼Œåœ¨ä¿ç•™åŸæ„çš„å‰æä¸‹å‹ç¼©æˆä¸€å¥è¯æ‘˜è¦ã€‚\n\n"
        "ã€ä»»åŠ¡è¦æ±‚ã€‘\n"
        "1. æ‘˜è¦éœ€å‡†ç¡®è¦†ç›–åŸæ–‡æ ¸å¿ƒä¿¡æ¯ï¼Œä¸å¾—æ·»åŠ ã€ç¼–é€ ã€æ‰©å†™ã€‚\n"
        "2. ä¸å…è®¸é‡å¤ç²˜è´´åŸæ–‡åŸå¥æˆ–å†—ä½™å†…å®¹ã€‚\n"
        "3. è¯­è¨€é£æ ¼åº”ç®€æ´ã€æ¸…æ™°ã€æ— ä¿®è¾å’Œä¸»è§‚è‰²å½©ï¼Œä½¿ç”¨ç®€ä½“ä¸­æ–‡ã€‚\n"
        "4. è¾“å‡ºæ ¼å¼ä¸ºä¸€æ•´å¥è¯ï¼Œä¸åŒ…å«å‰ç¼€ã€é¡¹ç›®ç¬¦å·æˆ–æ¢è¡Œã€‚\n"
        "5. å­—æ•°æ§åˆ¶åœ¨ä¸è¶…è¿‡æŒ‡å®šä¸Šé™ï¼Œèƒ½çŸ­åˆ™çŸ­ï¼Œå°½é‡ç²¾å‡†ã€‚\n"
        "6. å½“æ–‡æœ¬æ— æ˜æ˜¾ä¸­å¿ƒå†…å®¹æˆ–æ ¼å¼å¼‚å¸¸æ—¶ï¼Œè¯·ä»ä¸­æç‚¼å…³é”®è¯æˆ–ä¸»æ—¨è¿›è¡Œç®€è¦æ€»ç»“ã€‚"
    )

    user_prompt = (
        f"æ–‡æ¡£è·¯å¾„ï¼š{page_url}\n"
        f"æ‰€å±ç±»ç›®ï¼š{category}\n"
        f"æ–‡æœ¬å†…å®¹ï¼š{text}"
    )

    payload = {
        "model": model,
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ],
        "max_tokens": max_new_tokens,
        "temperature": 0.4,
        "top_p": 0.8,
    }

    try:
        async with sem:
            start = time.time()
            logger.debug(f"ğŸ“© vLLM å¼‚æ­¥æ‘˜è¦è¯·æ±‚: {page_url + ' ' + text[:64]}")
            result = await call_vllm_with_retry_weighted(payload, timeout=CONFIG.get("vllm_timeout", 60))
            summary = result["choices"][0]["message"]["content"].strip()
            duration = time.time() - start
            logger.debug(f"âœ… vLLM å¼‚æ­¥æ‘˜è¦æˆåŠŸ (è€—æ—¶ {duration:.2f}s)ï¼Œ{page_url + 'ï¼š' + text[:64]}ã€‚æ‘˜è¦å†…å®¹: {summary[:64]}...")
            return summary or fallback_summary
    except Exception as e:
        logger.error(f"âš ï¸ vLLM å¼‚æ­¥æ‘˜è¦å¤±è´¥: {e}ï¼Œè¿”å›æˆªæ–­æ–‡æœ¬")
        return fallback_summary




async def get_embeddings_from_vllm_async(
    texts: list[str],
    url: str,
    timeout: int = 10,
    max_concurrent_tasks: int = 16
) -> list[list[float]]:
    async def _fetch(text: str) -> list[float]:
        payload = {"input": text}
        async with sem:
            session = await get_aiohttp_session()
            try:
                async with session.post(url, json=payload, timeout=timeout) as resp:
                    resp.raise_for_status()
                    data = await resp.json()
                    # å‡è®¾æ¥å£è¿”å› {"data":[{"embedding": [...]}, ...]}
                    return data["data"][0]["embedding"]
            except Exception as e:
                logger.error(f"âŒ æ–‡æœ¬æ¨ç†å¤±è´¥: {e} | text={text!r}")
                raise

    # åˆ›å»ºå¹¶å‘ä»»åŠ¡
    tasks = [asyncio.create_task(_fetch(txt)) for txt in texts]
    # å¹¶å‘æ‰§è¡Œå¹¶æ”¶é›†ç»“æœ
    embeddings = await asyncio.gather(*tasks)
    return embeddings



def get_embedding_from_vllm(text: str, url) -> list[float]:
        "" "ä»vLLMæœåŠ¡è·å–æ–‡æœ¬åµŒå…¥å‘é‡ """
        payload = {"input": [text]}
        try:
            resp = requests.post(url, json=payload, timeout=5)
            resp.raise_for_status()
            data = resp.json()
            return data["data"][0]["embedding"]
        except requests.exceptions.Timeout:
            logger.error(f"vLLMè¯·æ±‚è¶…æ—¶ | è¶…æ—¶æ—¶é—´: 5s")
            raise
        except requests.exceptions.HTTPError as e:
            logger.error(f"vLLM HTTPé”™è¯¯ | çŠ¶æ€ç : {resp.status_code} | å“åº”: {resp.text}")
            raise
        except Exception as e:
            logger.error(f"vLLMåµŒå…¥å¤±è´¥ | é”™è¯¯: {str(e)}")
            raise
