# -*- coding: utf-8 -*-
"""
å¤§æ¨¡å‹æ‘˜è¦ç”Ÿæˆä¸å¤šè½®é—®ç­”é‡å†™æ¨¡å—ï¼ˆChatGLM / vLLM æ”¯æŒï¼‰

æœ¬æ¨¡å—å°è£…äº†ä½¿ç”¨ ChatGLM ä¸ vLLM æ¥å£å®Œæˆä»¥ä¸‹ä»»åŠ¡çš„èƒ½åŠ›ï¼š
1. æ–‡æ¡£å—æ‘˜è¦ç”Ÿæˆï¼ˆæ”¯æŒå¹¶å‘æ§åˆ¶å’Œç±»å‹è‡ªé€‚åº”ï¼‰
2. é—®é¢˜ç”Ÿæˆï¼ˆæ ¹æ®æ–‡æ¡£å—è‡ªåŠ¨ç”Ÿæˆæ½œåœ¨ç”¨æˆ·é—®é¢˜ï¼‰
3. å¤šè½®å¯¹è¯é‡å†™ï¼ˆå°†ç”¨æˆ·æ¨¡ç³Šæé—®é‡å†™ä¸ºç‹¬ç«‹æ¸…æ™°é—®é¢˜ï¼‰
4. å‘é‡ç”Ÿæˆï¼ˆåŸºäº vLLM Embedding æ¥å£ï¼‰

æ¨¡å—äº®ç‚¹ï¼š
------------------------------------------------
- æ”¯æŒ ChatGLM æœ¬åœ°æ¨¡å‹ä¸ vLLM éƒ¨ç½²æœåŠ¡ä¸¤ç§è°ƒç”¨æ–¹å¼
- æ‘˜è¦/é—®å¥ç”Ÿæˆæ ¹æ® URL è·¯å¾„æ™ºèƒ½åˆ†ç±»ï¼ˆè§„åˆ™ç±» / æ“ä½œç±» / ä¿¡æ¯ç±» / æ³›ç”¨ç±»ï¼‰
- å¤šè½® query é‡å†™ä»»åŠ¡æ”¯æŒä¸Šä¸‹æ–‡èåˆï¼Œæç¤ºè¯ç»è¿‡ prompt tuning ä¼˜åŒ–
- æä¾› vLLM å¹¶å‘æ§åˆ¶æœºåˆ¶ï¼ˆé€šè¿‡ asyncio.Semaphore å®ç°è¯·æ±‚é€Ÿç‡è°ƒæ§ï¼‰
- æ”¯æŒ vLLM åµŒå…¥æ¥å£ï¼Œå¯ç”¨äºåç»­å‘é‡åŒ–æœç´¢

ä¸»è¦å‡½æ•°è¯´æ˜ï¼š
------------------------------------------------
1. æ‘˜è¦ç”Ÿæˆï¼š
   - `generate_summary_vllm`: ä½¿ç”¨ vLLM æ¥å£ç”Ÿæˆæ‘˜è¦ï¼Œæ”¯æŒè¶…æ—¶ä¸å¹¶å‘é™åˆ¶
   - `generate_summary_ChatGLM`: ä½¿ç”¨æœ¬åœ° ChatGLM æ¨¡å‹ç”Ÿæˆæ‘˜è¦

2. é—®é¢˜ç”Ÿæˆï¼š
   - `generate_question_ChatGLM`: æ ¹æ®æ–‡æ¡£å†…å®¹å’Œç±»åˆ«ç”Ÿæˆä¸€ä¸ªä»£è¡¨æ€§é—®é¢˜

3. å¤šè½®å¯¹è¯é‡å†™ï¼š
   - `rewrite_query_ChatGLM`: ä½¿ç”¨ ChatGLM å¯¹è¯æ¨¡æ¿æ”¹å†™ç”¨æˆ·æ¨¡ç³Šé—®é¢˜
   - `rewrite_query_vllm`: ä½¿ç”¨ vLLM Chat API æ”¹å†™ç”¨æˆ·æ¨¡ç³Šé—®é¢˜

4. å…¶ä»–è¾…åŠ©ï¼š
   - `infer_chunk_category`: æ ¹æ® page_url åˆ†ç±»æ–‡æ¡£å†…å®¹ï¼ˆè§„åˆ™/æ“ä½œ/ä¿¡æ¯/æ³›ç”¨ï¼‰
   - `get_embedding_from_vllm`: è°ƒç”¨ vLLM embedding æ¥å£è·å–æ–‡æœ¬å‘é‡ï¼ˆé€‚é… BCE æ¨¡å‹ï¼‰

é…ç½®ä¾èµ–é¡¹ï¼š
------------------------------------------------
- `CONFIG` ä¸­å¯é…ç½®ï¼š
  - `"vllm_api_url"`ï¼švLLM æ¨ç†æœåŠ¡åœ°å€
  - `"vllm_max_concurrent_requests"`ï¼šæœ€å¤§å¹¶å‘æ•°
  - `"vllm_timeout"`ï¼šè¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰

"""


import httpx
import torch
import requests
import time
import asyncio
from utils.config import CONFIG, get_aiohttp_session, close_aiohttp_session, sem, logger
import aiohttp


timeout = CONFIG.get("vllm_timeout", 60)
api_url = CONFIG.get("vllm_api_url", "http://localhost:8011/v1/chat/completions")


# ======================== æ–‡æ¡£å—åˆ†ç±»å‡½æ•° ========================
def infer_chunk_category(page_url):
    if any(k in page_url for k in ["è§„åˆ™", "åˆ¶åº¦", "æ³•å¾‹", "å®¡æ ¸"]):
        return "è§„åˆ™ç±»"
    elif any(k in page_url for k in ["ä½¿ç”¨", "æŒ‡å—", "å¸®åŠ©", "æ“ä½œ", "åŠŸèƒ½"]):
        return "æ“ä½œç±»"
    elif any(k in page_url for k in ["ç”Ÿæ€", "è§’è‰²", "ç­–ç•¥", "æ¨å¹¿", "å¹³å°ä¿¡æ¯"]):
        return "ä¿¡æ¯ç±»"
    else:
        return "æ³›ç”¨ç±»"




def get_embedding_from_vllm(text: str) -> list[float]:
    url = "http://0.0.0.0:8010/v1/embeddings"
    payload = {
        "model": "/home/algo/AD_agent/models/bce-embedding-base_v1",
        "input": [text],    # ä¸€å®šè¦æ˜¯åˆ—è¡¨
    }
    resp = requests.post(url, json=payload, timeout=5)
    resp.raise_for_status()
    data = resp.json()
    return data["data"][0]["embedding"]
    


# ======================== vLLM æ‘˜è¦ç”Ÿæˆå‡½æ•° ========================
def generate_summary_vllm(text, page_url, max_new_tokens=150, model="glm") -> str:
    """ä½¿ç”¨ HTTP è°ƒç”¨ vLLM å¹¶å‘å—æ§æ‘˜è¦ç”Ÿæˆ"""
    if len(text) < max_new_tokens * 2:
        print("âš ï¸ æ–‡æœ¬é•¿åº¦ä¸è¶³ï¼Œä½¿ç”¨åŸæ–‡æœ¬")
        return text[:max_new_tokens]

    category = infer_chunk_category(page_url)
    text = text.strip().replace("\x00", "")

    prompt = (
        f"ä½ æ­£åœ¨å¤„ç†ä¸€ç¯‡ç”µå•†å¹³å°çš„çŸ¥è¯†å†…å®¹ï¼Œå±äºâ€œ{category}â€ç±»ã€‚\n"
        f"è¯·ä½ æ ¹æ®ä¸‹æ–¹å†…å®¹æç‚¼å…¶ä¸»è¦ä¿¡æ¯ï¼Œè¦æ±‚å¦‚ä¸‹ï¼š\n"
        f"1. æ¦‚æ‹¬è¦ç‚¹ï¼Œä¸è¦é‡å¤åŸæ–‡åŸå¥ï¼›\n"
        f"2. æ€»é•¿åº¦ä¸è¶…è¿‡{max_new_tokens}å­—ï¼Œä½¿ç”¨ç®€ä½“ä¸­æ–‡ï¼›\n"
        f"3. è¾“å‡ºæ ¼å¼ä¸ºå®Œæ•´ä¸€å¥è¯ã€‚\n"
        f"ğŸ“‚ æ¥æºè·¯å¾„ï¼š{page_url}\n"
        f"ğŸ“„ å†…å®¹ï¼š\n{text}"
    )

    payload = {
        "model": model,
        "messages": [{"role": "user", "content": prompt}],
        "max_tokens": max_new_tokens,
        "temperature": 0.4,
        "top_p": 0.8
    }

    api_url = CONFIG.get("vllm_api_url", "http://localhost:8000/v1/chat/completions")

    try:
        start = time.time()
        response = requests.post(api_url, headers={"Content-Type": "application/json"}, json=payload, timeout=60)
        response.raise_for_status()
        result = response.json()["choices"][0]["message"]["content"].strip()
        duration = time.time() - start
        print(f"âœ… vLLMæ‘˜è¦æˆåŠŸ (è€—æ—¶ {duration:.2f}s)")
        return result or text[:max_new_tokens]
    except Exception as e:
        print(f"âš ï¸ vLLM æ‘˜è¦ç”Ÿæˆå¤±è´¥: {e}ï¼Œfallback åˆ°æˆªæ–­æ–‡æœ¬")
        return text[:max_new_tokens]



async def generate_summary_vllm_async(text, page_url, model="glm", max_new_tokens=150):
    """
    çœŸæ­£å¼‚æ­¥å¹¶å‘è°ƒç”¨ vLLM æ¥å£ç”Ÿæˆæ‘˜è¦
    """
    category = infer_chunk_category(page_url)
    text = text.strip().replace("\x00", "")

    prompt = (
        f"ä½ æ­£åœ¨å¤„ç†ä¸€ç¯‡ç”µå•†å¹³å°çš„çŸ¥è¯†å†…å®¹ï¼Œå±äºâ€œ{category}â€ç±»ã€‚\n"
        f"è¯·ä½ æ ¹æ®ä¸‹æ–¹å†…å®¹æç‚¼å…¶ä¸»è¦ä¿¡æ¯ï¼Œè¦æ±‚å¦‚ä¸‹ï¼š\n"
        f"1. æ¦‚æ‹¬è¦ç‚¹ï¼Œä¸è¦é‡å¤åŸæ–‡åŸå¥ï¼›\n"
        f"2. æ€»é•¿åº¦ä¸è¶…è¿‡{max_new_tokens}å­—ï¼Œä½¿ç”¨ç®€ä½“ä¸­æ–‡ï¼›\n"
        f"3. è¾“å‡ºæ ¼å¼ä¸ºå®Œæ•´ä¸€å¥è¯ã€‚\n"
        f"ğŸ“‚ æ¥æºè·¯å¾„ï¼š{page_url}\n"
        f"ğŸ“„ å†…å®¹ï¼š\n{text}"
    )

    payload = {
        "model": model,
        "messages": [{"role": "user", "content": prompt}],
        "max_tokens": max_new_tokens,
        "temperature": 0.4,
        "top_p": 0.8,
    }

    url = CONFIG.get("vllm_api_url", "http://localhost:8000/v1/chat/completions")
    headers = {"Content-Type": "application/json"}

    try:
        async with aiohttp.ClientSession(headers=headers) as session:
            async with session.post(url, json=payload, timeout=CONFIG.get("vllm_timeout", 60)) as resp:
                resp.raise_for_status()
                result = await resp.json()
                return result["choices"][0]["message"]["content"].strip()
    except Exception as e:
        print(f"âš ï¸ vLLM å¼‚æ­¥æ‘˜è¦å¤±è´¥: {e}ï¼Œè¿”å›æˆªæ–­æ–‡æœ¬")
        return text[:max_new_tokens]


# ======================== ChatGLM æ‘˜è¦ç”Ÿæˆå‡½æ•° ========================
def generate_summary_ChatGLM(
    text,
    page_url,
    model,
    tokenizer,
    max_new_tokens=150,
):
    if len(text) < max_new_tokens * 2:
        print("âš ï¸ æ–‡æœ¬é•¿åº¦ä¸è¶³ï¼Œä½¿ç”¨åŸæ–‡æœ¬")
        return text[:max_new_tokens]

    category = infer_chunk_category(page_url)
    text = text.strip().replace("\x00", "")

    prompt = (
            f"ä½ æ­£åœ¨å¤„ç†ä¸€ç¯‡ç”µå•†å¹³å°çš„çŸ¥è¯†å†…å®¹ï¼Œå±äºâ€œ{category}â€ç±»ã€‚\n"
            f"è¯·ä½ æ ¹æ®ä¸‹æ–¹å†…å®¹æç‚¼å…¶ä¸»è¦ä¿¡æ¯ï¼Œè¦æ±‚å¦‚ä¸‹ï¼š\n"
            f"1. æ¦‚æ‹¬è¦ç‚¹ï¼Œä¸è¦é‡å¤åŸæ–‡åŸå¥ï¼›\n"
            f"2. æ€»é•¿åº¦ä¸è¶…è¿‡{max_new_tokens}å­—ï¼Œä½¿ç”¨ç®€ä½“ä¸­æ–‡ï¼›\n"
            f"3. è¾“å‡ºæ ¼å¼ä¸ºå®Œæ•´ä¸€å¥è¯ã€‚\n"
            f"ğŸ“‚ æ¥æºè·¯å¾„ï¼š{page_url}\n"
            f"ğŸ“„ å†…å®¹ï¼š\n{text}"
        )

    try:
        messages = [{"role": "user", "content": prompt}]
        inputs = tokenizer.apply_chat_template(
            messages,
            add_generation_prompt=True,
            return_tensors="pt",
            return_dict=True,
        ).to(model.device)

        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                do_sample=True,
                top_p=0.8,
                temperature=0.4
            )
        # è£å‰ªæ‰ prompt éƒ¨åˆ†
        generated_ids = outputs[:, inputs["input_ids"].shape[1]:]
        response = tokenizer.decode(generated_ids[0], skip_special_tokens=True).strip()
        return response if response else text[:max_new_tokens]
    except Exception as e:
        print(f"âš ï¸ ChatGLM æ‘˜è¦ç”Ÿæˆå¤±è´¥: {e}ï¼Œä½¿ç”¨ fallback")
        return text[:max_new_tokens]



# ======================== ChatGLM é—®é¢˜ç”Ÿæˆå‡½æ•° ========================
def generate_question_ChatGLM(
    text,
    page_url,
    model,
    tokenizer,
    max_new_tokens=64,
    fallback_question="è¯¥å†…å®¹å¯æ„é€ ç›¸å…³ä¸šåŠ¡é—®é¢˜"
):

    category = infer_chunk_category(page_url)
    text = text.strip().replace("\x00", "")

    if category == "è§„åˆ™ç±»":
        hint = "å¹³å°æ˜¯å¦å…è®¸ã€è§„åˆ™çº¦æŸã€è¿è§„å¤„ç†"
    elif category == "æ“ä½œç±»":
        hint = "å¦‚ä½•æ“ä½œã€æ˜¯å¦å¯ç”¨ã€ä½¿ç”¨æ–¹æ³•"
    elif category == "ä¿¡æ¯ç±»":
        hint = "å¹³å°èƒŒæ™¯ã€äº§å“å®šä½ã€ç­–ç•¥è®¾è®¡"
    else:
        hint = "ç”¨æˆ·å®é™…å¯èƒ½ä¼šé—®çš„é—®é¢˜"

    prompt = (
        f"ä½ æ˜¯ä¸€ä¸ªç”µå•†å¹³å°çŸ¥è¯†é—®ç­”æ„å»ºåŠ©æ‰‹ï¼Œè¯·æ ¹æ®ä»¥ä¸‹å†…å®¹ç”Ÿæˆä¸€ä¸ªæœ‰å®é™…ä»·å€¼çš„ç”¨æˆ·é—®é¢˜ã€‚\n"
        f"è¦æ±‚ï¼š\n"
        f"- é—®é¢˜åº”ä½“ç°â€œ{hint}â€ï¼›\n"
        f"- ç¦æ­¢å¤è¿°åŸæ–‡ï¼Œåº”æç‚¼æ“ä½œã€åˆ¤æ–­æˆ–å’¨è¯¢ç‚¹ï¼›\n"
        f"- åªè¾“å‡ºä¸€ä¸ªç®€ä½“ä¸­æ–‡é—®é¢˜å¥ï¼Œä¸åŠ è¯´æ˜ã€‚\n"
        f"ğŸ“‚ æ¥æºè·¯å¾„ï¼š{page_url}\n"
        f"ğŸ“„ å†…å®¹ï¼š\n{text}"
    )

    try:
        messages = [{"role": "user", "content": prompt}]
        inputs = tokenizer.apply_chat_template(
            messages,
            add_generation_prompt=True,
            return_tensors="pt",
            return_dict=True,
        ).to(model.device)

        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                do_sample=True,
                top_p=0.9,
                temperature=0.7
            )
        generated_ids = outputs[:, inputs["input_ids"].shape[1]:]
        response = tokenizer.decode(generated_ids[0], skip_special_tokens=True).strip()
        return response if response else fallback_question
    except Exception as e:
        print(f"âš ï¸ ChatGLM é—®é¢˜ç”Ÿæˆå¤±è´¥: {e}ï¼Œä½¿ç”¨ fallback")
        return fallback_question



# ======================== ChatGLM å¤šè½® Query Rewriting å‡½æ•° ========================
def rewrite_query_ChatGLM(
    dialogue: list,  # æ ¼å¼: [{"role": "user", "content": "..."}, {"role": "assistant", "content": "..."}]
    final_query: str,
    model,
    tokenizer,
    max_new_tokens=128,
):
    """
    åŸºäºå¯¹è¯å†å²å’Œå½“å‰æ¨¡ç³Šé—®é¢˜ï¼Œä½¿ç”¨ ChatGLM é‡å†™ä¸ºç‹¬ç«‹æ¸…æ™°é—®é¢˜
    """
    fallback_rewrite = final_query

    # æ„é€ æç¤ºè¯
    prompt = (
        "ä½ æ˜¯ä¸€ä¸ªç”µå•†å¹³å°æ™ºèƒ½å®¢æœçš„å¯¹è¯æ¸…æ™°åŒ–åŠ©æ‰‹ã€‚\n"
        "ç”¨æˆ·æå‡ºçš„é—®é¢˜å¯èƒ½å­˜åœ¨å¤æ‚æŒ‡ä»£ã€ä¸Šä¸‹æ–‡ä¾èµ–æˆ–è¡¨è¾¾æ¨¡ç³Šç­‰é—®é¢˜ã€‚\n"
        "ä½ éœ€è¦æ ¹æ®å¤šè½®å†å²å¯¹è¯ï¼Œä¸°å¯Œæ¶¦è‰²ç”¨æˆ·çš„å½“å‰é—®é¢˜ï¼Œä½¿å…¶æˆä¸ºä¸€ä¸ªæ¸…æ™°ã€å®Œæ•´çš„ç‹¬ç«‹é—®é¢˜ã€‚\n\n"
        "ä¸‹é¢æ˜¯è¦æ±‚ï¼š\n"
        "- å‡†ç¡®è§£æç”¨æˆ·çœŸå®æ„å›¾ï¼Œä½¿å¾—è¿™ä¸ªç‹¬ç«‹é—®é¢˜å°½é‡å®Œæ•´ï¼Œå°½å¯èƒ½åŒ…å«æ‰€æœ‰ä¿¡æ¯ï¼›\n"
        "- é—®é¢˜è¶Šä¸°å¯Œè¶Šå¥½ï¼Œç‰¹åˆ«æ˜¯è¦æ•æ‰åˆ°å…³é”®çš„æŒ‡ä»£ï¼Œåœºæ™¯ï¼Œç‰¹åˆ«é’ˆå¯¹çš„é—®é¢˜å’Œä¾‹å­ç­‰ç­‰ï¼›\n"
        "- ä¸å¯ä»¥æé€ ä¸å­˜åœ¨çš„ä¿¡æ¯ï¼›\n"
        "- ä¸æ·»åŠ è§£é‡Šã€æ³¨é‡Šã€å¼•å¯¼è¯­ç­‰ï¼Œåªè¾“å‡ºæ¶¦è‰²åçš„é—®é¢˜å¥ã€‚\n\n"
        "ä¸‹é¢æ˜¯å†å²å¯¹è¯ï¼š\n"
    )

    for turn in dialogue:
        role = "ç”¨æˆ·" if turn["speaker"] == "user" else "ç³»ç»Ÿ"
        content = turn["text"].replace("\n", " ").strip()
        prompt += f"{role}ï¼š{content}\n"

    prompt += f"ç”¨æˆ·å½“å‰é—®é¢˜æ˜¯ï¼š{final_query.strip()}\nè¯·ä½ éµå¾ªè¦æ±‚æ¶¦è‰²ä¸ºä¸€ä¸ªæ¸…æ™°ã€å®Œæ•´çš„ç‹¬ç«‹é—®é¢˜ï¼š"
    # print(f"ğŸ” é‡å†™æç¤ºè¯ï¼š\n{prompt[:256]}...")  # ä»…æ‰“å°å‰256å­—ç¬¦

    try:
        messages = [{"role": "user", "content": prompt}]
        inputs = tokenizer.apply_chat_template(
            messages,
            add_generation_prompt=True,
            return_tensors="pt",
            return_dict=True,
        ).to(model.device)

        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                do_sample=True,
                top_p=0.8,
                temperature=0.4
            )

        generated_ids = outputs[:, inputs["input_ids"].shape[1]:]
        response = tokenizer.decode(generated_ids[0], skip_special_tokens=True).strip()
        return response if response else fallback_rewrite
    except Exception as e:
        print(f"âš ï¸ ChatGLM é‡å†™å¤±è´¥: {e}ï¼Œè¿”å›åŸé—®é¢˜")
        return fallback_rewrite


def rewrite_query_vllm(
    dialogue: list,
    final_query: str,
    model: str = "glm",
    max_new_tokens: int = 128
) -> str:
    """
    åŸºäºå¯¹è¯å†å²å’Œå½“å‰æ¨¡ç³Šé—®é¢˜ï¼Œä½¿ç”¨ vLLM Chat æ¥å£é‡å†™ä¸ºç‹¬ç«‹æ¸…æ™°é—®é¢˜ï¼ˆå¸¦å¹¶å‘æ§åˆ¶ï¼‰
    """
    fallback_rewrite = final_query
    # æ„é€  prompt
    prompt = (
        "ä½ æ˜¯ä¸€ä¸ªç”µå•†å¹³å°æ™ºèƒ½å®¢æœçš„å¯¹è¯æ¸…æ™°åŒ–åŠ©æ‰‹ã€‚\n"
        "ç”¨æˆ·æå‡ºçš„é—®é¢˜å¯èƒ½å­˜åœ¨å¤æ‚æŒ‡ä»£ã€ä¸Šä¸‹æ–‡ä¾èµ–æˆ–è¡¨è¾¾æ¨¡ç³Šç­‰é—®é¢˜ã€‚\n"
        "ä½ éœ€è¦æ ¹æ®å¤šè½®å†å²å¯¹è¯ï¼Œä¸°å¯Œæ¶¦è‰²ç”¨æˆ·çš„å½“å‰é—®é¢˜ï¼Œä½¿å…¶æˆä¸ºä¸€ä¸ªæ¸…æ™°ã€å®Œæ•´çš„ç‹¬ç«‹é—®é¢˜ã€‚\n\n"
        "ä¸‹é¢æ˜¯è¦æ±‚ï¼š\n"
        "- å‡†ç¡®è§£æç”¨æˆ·çœŸå®æ„å›¾ï¼Œä½¿å¾—è¿™ä¸ªç‹¬ç«‹é—®é¢˜å°½é‡å®Œæ•´ï¼Œå°½å¯èƒ½åŒ…å«æ‰€æœ‰ä¿¡æ¯ï¼›\n"
        "- é—®é¢˜è¶Šå…·ä½“è¶Šå¥½ï¼Œç‰¹åˆ«æ˜¯è¦æ•æ‰åˆ°å…³é”®çš„æŒ‡ä»£ï¼Œåœºæ™¯ï¼Œç‰¹åˆ«é’ˆå¯¹çš„é—®é¢˜å’Œä¾‹å­ç­‰ç­‰ï¼›\n"
        "- ä¸å¯ä»¥æé€ ä¸å­˜åœ¨çš„ä¿¡æ¯ï¼ŒåŒæ—¶è¿‡æ»¤æ‰ä¸çœŸå®æ„å›¾æ— å…³çš„ä¿¡æ¯ï¼›\n"
        "- ä¸æ·»åŠ è§£é‡Šã€æ³¨é‡Šã€å¼•å¯¼è¯­ç­‰ï¼Œåªè¾“å‡ºæ¶¦è‰²åçš„é—®é¢˜å¥ã€‚\n\n"
        "ä¸‹é¢æ˜¯å†å²å¯¹è¯ï¼š\n"
    )

    for turn in dialogue:
        role = "ç”¨æˆ·" if turn.get("speaker") == "user" else "ç³»ç»Ÿ"
        content = turn.get("text", "").replace("\n", " ").strip()
        prompt += f"{role}ï¼š{content}\n"

    prompt += f"ç”¨æˆ·å½“å‰é—®é¢˜æ˜¯ï¼š{final_query.strip()}\nè¯·ä½ éµå¾ªè¦æ±‚æ¶¦è‰²ä¸ºä¸€ä¸ªæ¸…æ™°ã€å®Œæ•´çš„ç‹¬ç«‹é—®é¢˜ï¼š"

    payload = {
        "model": model,
        "messages": [{"role": "user", "content": prompt}],
        "max_tokens": max_new_tokens,
        "temperature": 0.4,
        "top_p": 0.8
    }
    try:
        start = time.time()
        response = requests.post(api_url, headers={"Content-Type": "application/json"}, json=payload, timeout=timeout)
        response.raise_for_status()
        result = response.json()["choices"][0]["message"]["content"].strip()
        duration = time.time() - start
        print(f"âœ… é‡å†™æˆåŠŸï¼ˆè€—æ—¶ {duration:.2f}sï¼‰")
        return result if result else fallback_rewrite
    except Exception as e:
        print(f"âš ï¸ vLLM é‡å†™å¤±è´¥: {e}")
        return fallback_rewrite


async def rewrite_query_vllm_async(dialogue, final_query, model="glm", max_new_tokens=128):
    """
    ä½¿ç”¨ vLLM å¼‚æ­¥æ¥å£é‡å†™ queryï¼Œå¸¦å…¨å±€ session å’Œå¹¶å‘æ§åˆ¶
    """
    fallback_rewrite = final_query
    prompt = (
        "ä½ æ˜¯ä¸€ä¸ªç”µå•†å¹³å°æ™ºèƒ½å®¢æœçš„å¯¹è¯æ¸…æ™°åŒ–åŠ©æ‰‹ã€‚\n"
        "ç”¨æˆ·æå‡ºçš„é—®é¢˜å¯èƒ½å­˜åœ¨å¤æ‚æŒ‡ä»£ã€ä¸Šä¸‹æ–‡ä¾èµ–æˆ–è¡¨è¾¾æ¨¡ç³Šç­‰é—®é¢˜ã€‚\n"
        "ä½ éœ€è¦æ ¹æ®å¤šè½®å†å²å¯¹è¯ï¼Œä¸°å¯Œæ¶¦è‰²ç”¨æˆ·çš„å½“å‰é—®é¢˜ï¼Œä½¿å…¶æˆä¸ºä¸€ä¸ªæ¸…æ™°ã€å®Œæ•´çš„ç‹¬ç«‹é—®é¢˜ã€‚\n\n"
        "ä¸‹é¢æ˜¯è¦æ±‚ï¼š\n"
        "- å‡†ç¡®è§£æç”¨æˆ·çœŸå®æ„å›¾ï¼Œä½¿å¾—è¿™ä¸ªç‹¬ç«‹é—®é¢˜å°½é‡å®Œæ•´ï¼Œå°½å¯èƒ½åŒ…å«æ‰€æœ‰ä¿¡æ¯ï¼›\n"
        "- é—®é¢˜è¶Šä¸°å¯Œè¶Šå¥½ï¼Œç‰¹åˆ«æ˜¯è¦æ•æ‰åˆ°å…³é”®çš„æŒ‡ä»£ï¼Œåœºæ™¯ï¼Œç‰¹åˆ«é’ˆå¯¹çš„é—®é¢˜å’Œä¾‹å­ç­‰ç­‰ï¼›\n"
        "- ä¸å¯ä»¥æé€ ä¸å­˜åœ¨çš„ä¿¡æ¯ï¼›\n"
        "- ä¸æ·»åŠ è§£é‡Šã€æ³¨é‡Šã€å¼•å¯¼è¯­ç­‰ï¼Œåªè¾“å‡ºæ¶¦è‰²åçš„é—®é¢˜å¥ã€‚\n\n"
        "ä¸‹é¢æ˜¯å†å²å¯¹è¯ï¼š\n"
    )
    for turn in dialogue:
        role = "ç”¨æˆ·" if turn.get("speaker") == "user" else "ç³»ç»Ÿ"
        content = turn.get("text", "").replace("\n", " ").strip()
        prompt += f"{role}ï¼š{content}\n"

    prompt += f"ç”¨æˆ·å½“å‰é—®é¢˜æ˜¯ï¼š{final_query.strip()}\nè¯·ä½ éµå¾ªè¦æ±‚æ¶¦è‰²ä¸ºä¸€ä¸ªæ¸…æ™°ã€å®Œæ•´çš„ç‹¬ç«‹é—®é¢˜ï¼š"

    payload = {
        "model": model,
        "messages": [{"role": "user", "content": prompt}],
        "max_tokens": max_new_tokens,
        "temperature": 0.4,
        "top_p": 0.8,
    }
    try:
        async with sem:
            session = await get_aiohttp_session()
            start = time.time()
            logger.debug(f"vLLM å¼‚æ­¥é‡å†™è¯·æ±‚: {final_query}")
            async with session.post(api_url, json=payload) as resp:
                resp.raise_for_status()
                result = await resp.json()
                rewritten = result["choices"][0]["message"]["content"].strip()
                logger.debug(f"{final_query} Query é‡å†™æˆåŠŸï¼Œè€—æ—¶ {time.time() - start:.2f}s, é‡å†™ç»“æœ: {rewritten }")
                return rewritten or fallback_rewrite
    except Exception as e:
        logger.error(f"âš ï¸ vLLM å¼‚æ­¥é‡å†™å¤±è´¥: {e}, è¿”å›åŸé—®é¢˜")
        return fallback_rewrite


async def generate_summary_vllm_async(text, page_url, model="glm", max_new_tokens=196):
    """
    ä½¿ç”¨ vLLM å¼‚æ­¥æ¥å£ç”Ÿæˆæ‘˜è¦ï¼Œå¸¦å…¨å±€ session å’Œå¹¶å‘æ§åˆ¶
    """
    category = infer_chunk_category(page_url)
    text = text.strip().replace("\x00", "")

    prompt = (
        f"ä½ æ­£åœ¨å¤„ç†ä¸€ç¯‡ç”µå•†å¹³å°çš„çŸ¥è¯†å†…å®¹ï¼Œå±äºâ€œ{category}â€ç±»ã€‚\n"
        f"è¯·ä½ æ ¹æ®ä¸‹æ–¹å†…å®¹æç‚¼å…¶ä¸»è¦ä¿¡æ¯ï¼Œè¦æ±‚å¦‚ä¸‹ï¼š\n"
        f"1. æ¦‚æ‹¬è¦ç‚¹ï¼Œä¸è¦é‡å¤åŸæ–‡åŸå¥ï¼›\n"
        f"2. æ€»é•¿åº¦ä¸è¶…è¿‡{max_new_tokens}å­—ï¼Œä½¿ç”¨ç®€ä½“ä¸­æ–‡ï¼›\n"
        f"3. è¾“å‡ºæ ¼å¼ä¸ºå®Œæ•´ä¸€å¥è¯ã€‚\n"
        f"ğŸ“‚ æ¥æºè·¯å¾„ï¼š{page_url}\n"
        f"ğŸ“„ å†…å®¹ï¼š\n{text}"
    )

    payload = {
        "model": model,
        "messages": [{"role": "user", "content": prompt}],
        "max_tokens": max_new_tokens,
        "temperature": 0.4,
        "top_p": 0.8,
    }

    try:
        async with sem:
            session = await get_aiohttp_session()
            start = time.time()
            logger.debug(f" vLLM å¼‚æ­¥æ‘˜è¦è¯·æ±‚: {page_url + ' ' + text[:64]}")
            async with session.post(api_url, json=payload) as resp:
                resp.raise_for_status()
                result = await resp.json()
                summary = result["choices"][0]["message"]["content"].strip()
                duration = time.time() - start
                logger.debug(f"âœ… vLLM å¼‚æ­¥æ‘˜è¦æˆåŠŸ (è€—æ—¶ {duration:.2f}s), {page_url + ' ' + text[:64]}, æ‘˜è¦å†…å®¹: {summary[:64]}...")
                return summary
    except Exception as e:
        logger.error(f"âš ï¸ vLLM å¼‚æ­¥æ‘˜è¦å¤±è´¥: {e}ï¼Œè¿”å›æˆªæ–­æ–‡æœ¬")
        return text[:max_new_tokens]
