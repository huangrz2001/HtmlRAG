INFO 06-19 01:54:10 [__init__.py:244] Automatically detected platform cuda.
INFO 06-19 01:54:12 [api_server.py:1287] vLLM API server version 0.9.1
INFO 06-19 01:54:13 [cli_args.py:309] non-default args: {'host': '0.0.0.0', 'port': 8011, 'model': '/home/algo/DeepSeek-Chat-7B', 'tokenizer': '/home/algo/DeepSeek-Chat-7B', 'trust_remote_code': True, 'dtype': 'float16', 'max_model_len': 4096, 'served_model_name': ['glm'], 'tensor_parallel_size': 2, 'gpu_memory_utilization': 0.7, 'enable_prefix_caching': True, 'max_num_batched_tokens': 8192, 'max_num_seqs': 4}
INFO 06-19 01:54:19 [config.py:823] This model supports multiple tasks: {'generate', 'score', 'reward', 'classify', 'embed'}. Defaulting to 'generate'.
WARNING 06-19 01:54:19 [config.py:3271] Casting torch.bfloat16 to torch.float16.
INFO 06-19 01:54:19 [config.py:1946] Defaulting to use mp for distributed inference
INFO 06-19 01:54:19 [config.py:2195] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 06-19 01:54:20 [env_override.py:17] NCCL_CUMEM_ENABLE is set to 0, skipping override. This may increase memory overhead with cudagraph+allreduce: https://github.com/NVIDIA/nccl/issues/1234
INFO 06-19 01:54:22 [__init__.py:244] Automatically detected platform cuda.
INFO 06-19 01:54:25 [core.py:455] Waiting for init message from front-end.
INFO 06-19 01:54:25 [core.py:70] Initializing a V1 LLM engine (v0.9.1) with config: model='/home/algo/DeepSeek-Chat-7B', speculative_config=None, tokenizer='/home/algo/DeepSeek-Chat-7B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=glm, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["none"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
WARNING 06-19 01:54:25 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 8 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 06-19 01:54:25 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1], buffer_handle=(2, 16777216, 10, 'psm_728942a2'), local_subscribe_addr='ipc:///tmp/8142e9bf-14fd-4058-9a60-b7a9104bf11a', remote_subscribe_addr=None, remote_addr_ipv6=False)
WARNING 06-19 01:54:26 [env_override.py:17] NCCL_CUMEM_ENABLE is set to 0, skipping override. This may increase memory overhead with cudagraph+allreduce: https://github.com/NVIDIA/nccl/issues/1234
WARNING 06-19 01:54:26 [env_override.py:17] NCCL_CUMEM_ENABLE is set to 0, skipping override. This may increase memory overhead with cudagraph+allreduce: https://github.com/NVIDIA/nccl/issues/1234
INFO 06-19 01:54:27 [__init__.py:244] Automatically detected platform cuda.
INFO 06-19 01:54:27 [__init__.py:244] Automatically detected platform cuda.
WARNING 06-19 01:54:30 [utils.py:2737] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7fa810c601c0>
WARNING 06-19 01:54:30 [utils.py:2737] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f18ed849880>
[1;36m(VllmWorker rank=0 pid=2848796)[0;0m INFO 06-19 01:54:30 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_1dfd5630'), local_subscribe_addr='ipc:///tmp/368f3391-49cc-48a1-97d6-b6a94d2c44e5', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(VllmWorker rank=1 pid=2848797)[0;0m INFO 06-19 01:54:30 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_28ef17b1'), local_subscribe_addr='ipc:///tmp/609e83eb-6604-4e35-9c68-b96af02918df', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(VllmWorker rank=1 pid=2848797)[0;0m INFO 06-19 01:54:31 [utils.py:1126] Found nccl from library libnccl.so.2
[1;36m(VllmWorker rank=0 pid=2848796)[0;0m INFO 06-19 01:54:31 [utils.py:1126] Found nccl from library libnccl.so.2
[1;36m(VllmWorker rank=1 pid=2848797)[0;0m INFO 06-19 01:54:31 [pynccl.py:70] vLLM is using nccl==2.26.2
[1;36m(VllmWorker rank=0 pid=2848796)[0;0m INFO 06-19 01:54:31 [pynccl.py:70] vLLM is using nccl==2.26.2
[1;36m(VllmWorker rank=1 pid=2848797)[0;0m INFO 06-19 01:54:31 [custom_all_reduce_utils.py:246] reading GPU P2P access cache from /home/algo/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
[1;36m(VllmWorker rank=0 pid=2848796)[0;0m INFO 06-19 01:54:31 [custom_all_reduce_utils.py:246] reading GPU P2P access cache from /home/algo/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
[1;36m(VllmWorker rank=1 pid=2848797)[0;0m WARNING 06-19 01:54:31 [custom_all_reduce.py:147] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorker rank=0 pid=2848796)[0;0m WARNING 06-19 01:54:31 [custom_all_reduce.py:147] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorker rank=0 pid=2848796)[0;0m INFO 06-19 01:54:31 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_cf24b604'), local_subscribe_addr='ipc:///tmp/ba07a199-cfc9-4a2f-b907-7ded1b9e3a20', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(VllmWorker rank=0 pid=2848796)[0;0m INFO 06-19 01:54:32 [parallel_state.py:1065] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(VllmWorker rank=1 pid=2848797)[0;0m INFO 06-19 01:54:32 [parallel_state.py:1065] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1
[1;36m(VllmWorker rank=0 pid=2848796)[0;0m WARNING 06-19 01:54:32 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(VllmWorker rank=1 pid=2848797)[0;0m WARNING 06-19 01:54:32 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(VllmWorker rank=1 pid=2848797)[0;0m INFO 06-19 01:54:32 [gpu_model_runner.py:1595] Starting to load model /home/algo/DeepSeek-Chat-7B...
[1;36m(VllmWorker rank=0 pid=2848796)[0;0m INFO 06-19 01:54:32 [gpu_model_runner.py:1595] Starting to load model /home/algo/DeepSeek-Chat-7B...
[1;36m(VllmWorker rank=1 pid=2848797)[0;0m INFO 06-19 01:54:32 [gpu_model_runner.py:1600] Loading model from scratch...
[1;36m(VllmWorker rank=0 pid=2848796)[0;0m INFO 06-19 01:54:32 [gpu_model_runner.py:1600] Loading model from scratch...
[1;36m(VllmWorker rank=1 pid=2848797)[0;0m INFO 06-19 01:54:32 [cuda.py:252] Using Flash Attention backend on V1 engine.
[1;36m(VllmWorker rank=0 pid=2848796)[0;0m INFO 06-19 01:54:32 [cuda.py:252] Using Flash Attention backend on V1 engine.
[1;36m(VllmWorker rank=0 pid=2848796)[0;0m Loading pt checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[1;36m(VllmWorker rank=0 pid=2848796)[0;0m Loading pt checkpoint shards:  50% Completed | 1/2 [00:05<00:05,  5.27s/it]
[1;36m(VllmWorker rank=1 pid=2848797)[0;0m INFO 06-19 01:54:39 [default_loader.py:272] Loading weights took 6.92 seconds
[1;36m(VllmWorker rank=0 pid=2848796)[0;0m Loading pt checkpoint shards: 100% Completed | 2/2 [00:06<00:00,  3.13s/it]
[1;36m(VllmWorker rank=0 pid=2848796)[0;0m Loading pt checkpoint shards: 100% Completed | 2/2 [00:06<00:00,  3.45s/it]
[1;36m(VllmWorker rank=0 pid=2848796)[0;0m 
[1;36m(VllmWorker rank=0 pid=2848796)[0;0m INFO 06-19 01:54:39 [default_loader.py:272] Loading weights took 6.91 seconds
[1;36m(VllmWorker rank=1 pid=2848797)[0;0m INFO 06-19 01:54:39 [gpu_model_runner.py:1624] Model loading took 6.4664 GiB and 7.047468 seconds
[1;36m(VllmWorker rank=0 pid=2848796)[0;0m INFO 06-19 01:54:39 [gpu_model_runner.py:1624] Model loading took 6.4664 GiB and 7.049866 seconds
[1;36m(VllmWorker rank=0 pid=2848796)[0;0m INFO 06-19 01:54:44 [backends.py:462] Using cache directory: /home/algo/.cache/vllm/torch_compile_cache/217334e5bc/rank_0_0 for vLLM's torch.compile
[1;36m(VllmWorker rank=0 pid=2848796)[0;0m INFO 06-19 01:54:44 [backends.py:472] Dynamo bytecode transform time: 4.15 s
[1;36m(VllmWorker rank=1 pid=2848797)[0;0m INFO 06-19 01:54:44 [backends.py:462] Using cache directory: /home/algo/.cache/vllm/torch_compile_cache/217334e5bc/rank_1_0 for vLLM's torch.compile
[1;36m(VllmWorker rank=1 pid=2848797)[0;0m INFO 06-19 01:54:44 [backends.py:472] Dynamo bytecode transform time: 4.18 s
[1;36m(VllmWorker rank=0 pid=2848796)[0;0m INFO 06-19 01:54:47 [backends.py:135] Directly load the compiled graph(s) for shape None from the cache, took 3.354 s
[1;36m(VllmWorker rank=1 pid=2848797)[0;0m INFO 06-19 01:54:47 [backends.py:135] Directly load the compiled graph(s) for shape None from the cache, took 3.390 s
[1;36m(VllmWorker rank=1 pid=2848797)[0;0m INFO 06-19 01:54:48 [monitor.py:34] torch.compile takes 4.18 s in total
[1;36m(VllmWorker rank=0 pid=2848796)[0;0m INFO 06-19 01:54:48 [monitor.py:34] torch.compile takes 4.15 s in total
[1;36m(VllmWorker rank=1 pid=2848797)[0;0m INFO 06-19 01:54:50 [gpu_worker.py:227] Available KV cache memory: 9.41 GiB
[1;36m(VllmWorker rank=0 pid=2848796)[0;0m INFO 06-19 01:54:50 [gpu_worker.py:227] Available KV cache memory: 9.41 GiB
INFO 06-19 01:54:50 [kv_cache_utils.py:715] GPU KV cache size: 41,104 tokens
INFO 06-19 01:54:50 [kv_cache_utils.py:719] Maximum concurrency for 4,096 tokens per request: 10.04x
INFO 06-19 01:54:50 [kv_cache_utils.py:715] GPU KV cache size: 41,120 tokens
INFO 06-19 01:54:50 [kv_cache_utils.py:719] Maximum concurrency for 4,096 tokens per request: 10.04x
[1;36m(VllmWorker rank=1 pid=2848797)[0;0m INFO 06-19 01:55:11 [gpu_model_runner.py:2048] Graph capturing finished in 21 secs, took 1.77 GiB
[1;36m(VllmWorker rank=0 pid=2848796)[0;0m INFO 06-19 01:55:11 [gpu_model_runner.py:2048] Graph capturing finished in 21 secs, took 1.77 GiB
INFO 06-19 01:55:11 [core.py:171] init engine (profile, create kv cache, warmup model) took 32.15 seconds
INFO 06-19 01:55:12 [loggers.py:137] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 2569
WARNING 06-19 01:55:12 [config.py:1363] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
INFO 06-19 01:55:12 [serving_chat.py:118] Using default chat sampling params from model: {'temperature': 0.7, 'top_p': 0.95}
INFO 06-19 01:55:12 [serving_completion.py:66] Using default completion sampling params from model: {'temperature': 0.7, 'top_p': 0.95}
INFO 06-19 01:55:12 [api_server.py:1349] Starting vLLM API server 0 on http://0.0.0.0:8011
INFO 06-19 01:55:12 [launcher.py:29] Available routes are:
INFO 06-19 01:55:12 [launcher.py:37] Route: /openapi.json, Methods: GET, HEAD
INFO 06-19 01:55:12 [launcher.py:37] Route: /docs, Methods: GET, HEAD
INFO 06-19 01:55:12 [launcher.py:37] Route: /docs/oauth2-redirect, Methods: GET, HEAD
INFO 06-19 01:55:12 [launcher.py:37] Route: /redoc, Methods: GET, HEAD
INFO 06-19 01:55:12 [launcher.py:37] Route: /health, Methods: GET
INFO 06-19 01:55:12 [launcher.py:37] Route: /load, Methods: GET
INFO 06-19 01:55:12 [launcher.py:37] Route: /ping, Methods: POST
INFO 06-19 01:55:12 [launcher.py:37] Route: /ping, Methods: GET
INFO 06-19 01:55:12 [launcher.py:37] Route: /tokenize, Methods: POST
INFO 06-19 01:55:12 [launcher.py:37] Route: /detokenize, Methods: POST
INFO 06-19 01:55:12 [launcher.py:37] Route: /v1/models, Methods: GET
INFO 06-19 01:55:12 [launcher.py:37] Route: /version, Methods: GET
INFO 06-19 01:55:12 [launcher.py:37] Route: /v1/chat/completions, Methods: POST
INFO 06-19 01:55:12 [launcher.py:37] Route: /v1/completions, Methods: POST
INFO 06-19 01:55:12 [launcher.py:37] Route: /v1/embeddings, Methods: POST
INFO 06-19 01:55:12 [launcher.py:37] Route: /pooling, Methods: POST
INFO 06-19 01:55:12 [launcher.py:37] Route: /classify, Methods: POST
INFO 06-19 01:55:12 [launcher.py:37] Route: /score, Methods: POST
INFO 06-19 01:55:12 [launcher.py:37] Route: /v1/score, Methods: POST
INFO 06-19 01:55:12 [launcher.py:37] Route: /v1/audio/transcriptions, Methods: POST
INFO 06-19 01:55:12 [launcher.py:37] Route: /rerank, Methods: POST
INFO 06-19 01:55:12 [launcher.py:37] Route: /v1/rerank, Methods: POST
INFO 06-19 01:55:12 [launcher.py:37] Route: /v2/rerank, Methods: POST
INFO 06-19 01:55:12 [launcher.py:37] Route: /invocations, Methods: POST
INFO 06-19 01:55:12 [launcher.py:37] Route: /metrics, Methods: GET
INFO:     Started server process [2848453]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO 06-19 04:11:37 [chat_utils.py:420] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
INFO 06-19 04:11:37 [logger.py:43] Received request chatcmpl-e357e2b2fa654739bdb9e9515451699d: prompt: '<｜begin▁of▁sentence｜>你是一个专业的问题重写模块，专门用于多轮对话场景下的指代补全与语义还原任务。\n请严格按照以下规则执行：\n\n【任务目标】\n1. 你的目标是准确解析用户真实意图，从历史中发掘指代和意图，使得这个独立问题尽量完整，尽可能包含所有信息关键词,特别是要补充关键的动机，指代和场景（润色后的问题至少涉及：用户面对的前置情况，什么限制，什么疑问等）。\n2. 仅当历史信息能够提供明确上下文时才进行补全，否则保持当前问题不变。\n3. 对于明显不需要补全的问题如：吹水："呵呵"，命令："关机"，无关内容："人生的意义"等，返回原句\n4. 不进行任何无关发挥、扩写、润色、修辞性描述、解释、总结、感情色彩。\n5. 不编造任何不存在的假设背景或新信息。\n6. 输出格式必须严格遵循：仅输出最终重写结果文本，不包含任何前缀、提示词、说明性文字或换行符。重写的疑问句以："我想知道"开头，非疑问句你自行适配\n7. 当无需重写时，直接输出原问题。\n\n【重写示例】\n历史对话：\n用户：小红鞋什么时候有货？\n系统：请问您指的是哪款小红鞋？\n用户：就是上次缺货那款\n当前问题：就是上次缺货那款\n重写结果：我想知道上次缺货的那款小红鞋什么时候有货？\n\n历史对话：\n用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：那我们还有其他计划，能投放吗？\n当前问题：那我们还有其他计划，能投放吗？\n重写结果：我想知道当账号处于限流状态时，是否可以继续开启新的广告投放计划？\n\n历史对话：\n用户：你是谁？\n当前问题：你是谁？\n重写结果：你是谁？\n\n历史对话：\n用户：开心\n当前问题：开心\n重写结果：开心\n\n历史对话：\n用户：我要买它\n系统：请问您指的是哪款商品？\n用户：之前推荐的那双\n当前问题：之前推荐的那双\n重写结果：我想要买你之前推荐的那双鞋。\n\n注意：严格按照以上风格工作，禁止任何额外输出。\n\nUser: 用户：125487952364\n系统：亲，您是想查询账户余额嘛？麻烦提供下业务号哈～\n用户：五张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/589a675592ab4d878cdd56b8f9dcf43a..png\n用户：五张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/a2dbe01a4d6848848935dca6e5f91911..png\n用户：五张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/2d54d547c2384735a90189966f0fcd8f..png\n用户：五张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/ffdf42977895448d8ffaa78d5cbd271b..png\n用户：五张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/c3f19ce42ba9490185fd3a622c51a918..png\n用户：你好帮我査一下下单了招募任务 达人交付完成后 怎么取消该计划\n\n当前问题：你好帮我査一下下单了招募任务 达人交付完成后 怎么取消该计划\n请你根据任务规则输出重写结果。\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=0.5, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 04:11:37 [async_llm.py:271] Added request chatcmpl-e357e2b2fa654739bdb9e9515451699d.
[1;36m(VllmWorker rank=0 pid=2848796)[0;0m ERROR 06-19 04:11:37 [multiproc_executor.py:527] WorkerProc hit an exception.
[1;36m(VllmWorker rank=0 pid=2848796)[0;0m ERROR 06-19 04:11:37 [multiproc_executor.py:527] Traceback (most recent call last):
[1;36m(VllmWorker rank=0 pid=2848796)[0;0m ERROR 06-19 04:11:37 [multiproc_executor.py:527]   File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/v1/executor/multiproc_executor.py", line 522, in worker_busy_loop
[1;36m(VllmWorker rank=0 pid=2848796)[0;0m ERROR 06-19 04:11:37 [multiproc_executor.py:527]     output = func(*args, **kwargs)
[1;36m(VllmWorker rank=0 pid=2848796)[0;0m ERROR 06-19 04:11:37 [multiproc_executor.py:527]   File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[1;36m(VllmWorker rank=0 pid=2848796)[0;0m ERROR 06-19 04:11:37 [multiproc_executor.py:527]     return func(*args, **kwargs)
[1;36m(VllmWorker rank=0 pid=2848796)[0;0m ERROR 06-19 04:11:37 [multiproc_executor.py:527]   File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/v1/worker/gpu_worker.py", line 293, in execute_model
[1;36m(VllmWorker rank=0 pid=2848796)[0;0m ERROR 06-19 04:11:37 [multiproc_executor.py:527]     output = self.model_runner.execute_model(scheduler_output,
[1;36m(VllmWorker rank=0 pid=2848796)[0;0m ERROR 06-19 04:11:37 [multiproc_executor.py:527]   File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[1;36m(VllmWorker rank=0 pid=2848796)[0;0m ERROR 06-19 04:11:37 [multiproc_executor.py:527]     return func(*args, **kwargs)
[1;36m(VllmWorker rank=0 pid=2848796)[0;0m ERROR 06-19 04:11:37 [multiproc_executor.py:527]   File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1177, in execute_model
[1;36m(VllmWorker rank=0 pid=2848796)[0;0m ERROR 06-19 04:11:37 [multiproc_executor.py:527]     self._update_states(scheduler_output)
[1;36m(VllmWorker rank=0 pid=2848796)[0;0m ERROR 06-19 04:11:37 [multiproc_executor.py:527]   File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/v1/worker/gpu_model_runner.py", line 465, in _update_states
[1;36m(VllmWorker rank=0 pid=2848796)[0;0m ERROR 06-19 04:11:37 [multiproc_executor.py:527]     for block_ids, new_block_ids in zip(  # type: ignore[call-overload]
[1;36m(VllmWorker rank=0 pid=2848796)[0;0m ERROR 06-19 04:11:37 [multiproc_executor.py:527] TypeError: zip() takes no keyword arguments
[1;36m(VllmWorker rank=1 pid=2848797)[0;0m ERROR 06-19 04:11:37 [multiproc_executor.py:527] WorkerProc hit an exception.
[1;36m(VllmWorker rank=1 pid=2848797)[0;0m ERROR 06-19 04:11:37 [multiproc_executor.py:527] Traceback (most recent call last):
[1;36m(VllmWorker rank=1 pid=2848797)[0;0m ERROR 06-19 04:11:37 [multiproc_executor.py:527]   File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/v1/executor/multiproc_executor.py", line 522, in worker_busy_loop
[1;36m(VllmWorker rank=1 pid=2848797)[0;0m ERROR 06-19 04:11:37 [multiproc_executor.py:527]     output = func(*args, **kwargs)
[1;36m(VllmWorker rank=1 pid=2848797)[0;0m ERROR 06-19 04:11:37 [multiproc_executor.py:527]   File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[1;36m(VllmWorker rank=1 pid=2848797)[0;0m ERROR 06-19 04:11:37 [multiproc_executor.py:527]     return func(*args, **kwargs)
[1;36m(VllmWorker rank=1 pid=2848797)[0;0m ERROR 06-19 04:11:37 [multiproc_executor.py:527]   File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/v1/worker/gpu_worker.py", line 293, in execute_model
[1;36m(VllmWorker rank=1 pid=2848797)[0;0m ERROR 06-19 04:11:37 [multiproc_executor.py:527]     output = self.model_runner.execute_model(scheduler_output,
[1;36m(VllmWorker rank=1 pid=2848797)[0;0m ERROR 06-19 04:11:37 [multiproc_executor.py:527]   File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[1;36m(VllmWorker rank=1 pid=2848797)[0;0m ERROR 06-19 04:11:37 [multiproc_executor.py:527]     return func(*args, **kwargs)
[1;36m(VllmWorker rank=1 pid=2848797)[0;0m ERROR 06-19 04:11:37 [multiproc_executor.py:527]   File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1177, in execute_model
[1;36m(VllmWorker rank=1 pid=2848797)[0;0m ERROR 06-19 04:11:37 [multiproc_executor.py:527]     self._update_states(scheduler_output)
[1;36m(VllmWorker rank=1 pid=2848797)[0;0m ERROR 06-19 04:11:37 [multiproc_executor.py:527]   File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/v1/worker/gpu_model_runner.py", line 465, in _update_states
[1;36m(VllmWorker rank=1 pid=2848797)[0;0m ERROR 06-19 04:11:37 [multiproc_executor.py:527]     for block_ids, new_block_ids in zip(  # type: ignore[call-overload]
[1;36m(VllmWorker rank=1 pid=2848797)[0;0m ERROR 06-19 04:11:37 [multiproc_executor.py:527] TypeError: zip() takes no keyword arguments
ERROR 06-19 04:11:37 [dump_input.py:69] Dumping input data
ERROR 06-19 04:11:37 [dump_input.py:71] V1 LLM engine (v0.9.1) with config: model='/home/algo/DeepSeek-Chat-7B', speculative_config=None, tokenizer='/home/algo/DeepSeek-Chat-7B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=glm, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["none"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}, 
ERROR 06-19 04:11:37 [dump_input.py:79] Dumping scheduler output for model execution:
ERROR 06-19 04:11:37 [dump_input.py:80] SchedulerOutput(scheduled_new_reqs=[], scheduled_cached_reqs=[CachedRequestData(req_id='chatcmpl-e357e2b2fa654739bdb9e9515451699d', resumed_from_preemption=false, new_token_ids=[12775], new_block_ids=[[]], num_computed_tokens=962)], num_scheduled_tokens={chatcmpl-e357e2b2fa654739bdb9e9515451699d: 1}, total_num_scheduled_tokens=1, scheduled_spec_decode_tokens={}, scheduled_encoder_inputs={}, num_common_prefix_blocks=[61], finished_req_ids=[], free_encoder_input_ids=[], structured_output_request_ids={}, grammar_bitmask=null, kv_connector_metadata=null)
ERROR 06-19 04:11:37 [dump_input.py:82] SchedulerStats(num_running_reqs=1, num_waiting_reqs=0, gpu_cache_usage=0.024133904242896076, prefix_cache_stats=PrefixCacheStats(reset=False, requests=0, queries=0, hits=0), spec_decoding_stats=None)
ERROR 06-19 04:11:37 [core.py:517] EngineCore encountered a fatal error.
ERROR 06-19 04:11:37 [core.py:517] Traceback (most recent call last):
ERROR 06-19 04:11:37 [core.py:517]   File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/v1/engine/core.py", line 508, in run_engine_core
ERROR 06-19 04:11:37 [core.py:517]     engine_core.run_busy_loop()
ERROR 06-19 04:11:37 [core.py:517]   File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/v1/engine/core.py", line 535, in run_busy_loop
ERROR 06-19 04:11:37 [core.py:517]     self._process_engine_step()
ERROR 06-19 04:11:37 [core.py:517]   File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/v1/engine/core.py", line 560, in _process_engine_step
ERROR 06-19 04:11:37 [core.py:517]     outputs, model_executed = self.step_fn()
ERROR 06-19 04:11:37 [core.py:517]   File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/v1/engine/core.py", line 231, in step
ERROR 06-19 04:11:37 [core.py:517]     model_output = self.execute_model(scheduler_output)
ERROR 06-19 04:11:37 [core.py:517]   File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/v1/engine/core.py", line 217, in execute_model
ERROR 06-19 04:11:37 [core.py:517]     raise err
ERROR 06-19 04:11:37 [core.py:517]   File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/v1/engine/core.py", line 211, in execute_model
ERROR 06-19 04:11:37 [core.py:517]     return self.model_executor.execute_model(scheduler_output)
ERROR 06-19 04:11:37 [core.py:517]   File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/v1/executor/multiproc_executor.py", line 163, in execute_model
ERROR 06-19 04:11:37 [core.py:517]     (output, ) = self.collective_rpc("execute_model",
ERROR 06-19 04:11:37 [core.py:517]   File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/v1/executor/multiproc_executor.py", line 220, in collective_rpc
ERROR 06-19 04:11:37 [core.py:517]     result = get_response(w, dequeue_timeout)
ERROR 06-19 04:11:37 [core.py:517]   File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/v1/executor/multiproc_executor.py", line 207, in get_response
ERROR 06-19 04:11:37 [core.py:517]     raise RuntimeError(
ERROR 06-19 04:11:37 [core.py:517] RuntimeError: Worker failed with error 'zip() takes no keyword arguments', please check the stack trace above for the root cause
ERROR 06-19 04:11:37 [async_llm.py:420] AsyncLLM output_handler failed.
ERROR 06-19 04:11:37 [async_llm.py:420] Traceback (most recent call last):
ERROR 06-19 04:11:37 [async_llm.py:420]   File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/v1/engine/async_llm.py", line 379, in output_handler
ERROR 06-19 04:11:37 [async_llm.py:420]     outputs = await engine_core.get_output_async()
ERROR 06-19 04:11:37 [async_llm.py:420]   File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/v1/engine/core_client.py", line 790, in get_output_async
ERROR 06-19 04:11:37 [async_llm.py:420]     raise self._format_exception(outputs) from None
ERROR 06-19 04:11:37 [async_llm.py:420] vllm.v1.engine.exceptions.EngineDeadError: EngineCore encountered an issue. See stack trace (above) for the root cause.
INFO 06-19 04:11:37 [async_llm.py:346] Request chatcmpl-e357e2b2fa654739bdb9e9515451699d failed (engine dead).
INFO:     127.0.0.1:36896 - "POST /v1/chat/completions HTTP/1.1" 500 Internal Server Error
INFO:     Shutting down
INFO:     Waiting for application shutdown.
INFO:     Application shutdown complete.
INFO:     Finished server process [2848453]
/usr/bin/python3: Error while finding module specification for 'vllm.entrypoints.openai.api_server' (ModuleNotFoundError: No module named 'vllm')
INFO 06-19 04:18:21 [__init__.py:244] Automatically detected platform cuda.
INFO 06-19 04:18:24 [api_server.py:1287] vLLM API server version 0.9.1
INFO 06-19 04:18:24 [cli_args.py:309] non-default args: {'host': '0.0.0.0', 'port': 8011, 'model': '/home/algo/DeepSeek-Chat-7B', 'tokenizer': '/home/algo/DeepSeek-Chat-7B', 'trust_remote_code': True, 'dtype': 'float16', 'max_model_len': 4096, 'served_model_name': ['glm'], 'tensor_parallel_size': 2, 'gpu_memory_utilization': 0.7, 'enable_prefix_caching': True, 'max_num_batched_tokens': 8192, 'max_num_seqs': 4}
INFO 06-19 04:18:30 [config.py:823] This model supports multiple tasks: {'generate', 'embed', 'score', 'reward', 'classify'}. Defaulting to 'generate'.
WARNING 06-19 04:18:30 [config.py:3271] Casting torch.bfloat16 to torch.float16.
INFO 06-19 04:18:30 [config.py:1946] Defaulting to use mp for distributed inference
INFO 06-19 04:18:30 [config.py:2195] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 06-19 04:18:32 [env_override.py:17] NCCL_CUMEM_ENABLE is set to 0, skipping override. This may increase memory overhead with cudagraph+allreduce: https://github.com/NVIDIA/nccl/issues/1234
INFO 06-19 04:18:34 [__init__.py:244] Automatically detected platform cuda.
INFO 06-19 04:18:36 [core.py:455] Waiting for init message from front-end.
INFO 06-19 04:18:36 [core.py:70] Initializing a V1 LLM engine (v0.9.1) with config: model='/home/algo/DeepSeek-Chat-7B', speculative_config=None, tokenizer='/home/algo/DeepSeek-Chat-7B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=glm, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["none"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
WARNING 06-19 04:18:36 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 8 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 06-19 04:18:36 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1], buffer_handle=(2, 16777216, 10, 'psm_786efd98'), local_subscribe_addr='ipc:///tmp/3c5eec16-b398-4bc7-9c46-3d924015dafa', remote_subscribe_addr=None, remote_addr_ipv6=False)
WARNING 06-19 04:18:37 [env_override.py:17] NCCL_CUMEM_ENABLE is set to 0, skipping override. This may increase memory overhead with cudagraph+allreduce: https://github.com/NVIDIA/nccl/issues/1234
WARNING 06-19 04:18:37 [env_override.py:17] NCCL_CUMEM_ENABLE is set to 0, skipping override. This may increase memory overhead with cudagraph+allreduce: https://github.com/NVIDIA/nccl/issues/1234
INFO 06-19 04:18:39 [__init__.py:244] Automatically detected platform cuda.
INFO 06-19 04:18:39 [__init__.py:244] Automatically detected platform cuda.
WARNING 06-19 04:18:42 [utils.py:2737] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7fd59dc971f0>
[1;36m(VllmWorker rank=1 pid=2910083)[0;0m INFO 06-19 04:18:42 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_fad7f32f'), local_subscribe_addr='ipc:///tmp/eb4f804b-5c83-45c6-88b9-469e74cb1b8c', remote_subscribe_addr=None, remote_addr_ipv6=False)
WARNING 06-19 04:18:42 [utils.py:2737] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7eff470f9280>
[1;36m(VllmWorker rank=0 pid=2910082)[0;0m INFO 06-19 04:18:42 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_029168e3'), local_subscribe_addr='ipc:///tmp/380552cd-b5f5-4c0f-bf9d-c56732709de5', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(VllmWorker rank=1 pid=2910083)[0;0m INFO 06-19 04:18:42 [utils.py:1126] Found nccl from library libnccl.so.2
[1;36m(VllmWorker rank=0 pid=2910082)[0;0m INFO 06-19 04:18:42 [utils.py:1126] Found nccl from library libnccl.so.2
[1;36m(VllmWorker rank=1 pid=2910083)[0;0m INFO 06-19 04:18:42 [pynccl.py:70] vLLM is using nccl==2.26.2
[1;36m(VllmWorker rank=0 pid=2910082)[0;0m INFO 06-19 04:18:42 [pynccl.py:70] vLLM is using nccl==2.26.2
[1;36m(VllmWorker rank=0 pid=2910082)[0;0m INFO 06-19 04:18:42 [custom_all_reduce_utils.py:246] reading GPU P2P access cache from /home/algo/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
[1;36m(VllmWorker rank=1 pid=2910083)[0;0m INFO 06-19 04:18:42 [custom_all_reduce_utils.py:246] reading GPU P2P access cache from /home/algo/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
[1;36m(VllmWorker rank=1 pid=2910083)[0;0m WARNING 06-19 04:18:42 [custom_all_reduce.py:147] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorker rank=0 pid=2910082)[0;0m WARNING 06-19 04:18:42 [custom_all_reduce.py:147] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorker rank=0 pid=2910082)[0;0m INFO 06-19 04:18:42 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_8429107a'), local_subscribe_addr='ipc:///tmp/bbab92e9-d132-40db-9fb2-d4af0aa1233a', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(VllmWorker rank=0 pid=2910082)[0;0m INFO 06-19 04:18:42 [parallel_state.py:1065] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(VllmWorker rank=1 pid=2910083)[0;0m INFO 06-19 04:18:42 [parallel_state.py:1065] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1
[1;36m(VllmWorker rank=0 pid=2910082)[0;0m WARNING 06-19 04:18:42 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(VllmWorker rank=1 pid=2910083)[0;0m WARNING 06-19 04:18:42 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(VllmWorker rank=1 pid=2910083)[0;0m INFO 06-19 04:18:42 [gpu_model_runner.py:1595] Starting to load model /home/algo/DeepSeek-Chat-7B...
[1;36m(VllmWorker rank=0 pid=2910082)[0;0m INFO 06-19 04:18:42 [gpu_model_runner.py:1595] Starting to load model /home/algo/DeepSeek-Chat-7B...
[1;36m(VllmWorker rank=1 pid=2910083)[0;0m INFO 06-19 04:18:43 [gpu_model_runner.py:1600] Loading model from scratch...
[1;36m(VllmWorker rank=0 pid=2910082)[0;0m INFO 06-19 04:18:43 [gpu_model_runner.py:1600] Loading model from scratch...
[1;36m(VllmWorker rank=1 pid=2910083)[0;0m INFO 06-19 04:18:43 [cuda.py:252] Using Flash Attention backend on V1 engine.
[1;36m(VllmWorker rank=0 pid=2910082)[0;0m INFO 06-19 04:18:43 [cuda.py:252] Using Flash Attention backend on V1 engine.
[1;36m(VllmWorker rank=0 pid=2910082)[0;0m Loading pt checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[1;36m(VllmWorker rank=0 pid=2910082)[0;0m Loading pt checkpoint shards:  50% Completed | 1/2 [00:05<00:05,  5.11s/it]
[1;36m(VllmWorker rank=1 pid=2910083)[0;0m INFO 06-19 04:18:49 [default_loader.py:272] Loading weights took 6.76 seconds
[1;36m(VllmWorker rank=0 pid=2910082)[0;0m Loading pt checkpoint shards: 100% Completed | 2/2 [00:06<00:00,  3.07s/it]
[1;36m(VllmWorker rank=0 pid=2910082)[0;0m Loading pt checkpoint shards: 100% Completed | 2/2 [00:06<00:00,  3.38s/it]
[1;36m(VllmWorker rank=0 pid=2910082)[0;0m 
[1;36m(VllmWorker rank=0 pid=2910082)[0;0m INFO 06-19 04:18:49 [default_loader.py:272] Loading weights took 6.76 seconds
[1;36m(VllmWorker rank=1 pid=2910083)[0;0m INFO 06-19 04:18:50 [gpu_model_runner.py:1624] Model loading took 6.4664 GiB and 6.885646 seconds
[1;36m(VllmWorker rank=0 pid=2910082)[0;0m INFO 06-19 04:18:50 [gpu_model_runner.py:1624] Model loading took 6.4664 GiB and 6.903775 seconds
[1;36m(VllmWorker rank=0 pid=2910082)[0;0m INFO 06-19 04:18:54 [backends.py:462] Using cache directory: /home/algo/.cache/vllm/torch_compile_cache/217334e5bc/rank_0_0 for vLLM's torch.compile
[1;36m(VllmWorker rank=0 pid=2910082)[0;0m INFO 06-19 04:18:54 [backends.py:472] Dynamo bytecode transform time: 4.13 s
[1;36m(VllmWorker rank=1 pid=2910083)[0;0m INFO 06-19 04:18:54 [backends.py:462] Using cache directory: /home/algo/.cache/vllm/torch_compile_cache/217334e5bc/rank_1_0 for vLLM's torch.compile
[1;36m(VllmWorker rank=1 pid=2910083)[0;0m INFO 06-19 04:18:54 [backends.py:472] Dynamo bytecode transform time: 4.18 s
[1;36m(VllmWorker rank=1 pid=2910083)[0;0m INFO 06-19 04:18:58 [backends.py:135] Directly load the compiled graph(s) for shape None from the cache, took 3.334 s
[1;36m(VllmWorker rank=0 pid=2910082)[0;0m INFO 06-19 04:18:58 [backends.py:135] Directly load the compiled graph(s) for shape None from the cache, took 3.404 s
[1;36m(VllmWorker rank=1 pid=2910083)[0;0m INFO 06-19 04:18:59 [monitor.py:34] torch.compile takes 4.18 s in total
[1;36m(VllmWorker rank=0 pid=2910082)[0;0m INFO 06-19 04:18:59 [monitor.py:34] torch.compile takes 4.13 s in total
[1;36m(VllmWorker rank=1 pid=2910083)[0;0m INFO 06-19 04:19:00 [gpu_worker.py:227] Available KV cache memory: 9.41 GiB
[1;36m(VllmWorker rank=0 pid=2910082)[0;0m INFO 06-19 04:19:00 [gpu_worker.py:227] Available KV cache memory: 9.41 GiB
INFO 06-19 04:19:01 [kv_cache_utils.py:715] GPU KV cache size: 41,104 tokens
INFO 06-19 04:19:01 [kv_cache_utils.py:719] Maximum concurrency for 4,096 tokens per request: 10.04x
INFO 06-19 04:19:01 [kv_cache_utils.py:715] GPU KV cache size: 41,120 tokens
INFO 06-19 04:19:01 [kv_cache_utils.py:719] Maximum concurrency for 4,096 tokens per request: 10.04x
[1;36m(VllmWorker rank=0 pid=2910082)[0;0m INFO 06-19 04:19:22 [gpu_model_runner.py:2048] Graph capturing finished in 21 secs, took 1.77 GiB
[1;36m(VllmWorker rank=1 pid=2910083)[0;0m INFO 06-19 04:19:22 [gpu_model_runner.py:2048] Graph capturing finished in 21 secs, took 1.77 GiB
INFO 06-19 04:19:22 [core.py:171] init engine (profile, create kv cache, warmup model) took 31.81 seconds
INFO 06-19 04:19:22 [loggers.py:137] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 2569
WARNING 06-19 04:19:22 [config.py:1363] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
INFO 06-19 04:19:22 [serving_chat.py:118] Using default chat sampling params from model: {'temperature': 0.7, 'top_p': 0.95}
INFO 06-19 04:19:22 [serving_completion.py:66] Using default completion sampling params from model: {'temperature': 0.7, 'top_p': 0.95}
INFO 06-19 04:19:22 [api_server.py:1349] Starting vLLM API server 0 on http://0.0.0.0:8011
INFO 06-19 04:19:22 [launcher.py:29] Available routes are:
INFO 06-19 04:19:22 [launcher.py:37] Route: /openapi.json, Methods: HEAD, GET
INFO 06-19 04:19:22 [launcher.py:37] Route: /docs, Methods: HEAD, GET
INFO 06-19 04:19:22 [launcher.py:37] Route: /docs/oauth2-redirect, Methods: HEAD, GET
INFO 06-19 04:19:22 [launcher.py:37] Route: /redoc, Methods: HEAD, GET
INFO 06-19 04:19:22 [launcher.py:37] Route: /health, Methods: GET
INFO 06-19 04:19:22 [launcher.py:37] Route: /load, Methods: GET
INFO 06-19 04:19:22 [launcher.py:37] Route: /ping, Methods: POST
INFO 06-19 04:19:22 [launcher.py:37] Route: /ping, Methods: GET
INFO 06-19 04:19:22 [launcher.py:37] Route: /tokenize, Methods: POST
INFO 06-19 04:19:22 [launcher.py:37] Route: /detokenize, Methods: POST
INFO 06-19 04:19:22 [launcher.py:37] Route: /v1/models, Methods: GET
INFO 06-19 04:19:22 [launcher.py:37] Route: /version, Methods: GET
INFO 06-19 04:19:22 [launcher.py:37] Route: /v1/chat/completions, Methods: POST
INFO 06-19 04:19:22 [launcher.py:37] Route: /v1/completions, Methods: POST
INFO 06-19 04:19:22 [launcher.py:37] Route: /v1/embeddings, Methods: POST
INFO 06-19 04:19:22 [launcher.py:37] Route: /pooling, Methods: POST
INFO 06-19 04:19:22 [launcher.py:37] Route: /classify, Methods: POST
INFO 06-19 04:19:22 [launcher.py:37] Route: /score, Methods: POST
INFO 06-19 04:19:22 [launcher.py:37] Route: /v1/score, Methods: POST
INFO 06-19 04:19:22 [launcher.py:37] Route: /v1/audio/transcriptions, Methods: POST
INFO 06-19 04:19:22 [launcher.py:37] Route: /rerank, Methods: POST
INFO 06-19 04:19:22 [launcher.py:37] Route: /v1/rerank, Methods: POST
INFO 06-19 04:19:22 [launcher.py:37] Route: /v2/rerank, Methods: POST
INFO 06-19 04:19:22 [launcher.py:37] Route: /invocations, Methods: POST
INFO 06-19 04:19:22 [launcher.py:37] Route: /metrics, Methods: GET
INFO:     Started server process [2909761]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     192.168.7.58:52356 - "POST /v1/chat/completions HTTP/1.1" 400 Bad Request
WARNING 06-19 04:26:21 [protocol.py:58] The following fields were present in the request but ignored: {'prompt'}
ERROR 06-19 04:26:21 [serving_chat.py:136] Error with model object='error' message='The model `embed` does not exist.' type='NotFoundError' param=None code=404
INFO:     192.168.7.58:52532 - "POST /v1/chat/completions HTTP/1.1" 404 Not Found
WARNING 06-19 04:26:39 [protocol.py:58] The following fields were present in the request but ignored: {'prompt'}
ERROR 06-19 04:26:39 [serving_chat.py:136] Error with model object='error' message='The model `/home/algo/DeepSeek-Chat-7B` does not exist.' type='NotFoundError' param=None code=404
INFO:     192.168.7.58:52547 - "POST /v1/chat/completions HTTP/1.1" 404 Not Found
WARNING 06-19 04:26:48 [protocol.py:58] The following fields were present in the request but ignored: {'prompt'}
INFO 06-19 04:26:48 [chat_utils.py:420] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
ERROR 06-19 04:26:48 [chat_utils.py:1254] An error occurred in `transformers` while applying chat template
ERROR 06-19 04:26:48 [chat_utils.py:1254] Traceback (most recent call last):
ERROR 06-19 04:26:48 [chat_utils.py:1254]   File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/entrypoints/chat_utils.py", line 1240, in apply_hf_chat_template
ERROR 06-19 04:26:48 [chat_utils.py:1254]     return tokenizer.apply_chat_template(
ERROR 06-19 04:26:48 [chat_utils.py:1254]   File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 1635, in apply_chat_template
ERROR 06-19 04:26:48 [chat_utils.py:1254]     isinstance(conversation[0], (list, tuple)) or hasattr(conversation[0], "messages")
ERROR 06-19 04:26:48 [chat_utils.py:1254] IndexError: list index out of range
ERROR 06-19 04:26:48 [serving_chat.py:200] Error in preprocessing prompt inputs
ERROR 06-19 04:26:48 [serving_chat.py:200] Traceback (most recent call last):
ERROR 06-19 04:26:48 [serving_chat.py:200]   File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/entrypoints/chat_utils.py", line 1240, in apply_hf_chat_template
ERROR 06-19 04:26:48 [serving_chat.py:200]     return tokenizer.apply_chat_template(
ERROR 06-19 04:26:48 [serving_chat.py:200]   File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 1635, in apply_chat_template
ERROR 06-19 04:26:48 [serving_chat.py:200]     isinstance(conversation[0], (list, tuple)) or hasattr(conversation[0], "messages")
ERROR 06-19 04:26:48 [serving_chat.py:200] IndexError: list index out of range
ERROR 06-19 04:26:48 [serving_chat.py:200] 
ERROR 06-19 04:26:48 [serving_chat.py:200] The above exception was the direct cause of the following exception:
ERROR 06-19 04:26:48 [serving_chat.py:200] 
ERROR 06-19 04:26:48 [serving_chat.py:200] Traceback (most recent call last):
ERROR 06-19 04:26:48 [serving_chat.py:200]   File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/entrypoints/openai/serving_chat.py", line 183, in create_chat_completion
ERROR 06-19 04:26:48 [serving_chat.py:200]     ) = await self._preprocess_chat(
ERROR 06-19 04:26:48 [serving_chat.py:200]   File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/entrypoints/openai/serving_engine.py", line 811, in _preprocess_chat
ERROR 06-19 04:26:48 [serving_chat.py:200]     request_prompt = apply_hf_chat_template(
ERROR 06-19 04:26:48 [serving_chat.py:200]   File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/utils.py", line 1267, in inner
ERROR 06-19 04:26:48 [serving_chat.py:200]     return fn(*args, **kwargs)
ERROR 06-19 04:26:48 [serving_chat.py:200]   File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/entrypoints/chat_utils.py", line 1256, in apply_hf_chat_template
ERROR 06-19 04:26:48 [serving_chat.py:200]     raise ValueError(str(e)) from e
ERROR 06-19 04:26:48 [serving_chat.py:200] ValueError: list index out of range
/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/entrypoints/openai/serving_chat.py:201: RuntimeWarning: coroutine 'AsyncMultiModalItemTracker.all_mm_data' was never awaited
  return self.create_error_response(f"{e} {e.__cause__}")
RuntimeWarning: Enable tracemalloc to get the object allocation traceback
INFO:     192.168.7.58:52575 - "POST /v1/chat/completions HTTP/1.1" 400 Bad Request
INFO:     192.168.7.58:52705 - "POST /v1/chat/completions HTTP/1.1" 400 Bad Request
WARNING 06-19 04:28:55 [protocol.py:58] The following fields were present in the request but ignored: {'prompt'}
ERROR 06-19 04:28:55 [chat_utils.py:1254] An error occurred in `transformers` while applying chat template
ERROR 06-19 04:28:55 [chat_utils.py:1254] Traceback (most recent call last):
ERROR 06-19 04:28:55 [chat_utils.py:1254]   File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/entrypoints/chat_utils.py", line 1240, in apply_hf_chat_template
ERROR 06-19 04:28:55 [chat_utils.py:1254]     return tokenizer.apply_chat_template(
ERROR 06-19 04:28:55 [chat_utils.py:1254]   File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 1635, in apply_chat_template
ERROR 06-19 04:28:55 [chat_utils.py:1254]     isinstance(conversation[0], (list, tuple)) or hasattr(conversation[0], "messages")
ERROR 06-19 04:28:55 [chat_utils.py:1254] IndexError: list index out of range
ERROR 06-19 04:28:55 [serving_chat.py:200] Error in preprocessing prompt inputs
ERROR 06-19 04:28:55 [serving_chat.py:200] Traceback (most recent call last):
ERROR 06-19 04:28:55 [serving_chat.py:200]   File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/entrypoints/chat_utils.py", line 1240, in apply_hf_chat_template
ERROR 06-19 04:28:55 [serving_chat.py:200]     return tokenizer.apply_chat_template(
ERROR 06-19 04:28:55 [serving_chat.py:200]   File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 1635, in apply_chat_template
ERROR 06-19 04:28:55 [serving_chat.py:200]     isinstance(conversation[0], (list, tuple)) or hasattr(conversation[0], "messages")
ERROR 06-19 04:28:55 [serving_chat.py:200] IndexError: list index out of range
ERROR 06-19 04:28:55 [serving_chat.py:200] 
ERROR 06-19 04:28:55 [serving_chat.py:200] The above exception was the direct cause of the following exception:
ERROR 06-19 04:28:55 [serving_chat.py:200] 
ERROR 06-19 04:28:55 [serving_chat.py:200] Traceback (most recent call last):
ERROR 06-19 04:28:55 [serving_chat.py:200]   File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/entrypoints/openai/serving_chat.py", line 183, in create_chat_completion
ERROR 06-19 04:28:55 [serving_chat.py:200]     ) = await self._preprocess_chat(
ERROR 06-19 04:28:55 [serving_chat.py:200]   File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/entrypoints/openai/serving_engine.py", line 811, in _preprocess_chat
ERROR 06-19 04:28:55 [serving_chat.py:200]     request_prompt = apply_hf_chat_template(
ERROR 06-19 04:28:55 [serving_chat.py:200]   File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/utils.py", line 1267, in inner
ERROR 06-19 04:28:55 [serving_chat.py:200]     return fn(*args, **kwargs)
ERROR 06-19 04:28:55 [serving_chat.py:200]   File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/entrypoints/chat_utils.py", line 1256, in apply_hf_chat_template
ERROR 06-19 04:28:55 [serving_chat.py:200]     raise ValueError(str(e)) from e
ERROR 06-19 04:28:55 [serving_chat.py:200] ValueError: list index out of range
INFO:     192.168.7.58:52724 - "POST /v1/chat/completions HTTP/1.1" 400 Bad Request
INFO:     192.168.7.58:52740 - "POST /v1/chat/completions HTTP/1.1" 400 Bad Request
INFO 06-19 04:30:30 [logger.py:43] Received request chatcmpl-2a1ab8bef00f49e4ba2a23bb62ea5b65: prompt: '<｜begin▁of▁sentence｜>User: 讲个笑话\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=4085, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 04:30:30 [async_llm.py:271] Added request chatcmpl-2a1ab8bef00f49e4ba2a23bb62ea5b65.
[1;36m(VllmWorker rank=0 pid=2910082)[0;0m ERROR 06-19 04:30:31 [multiproc_executor.py:527] WorkerProc hit an exception.
[1;36m(VllmWorker rank=0 pid=2910082)[0;0m ERROR 06-19 04:30:31 [multiproc_executor.py:527] Traceback (most recent call last):
[1;36m(VllmWorker rank=0 pid=2910082)[0;0m ERROR 06-19 04:30:31 [multiproc_executor.py:527]   File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/v1/executor/multiproc_executor.py", line 522, in worker_busy_loop
[1;36m(VllmWorker rank=0 pid=2910082)[0;0m ERROR 06-19 04:30:31 [multiproc_executor.py:527]     output = func(*args, **kwargs)
[1;36m(VllmWorker rank=0 pid=2910082)[0;0m ERROR 06-19 04:30:31 [multiproc_executor.py:527]   File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[1;36m(VllmWorker rank=0 pid=2910082)[0;0m ERROR 06-19 04:30:31 [multiproc_executor.py:527]     return func(*args, **kwargs)
[1;36m(VllmWorker rank=0 pid=2910082)[0;0m ERROR 06-19 04:30:31 [multiproc_executor.py:527]   File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/v1/worker/gpu_worker.py", line 293, in execute_model
[1;36m(VllmWorker rank=0 pid=2910082)[0;0m ERROR 06-19 04:30:31 [multiproc_executor.py:527]     output = self.model_runner.execute_model(scheduler_output,
[1;36m(VllmWorker rank=0 pid=2910082)[0;0m ERROR 06-19 04:30:31 [multiproc_executor.py:527]   File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[1;36m(VllmWorker rank=0 pid=2910082)[0;0m ERROR 06-19 04:30:31 [multiproc_executor.py:527]     return func(*args, **kwargs)
[1;36m(VllmWorker rank=0 pid=2910082)[0;0m ERROR 06-19 04:30:31 [multiproc_executor.py:527]   File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1177, in execute_model
[1;36m(VllmWorker rank=0 pid=2910082)[0;0m ERROR 06-19 04:30:31 [multiproc_executor.py:527]     self._update_states(scheduler_output)
[1;36m(VllmWorker rank=0 pid=2910082)[0;0m ERROR 06-19 04:30:31 [multiproc_executor.py:527]   File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/v1/worker/gpu_model_runner.py", line 465, in _update_states
[1;36m(VllmWorker rank=0 pid=2910082)[0;0m ERROR 06-19 04:30:31 [multiproc_executor.py:527]     for block_ids, new_block_ids in zip(  # type: ignore[call-overload]
[1;36m(VllmWorker rank=0 pid=2910082)[0;0m ERROR 06-19 04:30:31 [multiproc_executor.py:527] TypeError: zip() takes no keyword arguments
[1;36m(VllmWorker rank=1 pid=2910083)[0;0m ERROR 06-19 04:30:31 [multiproc_executor.py:527] WorkerProc hit an exception.
[1;36m(VllmWorker rank=1 pid=2910083)[0;0m ERROR 06-19 04:30:31 [multiproc_executor.py:527] Traceback (most recent call last):
[1;36m(VllmWorker rank=1 pid=2910083)[0;0m ERROR 06-19 04:30:31 [multiproc_executor.py:527]   File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/v1/executor/multiproc_executor.py", line 522, in worker_busy_loop
[1;36m(VllmWorker rank=1 pid=2910083)[0;0m ERROR 06-19 04:30:31 [multiproc_executor.py:527]     output = func(*args, **kwargs)
[1;36m(VllmWorker rank=1 pid=2910083)[0;0m ERROR 06-19 04:30:31 [multiproc_executor.py:527]   File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[1;36m(VllmWorker rank=1 pid=2910083)[0;0m ERROR 06-19 04:30:31 [multiproc_executor.py:527]     return func(*args, **kwargs)
[1;36m(VllmWorker rank=1 pid=2910083)[0;0m ERROR 06-19 04:30:31 [multiproc_executor.py:527]   File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/v1/worker/gpu_worker.py", line 293, in execute_model
[1;36m(VllmWorker rank=1 pid=2910083)[0;0m ERROR 06-19 04:30:31 [multiproc_executor.py:527]     output = self.model_runner.execute_model(scheduler_output,
[1;36m(VllmWorker rank=1 pid=2910083)[0;0m ERROR 06-19 04:30:31 [multiproc_executor.py:527]   File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[1;36m(VllmWorker rank=1 pid=2910083)[0;0m ERROR 06-19 04:30:31 [multiproc_executor.py:527]     return func(*args, **kwargs)
[1;36m(VllmWorker rank=1 pid=2910083)[0;0m ERROR 06-19 04:30:31 [multiproc_executor.py:527]   File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1177, in execute_model
[1;36m(VllmWorker rank=1 pid=2910083)[0;0m ERROR 06-19 04:30:31 [multiproc_executor.py:527]     self._update_states(scheduler_output)
[1;36m(VllmWorker rank=1 pid=2910083)[0;0m ERROR 06-19 04:30:31 [multiproc_executor.py:527]   File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/v1/worker/gpu_model_runner.py", line 465, in _update_states
[1;36m(VllmWorker rank=1 pid=2910083)[0;0m ERROR 06-19 04:30:31 [multiproc_executor.py:527]     for block_ids, new_block_ids in zip(  # type: ignore[call-overload]
[1;36m(VllmWorker rank=1 pid=2910083)[0;0m ERROR 06-19 04:30:31 [multiproc_executor.py:527] TypeError: zip() takes no keyword arguments
ERROR 06-19 04:30:31 [dump_input.py:69] Dumping input data
ERROR 06-19 04:30:31 [dump_input.py:71] V1 LLM engine (v0.9.1) with config: model='/home/algo/DeepSeek-Chat-7B', speculative_config=None, tokenizer='/home/algo/DeepSeek-Chat-7B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=glm, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["none"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}, 
ERROR 06-19 04:30:31 [dump_input.py:79] Dumping scheduler output for model execution:
ERROR 06-19 04:30:31 [dump_input.py:80] SchedulerOutput(scheduled_new_reqs=[], scheduled_cached_reqs=[CachedRequestData(req_id='chatcmpl-2a1ab8bef00f49e4ba2a23bb62ea5b65', resumed_from_preemption=false, new_token_ids=[4665], new_block_ids=[[]], num_computed_tokens=11)], num_scheduled_tokens={chatcmpl-2a1ab8bef00f49e4ba2a23bb62ea5b65: 1}, total_num_scheduled_tokens=1, scheduled_spec_decode_tokens={}, scheduled_encoder_inputs={}, num_common_prefix_blocks=[1], finished_req_ids=[], free_encoder_input_ids=[], structured_output_request_ids={}, grammar_bitmask=null, kv_connector_metadata=null)
ERROR 06-19 04:30:31 [dump_input.py:82] SchedulerStats(num_running_reqs=1, num_waiting_reqs=0, gpu_cache_usage=0.0007785130400934648, prefix_cache_stats=PrefixCacheStats(reset=False, requests=0, queries=0, hits=0), spec_decoding_stats=None)
ERROR 06-19 04:30:31 [core.py:517] EngineCore encountered a fatal error.
ERROR 06-19 04:30:31 [core.py:517] Traceback (most recent call last):
ERROR 06-19 04:30:31 [core.py:517]   File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/v1/engine/core.py", line 508, in run_engine_core
ERROR 06-19 04:30:31 [core.py:517]     engine_core.run_busy_loop()
ERROR 06-19 04:30:31 [core.py:517]   File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/v1/engine/core.py", line 535, in run_busy_loop
ERROR 06-19 04:30:31 [core.py:517]     self._process_engine_step()
ERROR 06-19 04:30:31 [core.py:517]   File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/v1/engine/core.py", line 560, in _process_engine_step
ERROR 06-19 04:30:31 [core.py:517]     outputs, model_executed = self.step_fn()
ERROR 06-19 04:30:31 [core.py:517]   File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/v1/engine/core.py", line 231, in step
ERROR 06-19 04:30:31 [core.py:517]     model_output = self.execute_model(scheduler_output)
ERROR 06-19 04:30:31 [core.py:517]   File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/v1/engine/core.py", line 217, in execute_model
ERROR 06-19 04:30:31 [core.py:517]     raise err
ERROR 06-19 04:30:31 [core.py:517]   File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/v1/engine/core.py", line 211, in execute_model
ERROR 06-19 04:30:31 [core.py:517]     return self.model_executor.execute_model(scheduler_output)
ERROR 06-19 04:30:31 [core.py:517]   File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/v1/executor/multiproc_executor.py", line 163, in execute_model
ERROR 06-19 04:30:31 [core.py:517]     (output, ) = self.collective_rpc("execute_model",
ERROR 06-19 04:30:31 [core.py:517]   File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/v1/executor/multiproc_executor.py", line 220, in collective_rpc
ERROR 06-19 04:30:31 [core.py:517]     result = get_response(w, dequeue_timeout)
ERROR 06-19 04:30:31 [core.py:517]   File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/v1/executor/multiproc_executor.py", line 207, in get_response
ERROR 06-19 04:30:31 [core.py:517]     raise RuntimeError(
ERROR 06-19 04:30:31 [core.py:517] RuntimeError: Worker failed with error 'zip() takes no keyword arguments', please check the stack trace above for the root cause
ERROR 06-19 04:30:31 [async_llm.py:420] AsyncLLM output_handler failed.
ERROR 06-19 04:30:31 [async_llm.py:420] Traceback (most recent call last):
ERROR 06-19 04:30:31 [async_llm.py:420]   File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/v1/engine/async_llm.py", line 379, in output_handler
ERROR 06-19 04:30:31 [async_llm.py:420]     outputs = await engine_core.get_output_async()
ERROR 06-19 04:30:31 [async_llm.py:420]   File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/v1/engine/core_client.py", line 790, in get_output_async
ERROR 06-19 04:30:31 [async_llm.py:420]     raise self._format_exception(outputs) from None
ERROR 06-19 04:30:31 [async_llm.py:420] vllm.v1.engine.exceptions.EngineDeadError: EngineCore encountered an issue. See stack trace (above) for the root cause.
INFO 06-19 04:30:31 [async_llm.py:346] Request chatcmpl-2a1ab8bef00f49e4ba2a23bb62ea5b65 failed (engine dead).
INFO:     192.168.7.58:52936 - "POST /v1/chat/completions HTTP/1.1" 500 Internal Server Error
INFO:     Shutting down
INFO:     Waiting for application shutdown.
INFO:     Application shutdown complete.
INFO:     Finished server process [2909761]
INFO 06-19 04:39:08 [__init__.py:243] Automatically detected platform cuda.
INFO 06-19 04:39:10 [__init__.py:31] Available plugins for group vllm.general_plugins:
INFO 06-19 04:39:10 [__init__.py:33] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
INFO 06-19 04:39:10 [__init__.py:36] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 06-19 04:39:11 [api_server.py:1289] vLLM API server version 0.9.0
INFO 06-19 04:39:11 [cli_args.py:300] non-default args: {'host': '0.0.0.0', 'port': 8011, 'model': '/home/algo/DeepSeek-Chat-7B', 'tokenizer': '/home/algo/DeepSeek-Chat-7B', 'trust_remote_code': True, 'dtype': 'float16', 'max_model_len': 4096, 'served_model_name': ['glm'], 'tensor_parallel_size': 2, 'gpu_memory_utilization': 0.7, 'enable_prefix_caching': True, 'max_num_batched_tokens': 8192, 'max_num_seqs': 4}
WARNING 06-19 04:39:11 [config.py:3135] Casting torch.bfloat16 to torch.float16.
INFO 06-19 04:39:17 [config.py:793] This model supports multiple tasks: {'reward', 'classify', 'score', 'embed', 'generate'}. Defaulting to 'generate'.
INFO 06-19 04:39:17 [config.py:1875] Defaulting to use mp for distributed inference
INFO 06-19 04:39:17 [config.py:2118] Chunked prefill is enabled with max_num_batched_tokens=8192.
INFO 06-19 04:39:21 [__init__.py:243] Automatically detected platform cuda.
INFO 06-19 04:39:23 [core.py:438] Waiting for init message from front-end.
INFO 06-19 04:39:23 [__init__.py:31] Available plugins for group vllm.general_plugins:
INFO 06-19 04:39:23 [__init__.py:33] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
INFO 06-19 04:39:23 [__init__.py:36] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 06-19 04:39:23 [core.py:65] Initializing a V1 LLM engine (v0.9.0) with config: model='/home/algo/DeepSeek-Chat-7B', speculative_config=None, tokenizer='/home/algo/DeepSeek-Chat-7B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=glm, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level": 3, "custom_ops": ["none"], "splitting_ops": ["vllm.unified_attention", "vllm.unified_attention_with_output"], "compile_sizes": [], "inductor_compile_config": {"enable_auto_functionalized_v2": false}, "use_cudagraph": true, "cudagraph_num_of_warmups": 1, "cudagraph_capture_sizes": [512, 504, 496, 488, 480, 472, 464, 456, 448, 440, 432, 424, 416, 408, 400, 392, 384, 376, 368, 360, 352, 344, 336, 328, 320, 312, 304, 296, 288, 280, 272, 264, 256, 248, 240, 232, 224, 216, 208, 200, 192, 184, 176, 168, 160, 152, 144, 136, 128, 120, 112, 104, 96, 88, 80, 72, 64, 56, 48, 40, 32, 24, 16, 8, 4, 2, 1], "max_capture_size": 512}
WARNING 06-19 04:39:23 [multiproc_worker_utils.py:306] Reducing Torch parallelism from 8 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 06-19 04:39:23 [shm_broadcast.py:250] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1], buffer_handle=(2, 10485760, 10, 'psm_d7738c44'), local_subscribe_addr='ipc:///tmp/aabd4243-e216-497d-9c54-e59110bed886', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 06-19 04:39:26 [__init__.py:243] Automatically detected platform cuda.
INFO 06-19 04:39:26 [__init__.py:243] Automatically detected platform cuda.
INFO 06-19 04:39:28 [__init__.py:31] Available plugins for group vllm.general_plugins:
INFO 06-19 04:39:28 [__init__.py:31] Available plugins for group vllm.general_plugins:
INFO 06-19 04:39:28 [__init__.py:33] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
INFO 06-19 04:39:28 [__init__.py:33] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
INFO 06-19 04:39:28 [__init__.py:36] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 06-19 04:39:28 [__init__.py:36] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
WARNING 06-19 04:39:28 [utils.py:2671] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f38686a5160>
WARNING 06-19 04:39:28 [utils.py:2671] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f97f771d100>
[1;36m(VllmWorker rank=1 pid=2925871)[0;0m INFO 06-19 04:39:28 [shm_broadcast.py:250] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_29148fd1'), local_subscribe_addr='ipc:///tmp/cb012c62-e699-4a53-9459-d62205c3b005', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(VllmWorker rank=0 pid=2925870)[0;0m INFO 06-19 04:39:28 [shm_broadcast.py:250] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_78990b1a'), local_subscribe_addr='ipc:///tmp/c77baf37-4d53-4099-b31c-4b34fe51728b', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(VllmWorker rank=1 pid=2925871)[0;0m INFO 06-19 04:39:29 [utils.py:1077] Found nccl from library libnccl.so.2
[1;36m(VllmWorker rank=1 pid=2925871)[0;0m INFO 06-19 04:39:29 [pynccl.py:69] vLLM is using nccl==2.26.2
[1;36m(VllmWorker rank=0 pid=2925870)[0;0m INFO 06-19 04:39:29 [utils.py:1077] Found nccl from library libnccl.so.2
[1;36m(VllmWorker rank=0 pid=2925870)[0;0m INFO 06-19 04:39:29 [pynccl.py:69] vLLM is using nccl==2.26.2
[1;36m(VllmWorker rank=1 pid=2925871)[0;0m INFO 06-19 04:39:29 [custom_all_reduce_utils.py:245] reading GPU P2P access cache from /home/algo/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
[1;36m(VllmWorker rank=0 pid=2925870)[0;0m INFO 06-19 04:39:29 [custom_all_reduce_utils.py:245] reading GPU P2P access cache from /home/algo/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
[1;36m(VllmWorker rank=1 pid=2925871)[0;0m WARNING 06-19 04:39:29 [custom_all_reduce.py:146] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorker rank=0 pid=2925870)[0;0m WARNING 06-19 04:39:29 [custom_all_reduce.py:146] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorker rank=0 pid=2925870)[0;0m INFO 06-19 04:39:29 [shm_broadcast.py:250] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_70b92b74'), local_subscribe_addr='ipc:///tmp/dbef057d-367f-4f65-b208-9fa76151f971', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(VllmWorker rank=1 pid=2925871)[0;0m INFO 06-19 04:39:29 [parallel_state.py:1064] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1
[1;36m(VllmWorker rank=0 pid=2925870)[0;0m INFO 06-19 04:39:29 [parallel_state.py:1064] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(VllmWorker rank=0 pid=2925870)[0;0m WARNING 06-19 04:39:29 [topk_topp_sampler.py:58] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(VllmWorker rank=1 pid=2925871)[0;0m WARNING 06-19 04:39:29 [topk_topp_sampler.py:58] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(VllmWorker rank=1 pid=2925871)[0;0m INFO 06-19 04:39:29 [gpu_model_runner.py:1531] Starting to load model /home/algo/DeepSeek-Chat-7B...
[1;36m(VllmWorker rank=0 pid=2925870)[0;0m INFO 06-19 04:39:29 [gpu_model_runner.py:1531] Starting to load model /home/algo/DeepSeek-Chat-7B...
[1;36m(VllmWorker rank=1 pid=2925871)[0;0m INFO 06-19 04:39:29 [cuda.py:217] Using Flash Attention backend on V1 engine.
[1;36m(VllmWorker rank=0 pid=2925870)[0;0m INFO 06-19 04:39:29 [cuda.py:217] Using Flash Attention backend on V1 engine.
[1;36m(VllmWorker rank=1 pid=2925871)[0;0m INFO 06-19 04:39:30 [backends.py:35] Using InductorAdaptor
[1;36m(VllmWorker rank=0 pid=2925870)[0;0m INFO 06-19 04:39:30 [backends.py:35] Using InductorAdaptor
[1;36m(VllmWorker rank=0 pid=2925870)[0;0m Loading pt checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[1;36m(VllmWorker rank=0 pid=2925870)[0;0m Loading pt checkpoint shards:  50% Completed | 1/2 [00:04<00:04,  4.94s/it]
[1;36m(VllmWorker rank=1 pid=2925871)[0;0m INFO 06-19 04:39:36 [default_loader.py:280] Loading weights took 6.54 seconds
[1;36m(VllmWorker rank=0 pid=2925870)[0;0m Loading pt checkpoint shards: 100% Completed | 2/2 [00:06<00:00,  2.99s/it]
[1;36m(VllmWorker rank=0 pid=2925870)[0;0m Loading pt checkpoint shards: 100% Completed | 2/2 [00:07<00:00,  3.68s/it]
[1;36m(VllmWorker rank=0 pid=2925870)[0;0m 
[1;36m(VllmWorker rank=0 pid=2925870)[0;0m INFO 06-19 04:39:37 [default_loader.py:280] Loading weights took 7.36 seconds
[1;36m(VllmWorker rank=1 pid=2925871)[0;0m INFO 06-19 04:39:37 [gpu_model_runner.py:1549] Model loading took 6.4664 GiB and 7.498356 seconds
[1;36m(VllmWorker rank=0 pid=2925870)[0;0m INFO 06-19 04:39:37 [gpu_model_runner.py:1549] Model loading took 6.4664 GiB and 7.499682 seconds
[1;36m(VllmWorker rank=0 pid=2925870)[0;0m INFO 06-19 04:39:41 [backends.py:459] Using cache directory: /home/algo/.cache/vllm/torch_compile_cache/070dd98b07/rank_0_0 for vLLM's torch.compile
[1;36m(VllmWorker rank=1 pid=2925871)[0;0m INFO 06-19 04:39:41 [backends.py:459] Using cache directory: /home/algo/.cache/vllm/torch_compile_cache/070dd98b07/rank_1_0 for vLLM's torch.compile
[1;36m(VllmWorker rank=1 pid=2925871)[0;0m INFO 06-19 04:39:42 [backends.py:469] Dynamo bytecode transform time: 4.85 s
[1;36m(VllmWorker rank=0 pid=2925870)[0;0m INFO 06-19 04:39:42 [backends.py:469] Dynamo bytecode transform time: 4.85 s
[1;36m(VllmWorker rank=1 pid=2925871)[0;0m INFO 06-19 04:39:44 [backends.py:158] Cache the graph of shape None for later use
[1;36m(VllmWorker rank=0 pid=2925870)[0;0m INFO 06-19 04:39:44 [backends.py:158] Cache the graph of shape None for later use
[1;36m(VllmWorker rank=0 pid=2925870)[0;0m INFO 06-19 04:39:59 [backends.py:170] Compiling a graph for general shape takes 17.04 s
[1;36m(VllmWorker rank=1 pid=2925871)[0;0m INFO 06-19 04:39:59 [backends.py:170] Compiling a graph for general shape takes 17.19 s
[1;36m(VllmWorker rank=0 pid=2925870)[0;0m INFO 06-19 04:40:08 [monitor.py:33] torch.compile takes 21.89 s in total
[1;36m(VllmWorker rank=1 pid=2925871)[0;0m INFO 06-19 04:40:08 [monitor.py:33] torch.compile takes 22.04 s in total
INFO 06-19 04:40:09 [kv_cache_utils.py:637] GPU KV cache size: 17,728 tokens
INFO 06-19 04:40:09 [kv_cache_utils.py:640] Maximum concurrency for 4,096 tokens per request: 4.33x
INFO 06-19 04:40:09 [kv_cache_utils.py:637] GPU KV cache size: 26,416 tokens
INFO 06-19 04:40:09 [kv_cache_utils.py:640] Maximum concurrency for 4,096 tokens per request: 6.45x
INFO 06-19 04:40:20 [__init__.py:243] Automatically detected platform cuda.
INFO 06-19 04:40:22 [__init__.py:31] Available plugins for group vllm.general_plugins:
INFO 06-19 04:40:22 [__init__.py:33] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
INFO 06-19 04:40:22 [__init__.py:36] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 06-19 04:40:23 [api_server.py:1289] vLLM API server version 0.9.0
INFO 06-19 04:40:24 [cli_args.py:300] non-default args: {'host': '0.0.0.0', 'port': 8011, 'model': '/home/algo/DeepSeek-Chat-7B', 'tokenizer': '/home/algo/DeepSeek-Chat-7B', 'trust_remote_code': True, 'dtype': 'float16', 'max_model_len': 4096, 'served_model_name': ['glm'], 'gpu_memory_utilization': 0.7, 'enable_prefix_caching': True, 'max_num_batched_tokens': 8192, 'max_num_seqs': 4}
WARNING 06-19 04:40:24 [config.py:3135] Casting torch.bfloat16 to torch.float16.
INFO 06-19 04:40:30 [config.py:793] This model supports multiple tasks: {'generate', 'score', 'reward', 'embed', 'classify'}. Defaulting to 'generate'.
INFO 06-19 04:40:30 [config.py:2118] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(VllmWorker rank=1 pid=2925871)[0;0m INFO 06-19 04:40:33 [gpu_model_runner.py:1933] Graph capturing finished in 24 secs, took 1.77 GiB
[1;36m(VllmWorker rank=0 pid=2925870)[0;0m INFO 06-19 04:40:33 [gpu_model_runner.py:1933] Graph capturing finished in 24 secs, took 1.77 GiB
INFO 06-19 04:40:33 [core.py:167] init engine (profile, create kv cache, warmup model) took 56.09 seconds
INFO 06-19 04:40:33 [loggers.py:134] vllm cache_config_info with initialization after num_gpu_blocks is: 1108
WARNING 06-19 04:40:33 [config.py:1339] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
INFO 06-19 04:40:33 [serving_chat.py:117] Using default chat sampling params from model: {'temperature': 0.7, 'top_p': 0.95}
INFO 06-19 04:40:33 [serving_completion.py:65] Using default completion sampling params from model: {'temperature': 0.7, 'top_p': 0.95}
INFO 06-19 04:40:33 [api_server.py:1336] Starting vLLM API server on http://0.0.0.0:8011
INFO 06-19 04:40:33 [launcher.py:28] Available routes are:
INFO 06-19 04:40:33 [launcher.py:36] Route: /openapi.json, Methods: GET, HEAD
INFO 06-19 04:40:33 [launcher.py:36] Route: /docs, Methods: GET, HEAD
INFO 06-19 04:40:33 [launcher.py:36] Route: /docs/oauth2-redirect, Methods: GET, HEAD
INFO 06-19 04:40:33 [launcher.py:36] Route: /redoc, Methods: GET, HEAD
INFO 06-19 04:40:33 [launcher.py:36] Route: /health, Methods: GET
INFO 06-19 04:40:33 [launcher.py:36] Route: /load, Methods: GET
INFO 06-19 04:40:33 [launcher.py:36] Route: /ping, Methods: POST
INFO 06-19 04:40:33 [launcher.py:36] Route: /ping, Methods: GET
INFO 06-19 04:40:33 [launcher.py:36] Route: /tokenize, Methods: POST
INFO 06-19 04:40:33 [launcher.py:36] Route: /detokenize, Methods: POST
INFO 06-19 04:40:33 [launcher.py:36] Route: /v1/models, Methods: GET
INFO 06-19 04:40:33 [launcher.py:36] Route: /version, Methods: GET
INFO 06-19 04:40:33 [launcher.py:36] Route: /v1/chat/completions, Methods: POST
INFO 06-19 04:40:33 [launcher.py:36] Route: /v1/completions, Methods: POST
INFO 06-19 04:40:33 [launcher.py:36] Route: /v1/embeddings, Methods: POST
INFO 06-19 04:40:33 [launcher.py:36] Route: /pooling, Methods: POST
INFO 06-19 04:40:33 [launcher.py:36] Route: /classify, Methods: POST
INFO 06-19 04:40:33 [launcher.py:36] Route: /score, Methods: POST
INFO 06-19 04:40:33 [launcher.py:36] Route: /v1/score, Methods: POST
INFO 06-19 04:40:33 [launcher.py:36] Route: /v1/audio/transcriptions, Methods: POST
INFO 06-19 04:40:33 [launcher.py:36] Route: /rerank, Methods: POST
INFO 06-19 04:40:33 [launcher.py:36] Route: /v1/rerank, Methods: POST
INFO 06-19 04:40:33 [launcher.py:36] Route: /v2/rerank, Methods: POST
INFO 06-19 04:40:33 [launcher.py:36] Route: /invocations, Methods: POST
INFO 06-19 04:40:33 [launcher.py:36] Route: /metrics, Methods: GET
INFO:     Started server process [2925537]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO 06-19 04:40:34 [__init__.py:243] Automatically detected platform cuda.
INFO 06-19 04:40:37 [core.py:438] Waiting for init message from front-end.
INFO 06-19 04:40:37 [__init__.py:31] Available plugins for group vllm.general_plugins:
INFO 06-19 04:40:37 [__init__.py:33] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
INFO 06-19 04:40:37 [__init__.py:36] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 06-19 04:40:37 [core.py:65] Initializing a V1 LLM engine (v0.9.0) with config: model='/home/algo/DeepSeek-Chat-7B', speculative_config=None, tokenizer='/home/algo/DeepSeek-Chat-7B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=glm, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level": 3, "custom_ops": ["none"], "splitting_ops": ["vllm.unified_attention", "vllm.unified_attention_with_output"], "compile_sizes": [], "inductor_compile_config": {"enable_auto_functionalized_v2": false}, "use_cudagraph": true, "cudagraph_num_of_warmups": 1, "cudagraph_capture_sizes": [512, 504, 496, 488, 480, 472, 464, 456, 448, 440, 432, 424, 416, 408, 400, 392, 384, 376, 368, 360, 352, 344, 336, 328, 320, 312, 304, 296, 288, 280, 272, 264, 256, 248, 240, 232, 224, 216, 208, 200, 192, 184, 176, 168, 160, 152, 144, 136, 128, 120, 112, 104, 96, 88, 80, 72, 64, 56, 48, 40, 32, 24, 16, 8, 4, 2, 1], "max_capture_size": 512}
WARNING 06-19 04:40:37 [utils.py:2671] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7fde379db0a0>
INFO 06-19 04:40:37 [parallel_state.py:1064] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
WARNING 06-19 04:40:37 [topk_topp_sampler.py:58] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
INFO 06-19 04:40:37 [gpu_model_runner.py:1531] Starting to load model /home/algo/DeepSeek-Chat-7B...
INFO 06-19 04:40:37 [cuda.py:217] Using Flash Attention backend on V1 engine.
ERROR 06-19 04:40:37 [core.py:500] EngineCore failed to start.
ERROR 06-19 04:40:37 [core.py:500] Traceback (most recent call last):
ERROR 06-19 04:40:37 [core.py:500]   File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/v1/engine/core.py", line 491, in run_engine_core
ERROR 06-19 04:40:37 [core.py:500]     engine_core = EngineCoreProc(*args, **kwargs)
ERROR 06-19 04:40:37 [core.py:500]   File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/v1/engine/core.py", line 390, in __init__
ERROR 06-19 04:40:37 [core.py:500]     super().__init__(vllm_config, executor_class, log_stats,
ERROR 06-19 04:40:37 [core.py:500]   File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/v1/engine/core.py", line 71, in __init__
ERROR 06-19 04:40:37 [core.py:500]     self.model_executor = executor_class(vllm_config)
ERROR 06-19 04:40:37 [core.py:500]   File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/executor/executor_base.py", line 52, in __init__
ERROR 06-19 04:40:37 [core.py:500]     self._init_executor()
ERROR 06-19 04:40:37 [core.py:500]   File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
ERROR 06-19 04:40:37 [core.py:500]     self.collective_rpc("load_model")
ERROR 06-19 04:40:37 [core.py:500]   File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
ERROR 06-19 04:40:37 [core.py:500]     answer = run_method(self.driver_worker, method, args, kwargs)
ERROR 06-19 04:40:37 [core.py:500]   File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/utils.py", line 2605, in run_method
ERROR 06-19 04:40:37 [core.py:500]     return func(*args, **kwargs)
ERROR 06-19 04:40:37 [core.py:500]   File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/v1/worker/gpu_worker.py", line 164, in load_model
ERROR 06-19 04:40:37 [core.py:500]     self.model_runner.load_model()
ERROR 06-19 04:40:37 [core.py:500]   File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1534, in load_model
ERROR 06-19 04:40:37 [core.py:500]     self.model = get_model(vllm_config=self.vllm_config)
ERROR 06-19 04:40:37 [core.py:500]   File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/model_executor/model_loader/__init__.py", line 58, in get_model
ERROR 06-19 04:40:37 [core.py:500]     return loader.load_model(vllm_config=vllm_config,
ERROR 06-19 04:40:37 [core.py:500]   File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/model_executor/model_loader/default_loader.py", line 273, in load_model
ERROR 06-19 04:40:37 [core.py:500]     model = initialize_model(vllm_config=vllm_config,
ERROR 06-19 04:40:37 [core.py:500]   File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/model_executor/model_loader/utils.py", line 61, in initialize_model
ERROR 06-19 04:40:37 [core.py:500]     return model_class(vllm_config=vllm_config, prefix=prefix)
ERROR 06-19 04:40:37 [core.py:500]   File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/model_executor/models/llama.py", line 520, in __init__
ERROR 06-19 04:40:37 [core.py:500]     self.model = self._init_model(vllm_config=vllm_config,
ERROR 06-19 04:40:37 [core.py:500]   File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/model_executor/models/llama.py", line 566, in _init_model
ERROR 06-19 04:40:37 [core.py:500]     return LlamaModel(vllm_config=vllm_config,
ERROR 06-19 04:40:37 [core.py:500]   File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/compilation/decorators.py", line 151, in __init__
ERROR 06-19 04:40:37 [core.py:500]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
ERROR 06-19 04:40:37 [core.py:500]   File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/model_executor/models/llama.py", line 345, in __init__
ERROR 06-19 04:40:37 [core.py:500]     self.start_layer, self.end_layer, self.layers = make_layers(
ERROR 06-19 04:40:37 [core.py:500]   File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/model_executor/models/utils.py", line 625, in make_layers
ERROR 06-19 04:40:37 [core.py:500]     [PPMissingLayer() for _ in range(start_layer)] + [
ERROR 06-19 04:40:37 [core.py:500]   File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/model_executor/models/utils.py", line 626, in <listcomp>
ERROR 06-19 04:40:37 [core.py:500]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
ERROR 06-19 04:40:37 [core.py:500]   File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/model_executor/models/llama.py", line 347, in <lambda>
ERROR 06-19 04:40:37 [core.py:500]     lambda prefix: layer_type(config=config,
ERROR 06-19 04:40:37 [core.py:500]   File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/model_executor/models/llama.py", line 278, in __init__
ERROR 06-19 04:40:37 [core.py:500]     self.mlp = LlamaMLP(
ERROR 06-19 04:40:37 [core.py:500]   File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/model_executor/models/llama.py", line 78, in __init__
ERROR 06-19 04:40:37 [core.py:500]     self.down_proj = RowParallelLinear(
ERROR 06-19 04:40:37 [core.py:500]   File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/model_executor/layers/linear.py", line 1199, in __init__
ERROR 06-19 04:40:37 [core.py:500]     self.quant_method.create_weights(
ERROR 06-19 04:40:37 [core.py:500]   File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
ERROR 06-19 04:40:37 [core.py:500]     weight = Parameter(torch.empty(sum(output_partition_sizes),
ERROR 06-19 04:40:37 [core.py:500]   File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/torch/utils/_device.py", line 104, in __torch_function__
ERROR 06-19 04:40:37 [core.py:500]     return func(*args, **kwargs)
ERROR 06-19 04:40:37 [core.py:500] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 86.00 MiB. GPU 0 has a total capacity of 23.65 GiB of which 82.81 MiB is free. Process 2217703 has 3.18 GiB memory in use. Process 2306136 has 1.65 GiB memory in use. Process 2925870 has 13.01 GiB memory in use. Including non-PyTorch memory, this process has 5.72 GiB memory in use. Of the allocated memory 5.28 GiB is allocated by PyTorch, and 573.50 KiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Process EngineCore_0:
Traceback (most recent call last):
  File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/v1/engine/core.py", line 504, in run_engine_core
    raise e
  File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/v1/engine/core.py", line 491, in run_engine_core
    engine_core = EngineCoreProc(*args, **kwargs)
  File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/v1/engine/core.py", line 390, in __init__
    super().__init__(vllm_config, executor_class, log_stats,
  File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/v1/engine/core.py", line 71, in __init__
    self.model_executor = executor_class(vllm_config)
  File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/executor/executor_base.py", line 52, in __init__
    self._init_executor()
  File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
    self.collective_rpc("load_model")
  File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
    answer = run_method(self.driver_worker, method, args, kwargs)
  File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/utils.py", line 2605, in run_method
    return func(*args, **kwargs)
  File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/v1/worker/gpu_worker.py", line 164, in load_model
    self.model_runner.load_model()
  File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1534, in load_model
    self.model = get_model(vllm_config=self.vllm_config)
  File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/model_executor/model_loader/__init__.py", line 58, in get_model
    return loader.load_model(vllm_config=vllm_config,
  File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/model_executor/model_loader/default_loader.py", line 273, in load_model
    model = initialize_model(vllm_config=vllm_config,
  File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/model_executor/model_loader/utils.py", line 61, in initialize_model
    return model_class(vllm_config=vllm_config, prefix=prefix)
  File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/model_executor/models/llama.py", line 520, in __init__
    self.model = self._init_model(vllm_config=vllm_config,
  File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/model_executor/models/llama.py", line 566, in _init_model
    return LlamaModel(vllm_config=vllm_config,
  File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/compilation/decorators.py", line 151, in __init__
    old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
  File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/model_executor/models/llama.py", line 345, in __init__
    self.start_layer, self.end_layer, self.layers = make_layers(
  File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/model_executor/models/utils.py", line 625, in make_layers
    [PPMissingLayer() for _ in range(start_layer)] + [
  File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/model_executor/models/utils.py", line 626, in <listcomp>
    maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
  File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/model_executor/models/llama.py", line 347, in <lambda>
    lambda prefix: layer_type(config=config,
  File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/model_executor/models/llama.py", line 278, in __init__
    self.mlp = LlamaMLP(
  File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/model_executor/models/llama.py", line 78, in __init__
    self.down_proj = RowParallelLinear(
  File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/model_executor/layers/linear.py", line 1199, in __init__
    self.quant_method.create_weights(
  File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/model_executor/layers/linear.py", line 189, in create_weights
    weight = Parameter(torch.empty(sum(output_partition_sizes),
  File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/torch/utils/_device.py", line 104, in __torch_function__
    return func(*args, **kwargs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 86.00 MiB. GPU 0 has a total capacity of 23.65 GiB of which 82.81 MiB is free. Process 2217703 has 3.18 GiB memory in use. Process 2306136 has 1.65 GiB memory in use. Process 2925870 has 13.01 GiB memory in use. Including non-PyTorch memory, this process has 5.72 GiB memory in use. Of the allocated memory 5.28 GiB is allocated by PyTorch, and 573.50 KiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W619 04:40:38.317491263 ProcessGroupNCCL.cpp:1476] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/entrypoints/openai/api_server.py", line 1376, in <module>
    uvloop.run(run_server(args))
  File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/uvloop/__init__.py", line 82, in run
    return loop.run_until_complete(wrapper())
  File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
  File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/uvloop/__init__.py", line 61, in wrapper
    return await main
  File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/entrypoints/openai/api_server.py", line 1324, in run_server
    async with build_async_engine_client(args) as engine_client:
  File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/contextlib.py", line 181, in __aenter__
    return await self.gen.__anext__()
  File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/entrypoints/openai/api_server.py", line 153, in build_async_engine_client
    async with build_async_engine_client_from_engine_args(
  File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/contextlib.py", line 181, in __aenter__
    return await self.gen.__anext__()
  File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/entrypoints/openai/api_server.py", line 185, in build_async_engine_client_from_engine_args
    async_llm = AsyncLLM.from_vllm_config(
  File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/v1/engine/async_llm.py", line 157, in from_vllm_config
    return cls(
  File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/v1/engine/async_llm.py", line 123, in __init__
    self.engine_core = core_client_class(
  File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/v1/engine/core_client.py", line 734, in __init__
    super().__init__(
  File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/v1/engine/core_client.py", line 418, in __init__
    self._wait_for_engine_startup(output_address, parallel_config)
  File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/v1/engine/core_client.py", line 484, in _wait_for_engine_startup
    raise RuntimeError("Engine core initialization failed. "
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
INFO 06-19 04:41:59 [launcher.py:79] Shutting down FastAPI HTTP server.
INFO:     Shutting down
INFO:     Waiting for application shutdown.
INFO:     Application shutdown complete.
INFO 06-19 04:42:20 [__init__.py:243] Automatically detected platform cuda.
INFO 06-19 04:42:22 [__init__.py:31] Available plugins for group vllm.general_plugins:
INFO 06-19 04:42:22 [__init__.py:33] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
INFO 06-19 04:42:22 [__init__.py:36] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 06-19 04:42:23 [api_server.py:1289] vLLM API server version 0.9.0
INFO 06-19 04:42:23 [cli_args.py:300] non-default args: {'host': '0.0.0.0', 'port': 8011, 'model': '/home/algo/DeepSeek-Chat-7B', 'tokenizer': '/home/algo/DeepSeek-Chat-7B', 'trust_remote_code': True, 'dtype': 'float16', 'max_model_len': 4096, 'served_model_name': ['glm'], 'gpu_memory_utilization': 0.7, 'enable_prefix_caching': True, 'max_num_batched_tokens': 8192, 'max_num_seqs': 4}
WARNING 06-19 04:42:23 [config.py:3135] Casting torch.bfloat16 to torch.float16.
INFO 06-19 04:42:29 [config.py:793] This model supports multiple tasks: {'score', 'reward', 'classify', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 06-19 04:42:29 [config.py:2118] Chunked prefill is enabled with max_num_batched_tokens=8192.
INFO 06-19 04:42:33 [__init__.py:243] Automatically detected platform cuda.
INFO 06-19 04:42:35 [core.py:438] Waiting for init message from front-end.
INFO 06-19 04:42:35 [__init__.py:31] Available plugins for group vllm.general_plugins:
INFO 06-19 04:42:35 [__init__.py:33] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
INFO 06-19 04:42:35 [__init__.py:36] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 06-19 04:42:35 [core.py:65] Initializing a V1 LLM engine (v0.9.0) with config: model='/home/algo/DeepSeek-Chat-7B', speculative_config=None, tokenizer='/home/algo/DeepSeek-Chat-7B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=glm, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level": 3, "custom_ops": ["none"], "splitting_ops": ["vllm.unified_attention", "vllm.unified_attention_with_output"], "compile_sizes": [], "inductor_compile_config": {"enable_auto_functionalized_v2": false}, "use_cudagraph": true, "cudagraph_num_of_warmups": 1, "cudagraph_capture_sizes": [512, 504, 496, 488, 480, 472, 464, 456, 448, 440, 432, 424, 416, 408, 400, 392, 384, 376, 368, 360, 352, 344, 336, 328, 320, 312, 304, 296, 288, 280, 272, 264, 256, 248, 240, 232, 224, 216, 208, 200, 192, 184, 176, 168, 160, 152, 144, 136, 128, 120, 112, 104, 96, 88, 80, 72, 64, 56, 48, 40, 32, 24, 16, 8, 4, 2, 1], "max_capture_size": 512}
WARNING 06-19 04:42:35 [utils.py:2671] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f709fddb250>
INFO 06-19 04:42:35 [parallel_state.py:1064] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
WARNING 06-19 04:42:35 [topk_topp_sampler.py:58] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
INFO 06-19 04:42:36 [gpu_model_runner.py:1531] Starting to load model /home/algo/DeepSeek-Chat-7B...
INFO 06-19 04:42:36 [cuda.py:217] Using Flash Attention backend on V1 engine.
INFO 06-19 04:42:36 [backends.py:35] Using InductorAdaptor
Loading pt checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
Loading pt checkpoint shards:  50% Completed | 1/2 [00:03<00:03,  3.36s/it]
Loading pt checkpoint shards: 100% Completed | 2/2 [00:04<00:00,  2.15s/it]
Loading pt checkpoint shards: 100% Completed | 2/2 [00:04<00:00,  2.33s/it]

INFO 06-19 04:42:40 [default_loader.py:280] Loading weights took 4.67 seconds
INFO 06-19 04:42:41 [gpu_model_runner.py:1549] Model loading took 12.8726 GiB and 4.815828 seconds
INFO 06-19 04:42:45 [backends.py:459] Using cache directory: /home/algo/.cache/vllm/torch_compile_cache/ea48b2c474/rank_0_0 for vLLM's torch.compile
INFO 06-19 04:42:45 [backends.py:469] Dynamo bytecode transform time: 4.03 s
INFO 06-19 04:42:46 [backends.py:158] Cache the graph of shape None for later use
INFO 06-19 04:43:01 [backends.py:170] Compiling a graph for general shape takes 16.18 s
INFO 06-19 04:43:09 [monitor.py:33] torch.compile takes 20.21 s in total
ERROR 06-19 04:43:10 [core.py:500] EngineCore failed to start.
ERROR 06-19 04:43:10 [core.py:500] Traceback (most recent call last):
ERROR 06-19 04:43:10 [core.py:500]   File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/v1/engine/core.py", line 491, in run_engine_core
ERROR 06-19 04:43:10 [core.py:500]     engine_core = EngineCoreProc(*args, **kwargs)
ERROR 06-19 04:43:10 [core.py:500]   File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/v1/engine/core.py", line 390, in __init__
ERROR 06-19 04:43:10 [core.py:500]     super().__init__(vllm_config, executor_class, log_stats,
ERROR 06-19 04:43:10 [core.py:500]   File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/v1/engine/core.py", line 78, in __init__
ERROR 06-19 04:43:10 [core.py:500]     self._initialize_kv_caches(vllm_config)
ERROR 06-19 04:43:10 [core.py:500]   File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/v1/engine/core.py", line 141, in _initialize_kv_caches
ERROR 06-19 04:43:10 [core.py:500]     kv_cache_configs = [
ERROR 06-19 04:43:10 [core.py:500]   File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/v1/engine/core.py", line 142, in <listcomp>
ERROR 06-19 04:43:10 [core.py:500]     get_kv_cache_config(vllm_config, kv_cache_spec_one_worker,
ERROR 06-19 04:43:10 [core.py:500]   File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/v1/core/kv_cache_utils.py", line 703, in get_kv_cache_config
ERROR 06-19 04:43:10 [core.py:500]     check_enough_kv_cache_memory(vllm_config, kv_cache_spec, available_memory)
ERROR 06-19 04:43:10 [core.py:500]   File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/v1/core/kv_cache_utils.py", line 532, in check_enough_kv_cache_memory
ERROR 06-19 04:43:10 [core.py:500]     raise ValueError("No available memory for the cache blocks. "
ERROR 06-19 04:43:10 [core.py:500] ValueError: No available memory for the cache blocks. Try increasing `gpu_memory_utilization` when initializing the engine.
Process EngineCore_0:
Traceback (most recent call last):
  File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/v1/engine/core.py", line 504, in run_engine_core
    raise e
  File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/v1/engine/core.py", line 491, in run_engine_core
    engine_core = EngineCoreProc(*args, **kwargs)
  File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/v1/engine/core.py", line 390, in __init__
    super().__init__(vllm_config, executor_class, log_stats,
  File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/v1/engine/core.py", line 78, in __init__
    self._initialize_kv_caches(vllm_config)
  File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/v1/engine/core.py", line 141, in _initialize_kv_caches
    kv_cache_configs = [
  File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/v1/engine/core.py", line 142, in <listcomp>
    get_kv_cache_config(vllm_config, kv_cache_spec_one_worker,
  File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/v1/core/kv_cache_utils.py", line 703, in get_kv_cache_config
    check_enough_kv_cache_memory(vllm_config, kv_cache_spec, available_memory)
  File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/v1/core/kv_cache_utils.py", line 532, in check_enough_kv_cache_memory
    raise ValueError("No available memory for the cache blocks. "
ValueError: No available memory for the cache blocks. Try increasing `gpu_memory_utilization` when initializing the engine.
[rank0]:[W619 04:43:11.406250091 ProcessGroupNCCL.cpp:1476] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/entrypoints/openai/api_server.py", line 1376, in <module>
    uvloop.run(run_server(args))
  File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/uvloop/__init__.py", line 82, in run
    return loop.run_until_complete(wrapper())
  File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
  File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/uvloop/__init__.py", line 61, in wrapper
    return await main
  File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/entrypoints/openai/api_server.py", line 1324, in run_server
    async with build_async_engine_client(args) as engine_client:
  File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/contextlib.py", line 181, in __aenter__
    return await self.gen.__anext__()
  File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/entrypoints/openai/api_server.py", line 153, in build_async_engine_client
    async with build_async_engine_client_from_engine_args(
  File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/contextlib.py", line 181, in __aenter__
    return await self.gen.__anext__()
  File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/entrypoints/openai/api_server.py", line 185, in build_async_engine_client_from_engine_args
    async_llm = AsyncLLM.from_vllm_config(
  File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/v1/engine/async_llm.py", line 157, in from_vllm_config
    return cls(
  File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/v1/engine/async_llm.py", line 123, in __init__
    self.engine_core = core_client_class(
  File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/v1/engine/core_client.py", line 734, in __init__
    super().__init__(
  File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/v1/engine/core_client.py", line 418, in __init__
    self._wait_for_engine_startup(output_address, parallel_config)
  File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/v1/engine/core_client.py", line 484, in _wait_for_engine_startup
    raise RuntimeError("Engine core initialization failed. "
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
INFO 06-19 05:09:50 [__init__.py:239] Automatically detected platform cuda.
INFO 06-19 05:09:52 [api_server.py:1043] vLLM API server version 0.8.5.post1
INFO 06-19 05:09:52 [api_server.py:1044] args: Namespace(host='0.0.0.0', port=8011, uvicorn_log_level='info', disable_uvicorn_access_log=False, allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, enable_ssl_refresh=False, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='/home/algo/DeepSeek-Chat-7B', task='auto', tokenizer='/home/algo/DeepSeek-Chat-7B', hf_config_path=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=True, allowed_local_media_path=None, load_format='auto', download_dir=None, model_loader_extra_config={}, use_tqdm_on_load=True, config_format=<ConfigFormat.AUTO: 'auto'>, dtype='float16', max_model_len=4096, guided_decoding_backend='auto', reasoning_parser=None, logits_processor_pattern=None, model_impl='auto', distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=2, data_parallel_size=1, enable_expert_parallel=False, max_parallel_loading_workers=None, ray_workers_use_nsight=False, disable_custom_all_reduce=False, block_size=None, gpu_memory_utilization=0.7, swap_space=4.0, kv_cache_dtype='auto', num_gpu_blocks_override=None, enable_prefix_caching=True, prefix_caching_hash_algo='builtin', cpu_offload_gb=0, calculate_kv_scales=False, disable_sliding_window=False, use_v2_block_manager=True, seed=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_token=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config={}, limit_mm_per_prompt={}, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=None, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=None, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', speculative_config=None, ignore_patterns=[], served_model_name=['glm'], qlora_adapter_name_or_path=None, show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, max_num_batched_tokens=8192, max_num_seqs=4, max_num_partial_prefills=1, max_long_partial_prefills=1, long_prefill_token_threshold=0, num_lookahead_slots=0, scheduler_delay_factor=0.0, preemption_mode=None, num_scheduler_steps=1, multi_step_stream_outputs=True, scheduling_policy='fcfs', enable_chunked_prefill=None, disable_chunked_mm_input=False, scheduler_cls='vllm.core.scheduler.Scheduler', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', worker_extension_cls='', generation_config='auto', override_generation_config=None, enable_sleep_mode=False, additional_config=None, enable_reasoning=False, disable_cascade_attn=False, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, enable_server_load_tracking=False)
WARNING 06-19 05:09:52 [config.py:2972] Casting torch.bfloat16 to torch.float16.
INFO 06-19 05:09:57 [config.py:717] This model supports multiple tasks: {'embed', 'reward', 'classify', 'generate', 'score'}. Defaulting to 'generate'.
INFO 06-19 05:09:57 [config.py:1770] Defaulting to use mp for distributed inference
INFO 06-19 05:09:57 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=8192.
INFO 06-19 05:10:00 [__init__.py:239] Automatically detected platform cuda.
INFO 06-19 05:10:02 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='/home/algo/DeepSeek-Chat-7B', speculative_config=None, tokenizer='/home/algo/DeepSeek-Chat-7B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=glm, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"level":3,"custom_ops":["none"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":512}
WARNING 06-19 05:10:02 [multiproc_worker_utils.py:306] Reducing Torch parallelism from 8 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 06-19 05:10:02 [shm_broadcast.py:266] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1], buffer_handle=(2, 10485760, 10, 'psm_81d43d18'), local_subscribe_addr='ipc:///tmp/8eee0e9e-cae6-4038-bbf0-15b7c33fada3', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 06-19 05:10:05 [__init__.py:239] Automatically detected platform cuda.
INFO 06-19 05:10:05 [__init__.py:239] Automatically detected platform cuda.
WARNING 06-19 05:10:07 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f24d59400a0>
WARNING 06-19 05:10:07 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f71e5db40a0>
[1;36m(VllmWorker rank=0 pid=2951451)[0;0m INFO 06-19 05:10:07 [shm_broadcast.py:266] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_5dec8184'), local_subscribe_addr='ipc:///tmp/f6a046de-c69f-4bc3-a257-9737fa4ccb0d', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(VllmWorker rank=1 pid=2951452)[0;0m INFO 06-19 05:10:07 [shm_broadcast.py:266] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_263bb78f'), local_subscribe_addr='ipc:///tmp/ee3be601-47fa-4dd0-b944-fdfda23e99c5', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(VllmWorker rank=1 pid=2951452)[0;0m INFO 06-19 05:10:08 [utils.py:1055] Found nccl from library libnccl.so.2
[1;36m(VllmWorker rank=0 pid=2951451)[0;0m INFO 06-19 05:10:08 [utils.py:1055] Found nccl from library libnccl.so.2
[1;36m(VllmWorker rank=1 pid=2951452)[0;0m INFO 06-19 05:10:08 [pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorker rank=0 pid=2951451)[0;0m INFO 06-19 05:10:08 [pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorker rank=0 pid=2951451)[0;0m INFO 06-19 05:10:08 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /home/algo/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
[1;36m(VllmWorker rank=1 pid=2951452)[0;0m INFO 06-19 05:10:08 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /home/algo/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
[1;36m(VllmWorker rank=1 pid=2951452)[0;0m WARNING 06-19 05:10:08 [custom_all_reduce.py:146] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorker rank=0 pid=2951451)[0;0m WARNING 06-19 05:10:08 [custom_all_reduce.py:146] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorker rank=0 pid=2951451)[0;0m INFO 06-19 05:10:08 [shm_broadcast.py:266] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_a52bb9b9'), local_subscribe_addr='ipc:///tmp/ac6fed29-24d7-42be-8583-4c04a24efe40', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(VllmWorker rank=1 pid=2951452)[0;0m INFO 06-19 05:10:08 [parallel_state.py:1004] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1
[1;36m(VllmWorker rank=0 pid=2951451)[0;0m INFO 06-19 05:10:08 [parallel_state.py:1004] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0
[1;36m(VllmWorker rank=1 pid=2951452)[0;0m INFO 06-19 05:10:08 [cuda.py:221] Using Flash Attention backend on V1 engine.
[1;36m(VllmWorker rank=0 pid=2951451)[0;0m INFO 06-19 05:10:08 [cuda.py:221] Using Flash Attention backend on V1 engine.
[1;36m(VllmWorker rank=1 pid=2951452)[0;0m WARNING 06-19 05:10:08 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(VllmWorker rank=0 pid=2951451)[0;0m WARNING 06-19 05:10:08 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(VllmWorker rank=1 pid=2951452)[0;0m INFO 06-19 05:10:08 [gpu_model_runner.py:1329] Starting to load model /home/algo/DeepSeek-Chat-7B...
[1;36m(VllmWorker rank=0 pid=2951451)[0;0m INFO 06-19 05:10:08 [gpu_model_runner.py:1329] Starting to load model /home/algo/DeepSeek-Chat-7B...
[1;36m(VllmWorker rank=0 pid=2951451)[0;0m Loading pt checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[1;36m(VllmWorker rank=0 pid=2951451)[0;0m Loading pt checkpoint shards:  50% Completed | 1/2 [00:04<00:04,  4.90s/it]
[1;36m(VllmWorker rank=1 pid=2951452)[0;0m INFO 06-19 05:10:15 [loader.py:458] Loading weights took 6.56 seconds
[1;36m(VllmWorker rank=0 pid=2951451)[0;0m Loading pt checkpoint shards: 100% Completed | 2/2 [00:06<00:00,  2.99s/it]
[1;36m(VllmWorker rank=0 pid=2951451)[0;0m Loading pt checkpoint shards: 100% Completed | 2/2 [00:06<00:00,  3.28s/it]
[1;36m(VllmWorker rank=0 pid=2951451)[0;0m 
[1;36m(VllmWorker rank=0 pid=2951451)[0;0m INFO 06-19 05:10:15 [loader.py:458] Loading weights took 6.55 seconds
[1;36m(VllmWorker rank=1 pid=2951452)[0;0m INFO 06-19 05:10:15 [gpu_model_runner.py:1347] Model loading took 6.4664 GiB and 6.676886 seconds
[1;36m(VllmWorker rank=0 pid=2951451)[0;0m INFO 06-19 05:10:15 [gpu_model_runner.py:1347] Model loading took 6.4664 GiB and 6.680969 seconds
[1;36m(VllmWorker rank=0 pid=2951451)[0;0m INFO 06-19 05:10:20 [backends.py:420] Using cache directory: /home/algo/.cache/vllm/torch_compile_cache/513ebe6e03/rank_0_0 for vLLM's torch.compile
[1;36m(VllmWorker rank=0 pid=2951451)[0;0m INFO 06-19 05:10:20 [backends.py:430] Dynamo bytecode transform time: 4.49 s
[1;36m(VllmWorker rank=1 pid=2951452)[0;0m INFO 06-19 05:10:20 [backends.py:420] Using cache directory: /home/algo/.cache/vllm/torch_compile_cache/513ebe6e03/rank_1_0 for vLLM's torch.compile
[1;36m(VllmWorker rank=1 pid=2951452)[0;0m INFO 06-19 05:10:20 [backends.py:430] Dynamo bytecode transform time: 4.49 s
[1;36m(VllmWorker rank=1 pid=2951452)[0;0m INFO 06-19 05:10:23 [backends.py:118] Directly load the compiled graph(s) for shape None from the cache, took 3.035 s
[1;36m(VllmWorker rank=0 pid=2951451)[0;0m INFO 06-19 05:10:23 [backends.py:118] Directly load the compiled graph(s) for shape None from the cache, took 3.040 s
[1;36m(VllmWorker rank=0 pid=2951451)[0;0m INFO 06-19 05:10:24 [monitor.py:33] torch.compile takes 4.49 s in total
[1;36m(VllmWorker rank=1 pid=2951452)[0;0m INFO 06-19 05:10:24 [monitor.py:33] torch.compile takes 4.49 s in total
INFO 06-19 05:10:25 [kv_cache_utils.py:634] GPU KV cache size: 17,968 tokens
INFO 06-19 05:10:25 [kv_cache_utils.py:637] Maximum concurrency for 4,096 tokens per request: 4.39x
INFO 06-19 05:10:25 [kv_cache_utils.py:634] GPU KV cache size: 26,672 tokens
INFO 06-19 05:10:25 [kv_cache_utils.py:637] Maximum concurrency for 4,096 tokens per request: 6.51x
[1;36m(VllmWorker rank=1 pid=2951452)[0;0m INFO 06-19 05:10:47 [gpu_model_runner.py:1686] Graph capturing finished in 21 secs, took 2.14 GiB
[1;36m(VllmWorker rank=0 pid=2951451)[0;0m INFO 06-19 05:10:47 [gpu_model_runner.py:1686] Graph capturing finished in 21 secs, took 2.14 GiB
INFO 06-19 05:10:47 [core.py:159] init engine (profile, create kv cache, warmup model) took 31.49 seconds
INFO 06-19 05:10:47 [core_client.py:439] Core engine process 0 ready.
WARNING 06-19 05:10:47 [config.py:1239] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
INFO 06-19 05:10:47 [serving_chat.py:118] Using default chat sampling params from model: {'temperature': 0.7, 'top_p': 0.95}
INFO 06-19 05:10:47 [serving_completion.py:61] Using default completion sampling params from model: {'temperature': 0.7, 'top_p': 0.95}
INFO 06-19 05:10:47 [api_server.py:1090] Starting vLLM API server on http://0.0.0.0:8011
INFO 06-19 05:10:47 [launcher.py:28] Available routes are:
INFO 06-19 05:10:47 [launcher.py:36] Route: /openapi.json, Methods: GET, HEAD
INFO 06-19 05:10:47 [launcher.py:36] Route: /docs, Methods: GET, HEAD
INFO 06-19 05:10:47 [launcher.py:36] Route: /docs/oauth2-redirect, Methods: GET, HEAD
INFO 06-19 05:10:47 [launcher.py:36] Route: /redoc, Methods: GET, HEAD
INFO 06-19 05:10:47 [launcher.py:36] Route: /health, Methods: GET
INFO 06-19 05:10:47 [launcher.py:36] Route: /load, Methods: GET
INFO 06-19 05:10:47 [launcher.py:36] Route: /ping, Methods: GET, POST
INFO 06-19 05:10:47 [launcher.py:36] Route: /tokenize, Methods: POST
INFO 06-19 05:10:47 [launcher.py:36] Route: /detokenize, Methods: POST
INFO 06-19 05:10:47 [launcher.py:36] Route: /v1/models, Methods: GET
INFO 06-19 05:10:47 [launcher.py:36] Route: /version, Methods: GET
INFO 06-19 05:10:47 [launcher.py:36] Route: /v1/chat/completions, Methods: POST
INFO 06-19 05:10:47 [launcher.py:36] Route: /v1/completions, Methods: POST
INFO 06-19 05:10:47 [launcher.py:36] Route: /v1/embeddings, Methods: POST
INFO 06-19 05:10:47 [launcher.py:36] Route: /pooling, Methods: POST
INFO 06-19 05:10:47 [launcher.py:36] Route: /score, Methods: POST
INFO 06-19 05:10:47 [launcher.py:36] Route: /v1/score, Methods: POST
INFO 06-19 05:10:47 [launcher.py:36] Route: /v1/audio/transcriptions, Methods: POST
INFO 06-19 05:10:47 [launcher.py:36] Route: /rerank, Methods: POST
INFO 06-19 05:10:47 [launcher.py:36] Route: /v1/rerank, Methods: POST
INFO 06-19 05:10:47 [launcher.py:36] Route: /v2/rerank, Methods: POST
INFO 06-19 05:10:47 [launcher.py:36] Route: /invocations, Methods: POST
INFO 06-19 05:10:47 [launcher.py:36] Route: /metrics, Methods: GET
INFO:     Started server process [2951169]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO 06-19 05:10:50 [chat_utils.py:397] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
INFO 06-19 05:10:50 [logger.py:39] Received request chatcmpl-fb20a03327154c87ae09d0b23cc6b6af: prompt: '<｜begin▁of▁sentence｜>User: 蛋炒饭怎么做\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=4084, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 05:10:50 [async_llm.py:252] Added request chatcmpl-fb20a03327154c87ae09d0b23cc6b6af.
INFO:     192.168.7.58:55509 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-19 05:10:57 [loggers.py:111] Engine 000: Avg prompt throughput: 0.2 tokens/s, Avg generation throughput: 5.7 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 0.0%
INFO 06-19 05:11:07 [loggers.py:111] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 0.0%
INFO 06-19 05:40:07 [logger.py:39] Received request chatcmpl-a8703c37160f4ddfbe08b82e04680bc6: prompt: '<｜begin▁of▁sentence｜>你是一个专业的问题重写模块，专门用于多轮对话场景下的指代补全与语义还原任务。\n请严格按照以下规则执行：\n\n【任务目标】\n1. 你的目标是准确解析用户真实意图，从历史中发掘指代和意图，使得这个独立问题尽量完整，尽可能包含所有信息关键词,特别是要补充关键的动机，指代和场景（润色后的问题至少涉及：用户面对的前置情况，什么限制，什么疑问等）。\n2. 仅当历史信息能够提供明确上下文时才进行补全，否则保持当前问题不变。\n3. 对于明显不需要补全的问题如：吹水："呵呵"，命令："关机"，无关内容："人生的意义"等，返回原句\n4. 不进行任何无关发挥、扩写、润色、修辞性描述、解释、总结、感情色彩。\n5. 不编造任何不存在的假设背景或新信息。\n6. 输出格式必须严格遵循：仅输出最终重写结果文本，不包含任何前缀、提示词、说明性文字或换行符。重写的疑问句以："我想知道"开头，非疑问句你自行适配\n7. 当无需重写时，直接输出原问题。\n\n【重写示例】\n历史对话：\n用户：小红鞋什么时候有货？\n系统：请问您指的是哪款小红鞋？\n用户：就是上次缺货那款\n当前问题：就是上次缺货那款\n重写结果：我想知道上次缺货的那款小红鞋什么时候有货？\n\n历史对话：\n用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：那我们还有其他计划，能投放吗？\n当前问题：那我们还有其他计划，能投放吗？\n重写结果：我想知道当账号处于限流状态时，是否可以继续开启新的广告投放计划？\n\n历史对话：\n用户：你是谁？\n当前问题：你是谁？\n重写结果：你是谁？\n\n历史对话：\n用户：开心\n当前问题：开心\n重写结果：开心\n\n历史对话：\n用户：我要买它\n系统：请问您指的是哪款商品？\n用户：之前推荐的那双\n当前问题：之前推荐的那双\n重写结果：我想要买你之前推荐的那双鞋。\n\n注意：严格按照以上风格工作，禁止任何额外输出。\n\nUser: 用户：五张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/589a675592ab4d878cdd56b8f9dcf43a..png\n用户：五张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/a2dbe01a4d6848848935dca6e5f91911..png\n用户：五张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/2d54d547c2384735a90189966f0fcd8f..png\n用户：五张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/ffdf42977895448d8ffaa78d5cbd271b..png\n用户：五张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/c3f19ce42ba9490185fd3a622c51a918..png\n用户：五张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/589a675592ab4d878cdd56b8f9dcf43a..png\n用户：五张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/a2dbe01a4d6848848935dca6e5f91911..png\n用户：五张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/2d54d547c2384735a90189966f0fcd8f..png\n用户：五张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/ffdf42977895448d8ffaa78d5cbd271b..png\n用户：五张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/c3f19ce42ba9490185fd3a622c51a918..png\n用户：报错\n\n当前问题：报错\n请你根据任务规则输出重写结果。\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=0.5, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 05:40:07 [async_llm.py:252] Added request chatcmpl-a8703c37160f4ddfbe08b82e04680bc6.
INFO 06-19 05:40:07 [loggers.py:111] Engine 000: Avg prompt throughput: 121.1 tokens/s, Avg generation throughput: 1.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 6.9%, Prefix cache hit rate: 0.0%
INFO:     127.0.0.1:45988 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-19 05:40:17 [loggers.py:111] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.2 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 0.0%
INFO 06-19 05:40:27 [loggers.py:111] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 0.0%
INFO 06-19 06:02:55 [logger.py:39] Received request chatcmpl-a0e8b3eb368344298b73c7e499a3f04d: prompt: '<｜begin▁of▁sentence｜>你是一个专业的问题重写模块，专门用于多轮对话场景下的指代补全与语义还原任务。\n请严格按照以下规则执行：\n\n【任务目标】\n1. 你的目标是准确解析用户真实意图，从历史中发掘指代和意图，使得这个独立问题尽量完整，尽可能包含所有信息关键词,特别是要补充关键的动机，指代和场景（润色后的问题至少涉及：用户面对的前置情况，什么限制，什么疑问等）。\n2. 仅当历史信息能够提供明确上下文时才进行补全，否则保持当前问题不变。\n3. 对于明显不需要补全的问题如：吹水："呵呵"，命令："关机"，无关内容："人生的意义"等，返回原句\n4. 不进行任何无关发挥、扩写、润色、修辞性描述、解释、总结、感情色彩。\n5. 不编造任何不存在的假设背景或新信息。\n6. 输出格式必须严格遵循：仅输出最终重写结果文本，不包含任何前缀、提示词、说明性文字或换行符。重写的疑问句以："我想知道"开头，非疑问句你自行适配\n7. 当无需重写时，直接输出原问题。\n\n【重写示例】\n历史对话：\n用户：小红鞋什么时候有货？\n系统：请问您指的是哪款小红鞋？\n用户：就是上次缺货那款\n当前问题：就是上次缺货那款\n重写结果：我想知道上次缺货的那款小红鞋什么时候有货？\n\n历史对话：\n用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：那我们还有其他计划，能投放吗？\n当前问题：那我们还有其他计划，能投放吗？\n重写结果：我想知道当账号处于限流状态时，是否可以继续开启新的广告投放计划？\n\n历史对话：\n用户：你是谁？\n当前问题：你是谁？\n重写结果：你是谁？\n\n历史对话：\n用户：开心\n当前问题：开心\n重写结果：开心\n\n历史对话：\n用户：我要买它\n系统：请问您指的是哪款商品？\n用户：之前推荐的那双\n当前问题：之前推荐的那双\n重写结果：我想要买你之前推荐的那双鞋。\n\n注意：严格按照以上风格工作，禁止任何额外输出。\n\nUser: 用户：五张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/589a675592ab4d878cdd56b8f9dcf43a..png\n用户：五张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/a2dbe01a4d6848848935dca6e5f91911..png\n用户：五张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/2d54d547c2384735a90189966f0fcd8f..png\n用户：五张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/ffdf42977895448d8ffaa78d5cbd271b..png\n用户：五张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/c3f19ce42ba9490185fd3a622c51a918..png\n用户：五张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/589a675592ab4d878cdd56b8f9dcf43a..png\n用户：五张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/a2dbe01a4d6848848935dca6e5f91911..png\n用户：五张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/2d54d547c2384735a90189966f0fcd8f..png\n用户：五张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/ffdf42977895448d8ffaa78d5cbd271b..png\n用户：五张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/c3f19ce42ba9490185fd3a622c51a918..png\n用户：五张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/589a675592ab4d878cdd56b8f9dcf43a..png\n用户：五张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/a2dbe01a4d6848848935dca6e5f91911..png\n用户：五张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/2d54d547c2384735a90189966f0fcd8f..png\n用户：五张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/ffdf42977895448d8ffaa78d5cbd271b..png\n用户：五张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/c3f19ce42ba9490185fd3a622c51a918..png\n用户：报错\n系统：哎呀，看您一直在要图片链接，是不是遇到啥问题啦？具体是图片打不开还是内容不对呀？跟我说说呗～\n用户：你好\n系统：嗨～刚刚看到您一直在发图片链接，是不是系统遇到什么问题啦？需要我帮您看看具体是什么报错嘛？\n用户：没有遇见问题，就是咨询一下\n\n当前问题：没有遇见问题，就是咨询一下\n请你根据任务规则输出重写结果。\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=0.5, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 06:02:55 [async_llm.py:252] Added request chatcmpl-a0e8b3eb368344298b73c7e499a3f04d.
INFO:     127.0.0.1:46600 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-19 06:02:57 [loggers.py:111] Engine 000: Avg prompt throughput: 161.2 tokens/s, Avg generation throughput: 4.6 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 42.3%
INFO 06-19 06:03:07 [loggers.py:111] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 42.3%
INFO 06-19 21:09:51 [logger.py:39] Received request chatcmpl-fb6c01e25c324f12a16c089739fa26cc: prompt: '<｜begin▁of▁sentence｜>你是一个专业的问题重写模块，专门用于多轮对话场景下的指代补全与语义还原任务。\n请严格按照以下规则执行：\n\n【任务目标】\n1. 你的目标是准确解析用户真实意图，从历史中发掘指代和意图，使得这个独立问题尽量完整，尽可能包含所有信息关键词,特别是要补充关键的动机，指代和场景（润色后的问题至少涉及：用户面对的前置情况，什么限制，什么疑问等）。\n2. 仅当历史信息能够提供明确上下文时才进行补全，否则保持当前问题不变。\n3. 对于明显不需要补全的问题如：吹水："呵呵"，命令："关机"，无关内容："人生的意义"等，返回原句\n4. 不进行任何无关发挥、扩写、润色、修辞性描述、解释、总结、感情色彩。\n5. 不编造任何不存在的假设背景或新信息。\n6. 输出格式必须严格遵循：仅输出最终重写结果文本，不包含任何前缀、提示词、说明性文字或换行符。重写的疑问句以："我想知道"开头，非疑问句你自行适配\n7. 当无需重写时，直接输出原问题。\n\n【重写示例】\n历史对话：\n用户：小红鞋什么时候有货？\n系统：请问您指的是哪款小红鞋？\n用户：就是上次缺货那款\n当前问题：就是上次缺货那款\n重写结果：我想知道上次缺货的那款小红鞋什么时候有货？\n\n历史对话：\n用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：那我们还有其他计划，能投放吗？\n当前问题：那我们还有其他计划，能投放吗？\n重写结果：我想知道当账号处于限流状态时，是否可以继续开启新的广告投放计划？\n\n历史对话：\n用户：你是谁？\n当前问题：你是谁？\n重写结果：你是谁？\n\n历史对话：\n用户：开心\n当前问题：开心\n重写结果：开心\n\n历史对话：\n用户：我要买它\n系统：请问您指的是哪款商品？\n用户：之前推荐的那双\n当前问题：之前推荐的那双\n重写结果：我想要买你之前推荐的那双鞋。\n\n注意：严格按照以上风格工作，禁止任何额外输出。\n\nUser: 用户：五张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/589a675592ab4d878cdd56b8f9dcf43a..png\n用户：五张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/a2dbe01a4d6848848935dca6e5f91911..png\n用户：五张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/2d54d547c2384735a90189966f0fcd8f..png\n用户：五张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/ffdf42977895448d8ffaa78d5cbd271b..png\n用户：五张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/c3f19ce42ba9490185fd3a622c51a918..png\n用户：五张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/589a675592ab4d878cdd56b8f9dcf43a..png\n用户：五张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/a2dbe01a4d6848848935dca6e5f91911..png\n用户：五张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/2d54d547c2384735a90189966f0fcd8f..png\n用户：五张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/ffdf42977895448d8ffaa78d5cbd271b..png\n用户：五张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/c3f19ce42ba9490185fd3a622c51a918..png\n用户：五张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/589a675592ab4d878cdd56b8f9dcf43a..png\n用户：五张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/a2dbe01a4d6848848935dca6e5f91911..png\n用户：五张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/2d54d547c2384735a90189966f0fcd8f..png\n用户：五张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/ffdf42977895448d8ffaa78d5cbd271b..png\n用户：五张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/c3f19ce42ba9490185fd3a622c51a918..png\n用户：报错\n系统：哎呀，看您一直在要图片链接，是不是遇到啥问题啦？具体是图片打不开还是内容不对呀？跟我说说呗～\n用户：你好\n系统：嗨～刚刚看到您一直在发图片链接，是不是系统遇到什么问题啦？需要我帮您看看具体是什么报错嘛？\n用户：没有遇见问题，就是咨询一下\n系统：哎呀，看您一直在发图片链接，是不是遇到啥问题啦？具体是图片打不开还是内容不对呀？跟我说说呗～\n用户：五张图片\n\n当前问题：五张图片\n请你根据任务规则输出重写结果。\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=0.5, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 21:09:51 [async_llm.py:252] Added request chatcmpl-fb6c01e25c324f12a16c089739fa26cc.
INFO:     127.0.0.1:41776 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-19 21:09:58 [loggers.py:111] Engine 000: Avg prompt throughput: 164.8 tokens/s, Avg generation throughput: 1.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 62.5%
INFO 06-19 21:10:08 [loggers.py:111] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 62.5%
INFO 06-19 21:10:50 [logger.py:39] Received request chatcmpl-58f815d42ead4eeab4fb3dbdaae36288: prompt: '<｜begin▁of▁sentence｜>你是一个专业的问题重写模块，专门用于多轮对话场景下的指代补全与语义还原任务。\n请严格按照以下规则执行：\n\n【任务目标】\n1. 你的目标是准确解析用户真实意图，从历史中发掘指代和意图，使得这个独立问题尽量完整，尽可能包含所有信息关键词,特别是要补充关键的动机，指代和场景（润色后的问题至少涉及：用户面对的前置情况，什么限制，什么疑问等）。\n2. 仅当历史信息能够提供明确上下文时才进行补全，否则保持当前问题不变。\n3. 对于明显不需要补全的问题如：吹水："呵呵"，命令："关机"，无关内容："人生的意义"等，返回原句\n4. 不进行任何无关发挥、扩写、润色、修辞性描述、解释、总结、感情色彩。\n5. 不编造任何不存在的假设背景或新信息。\n6. 输出格式必须严格遵循：仅输出最终重写结果文本，不包含任何前缀、提示词、说明性文字或换行符。重写的疑问句以："我想知道"开头，非疑问句你自行适配\n7. 当无需重写时，直接输出原问题。\n\n【重写示例】\n历史对话：\n用户：小红鞋什么时候有货？\n系统：请问您指的是哪款小红鞋？\n用户：就是上次缺货那款\n当前问题：就是上次缺货那款\n重写结果：我想知道上次缺货的那款小红鞋什么时候有货？\n\n历史对话：\n用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：那我们还有其他计划，能投放吗？\n当前问题：那我们还有其他计划，能投放吗？\n重写结果：我想知道当账号处于限流状态时，是否可以继续开启新的广告投放计划？\n\n历史对话：\n用户：你是谁？\n当前问题：你是谁？\n重写结果：你是谁？\n\n历史对话：\n用户：开心\n当前问题：开心\n重写结果：开心\n\n历史对话：\n用户：我要买它\n系统：请问您指的是哪款商品？\n用户：之前推荐的那双\n当前问题：之前推荐的那双\n重写结果：我想要买你之前推荐的那双鞋。\n\n注意：严格按照以上风格工作，禁止任何额外输出。\n\nUser: 用户：五张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/589a675592ab4d878cdd56b8f9dcf43a..png\n用户：五张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/a2dbe01a4d6848848935dca6e5f91911..png\n用户：五张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/2d54d547c2384735a90189966f0fcd8f..png\n用户：五张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/ffdf42977895448d8ffaa78d5cbd271b..png\n用户：五张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/c3f19ce42ba9490185fd3a622c51a918..png\n用户：五张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/589a675592ab4d878cdd56b8f9dcf43a..png\n用户：五张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/a2dbe01a4d6848848935dca6e5f91911..png\n用户：五张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/2d54d547c2384735a90189966f0fcd8f..png\n用户：五张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/ffdf42977895448d8ffaa78d5cbd271b..png\n用户：五张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/c3f19ce42ba9490185fd3a622c51a918..png\n用户：五张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/589a675592ab4d878cdd56b8f9dcf43a..png\n用户：五张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/a2dbe01a4d6848848935dca6e5f91911..png\n用户：五张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/2d54d547c2384735a90189966f0fcd8f..png\n用户：五张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/ffdf42977895448d8ffaa78d5cbd271b..png\n用户：五张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/c3f19ce42ba9490185fd3a622c51a918..png\n用户：报错\n系统：哎呀，看您一直在要图片链接，是不是遇到啥问题啦？具体是图片打不开还是内容不对呀？跟我说说呗～\n用户：你好\n系统：嗨～刚刚看到您一直在发图片链接，是不是系统遇到什么问题啦？需要我帮您看看具体是什么报错嘛？\n用户：没有遇见问题，就是咨询一下\n系统：哎呀，看您一直在发图片链接，是不是遇到啥问题啦？具体是图片打不开还是内容不对呀？跟我说说呗～\n用户：五张图片\n系统：哦哦～原来是想问图片链接呀！不过您刚才发的那些链接好像都是重复的同一组图片呢🤔 您是需要特定类型的图片素材吗？还是想了解怎么在抖音广告里上传图片呀？\n用户：一张图片\n\n当前问题：一张图片\n请你根据任务规则输出重写结果。\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=0.5, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 21:10:50 [async_llm.py:252] Added request chatcmpl-58f815d42ead4eeab4fb3dbdaae36288.
INFO:     127.0.0.1:43340 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-19 21:10:58 [loggers.py:111] Engine 000: Avg prompt throughput: 170.4 tokens/s, Avg generation throughput: 0.8 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 71.5%
INFO 06-19 21:11:08 [loggers.py:111] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 71.5%
INFO 06-19 21:12:30 [logger.py:39] Received request chatcmpl-fb8188cb04a546da90888a59ed8511fb: prompt: '<｜begin▁of▁sentence｜>你是一个专业的问题重写模块，专门用于多轮对话场景下的指代补全与语义还原任务。\n请严格按照以下规则执行：\n\n【任务目标】\n1. 你的目标是准确解析用户真实意图，从历史中发掘指代和意图，使得这个独立问题尽量完整，尽可能包含所有信息关键词,特别是要补充关键的动机，指代和场景（润色后的问题至少涉及：用户面对的前置情况，什么限制，什么疑问等）。\n2. 仅当历史信息能够提供明确上下文时才进行补全，否则保持当前问题不变。\n3. 对于明显不需要补全的问题如：吹水："呵呵"，命令："关机"，无关内容："人生的意义"等，返回原句\n4. 不进行任何无关发挥、扩写、润色、修辞性描述、解释、总结、感情色彩。\n5. 不编造任何不存在的假设背景或新信息。\n6. 输出格式必须严格遵循：仅输出最终重写结果文本，不包含任何前缀、提示词、说明性文字或换行符。重写的疑问句以："我想知道"开头，非疑问句你自行适配\n7. 当无需重写时，直接输出原问题。\n\n【重写示例】\n历史对话：\n用户：小红鞋什么时候有货？\n系统：请问您指的是哪款小红鞋？\n用户：就是上次缺货那款\n当前问题：就是上次缺货那款\n重写结果：我想知道上次缺货的那款小红鞋什么时候有货？\n\n历史对话：\n用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：那我们还有其他计划，能投放吗？\n当前问题：那我们还有其他计划，能投放吗？\n重写结果：我想知道当账号处于限流状态时，是否可以继续开启新的广告投放计划？\n\n历史对话：\n用户：你是谁？\n当前问题：你是谁？\n重写结果：你是谁？\n\n历史对话：\n用户：开心\n当前问题：开心\n重写结果：开心\n\n历史对话：\n用户：我要买它\n系统：请问您指的是哪款商品？\n用户：之前推荐的那双\n当前问题：之前推荐的那双\n重写结果：我想要买你之前推荐的那双鞋。\n\n注意：严格按照以上风格工作，禁止任何额外输出。\n\nUser: 用户：哈哈一张图片\n系统：哎呀～您发的是图片呀，我这边暂时看不到具体内容呢😅 如果是想咨询抖音广告相关的问题，可以文字描述下需求哈～\n用户：五张图片\n\n当前问题：五张图片\n请你根据任务规则输出重写结果。\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=0.5, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 21:12:30 [async_llm.py:252] Added request chatcmpl-fb8188cb04a546da90888a59ed8511fb.
INFO:     127.0.0.1:37620 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-19 21:12:38 [loggers.py:111] Engine 000: Avg prompt throughput: 62.6 tokens/s, Avg generation throughput: 1.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 73.0%
INFO 06-19 21:12:45 [logger.py:39] Received request chatcmpl-189574f17c644d9ea6d7dac52bca2abc: prompt: '<｜begin▁of▁sentence｜>你是一个专业的问题重写模块，专门用于多轮对话场景下的指代补全与语义还原任务。\n请严格按照以下规则执行：\n\n【任务目标】\n1. 你的目标是准确解析用户真实意图，从历史中发掘指代和意图，使得这个独立问题尽量完整，尽可能包含所有信息关键词,特别是要补充关键的动机，指代和场景（润色后的问题至少涉及：用户面对的前置情况，什么限制，什么疑问等）。\n2. 仅当历史信息能够提供明确上下文时才进行补全，否则保持当前问题不变。\n3. 对于明显不需要补全的问题如：吹水："呵呵"，命令："关机"，无关内容："人生的意义"等，返回原句\n4. 不进行任何无关发挥、扩写、润色、修辞性描述、解释、总结、感情色彩。\n5. 不编造任何不存在的假设背景或新信息。\n6. 输出格式必须严格遵循：仅输出最终重写结果文本，不包含任何前缀、提示词、说明性文字或换行符。重写的疑问句以："我想知道"开头，非疑问句你自行适配\n7. 当无需重写时，直接输出原问题。\n\n【重写示例】\n历史对话：\n用户：小红鞋什么时候有货？\n系统：请问您指的是哪款小红鞋？\n用户：就是上次缺货那款\n当前问题：就是上次缺货那款\n重写结果：我想知道上次缺货的那款小红鞋什么时候有货？\n\n历史对话：\n用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：那我们还有其他计划，能投放吗？\n当前问题：那我们还有其他计划，能投放吗？\n重写结果：我想知道当账号处于限流状态时，是否可以继续开启新的广告投放计划？\n\n历史对话：\n用户：你是谁？\n当前问题：你是谁？\n重写结果：你是谁？\n\n历史对话：\n用户：开心\n当前问题：开心\n重写结果：开心\n\n历史对话：\n用户：我要买它\n系统：请问您指的是哪款商品？\n用户：之前推荐的那双\n当前问题：之前推荐的那双\n重写结果：我想要买你之前推荐的那双鞋。\n\n注意：严格按照以上风格工作，禁止任何额外输出。\n\nUser: 用户：五张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/589a675592ab4d878cdd56b8f9dcf43a..png\n用户：五张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/a2dbe01a4d6848848935dca6e5f91911..png\n用户：五张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/2d54d547c2384735a90189966f0fcd8f..png\n用户：五张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/ffdf42977895448d8ffaa78d5cbd271b..png\n用户：五张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/c3f19ce42ba9490185fd3a622c51a918..png\n用户：五张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/589a675592ab4d878cdd56b8f9dcf43a..png\n用户：五张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/a2dbe01a4d6848848935dca6e5f91911..png\n用户：五张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/2d54d547c2384735a90189966f0fcd8f..png\n用户：五张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/ffdf42977895448d8ffaa78d5cbd271b..png\n用户：五张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/c3f19ce42ba9490185fd3a622c51a918..png\n用户：五张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/589a675592ab4d878cdd56b8f9dcf43a..png\n用户：五张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/a2dbe01a4d6848848935dca6e5f91911..png\n用户：五张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/2d54d547c2384735a90189966f0fcd8f..png\n用户：五张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/ffdf42977895448d8ffaa78d5cbd271b..png\n用户：五张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/c3f19ce42ba9490185fd3a622c51a918..png\n用户：五张图片\n\n当前问题：五张图片\n请你根据任务规则输出重写结果。\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=0.5, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 21:12:45 [async_llm.py:252] Added request chatcmpl-189574f17c644d9ea6d7dac52bca2abc.
INFO:     127.0.0.1:43982 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-19 21:12:48 [loggers.py:111] Engine 000: Avg prompt throughput: 152.9 tokens/s, Avg generation throughput: 0.9 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 77.8%
INFO 06-19 21:12:58 [loggers.py:111] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 77.8%
INFO 06-19 21:13:05 [logger.py:39] Received request chatcmpl-a9f02b7439974035b95232c568caaabe: prompt: '<｜begin▁of▁sentence｜>你是一个专业的问题重写模块，专门用于多轮对话场景下的指代补全与语义还原任务。\n请严格按照以下规则执行：\n\n【任务目标】\n1. 你的目标是准确解析用户真实意图，从历史中发掘指代和意图，使得这个独立问题尽量完整，尽可能包含所有信息关键词,特别是要补充关键的动机，指代和场景（润色后的问题至少涉及：用户面对的前置情况，什么限制，什么疑问等）。\n2. 仅当历史信息能够提供明确上下文时才进行补全，否则保持当前问题不变。\n3. 对于明显不需要补全的问题如：吹水："呵呵"，命令："关机"，无关内容："人生的意义"等，返回原句\n4. 不进行任何无关发挥、扩写、润色、修辞性描述、解释、总结、感情色彩。\n5. 不编造任何不存在的假设背景或新信息。\n6. 输出格式必须严格遵循：仅输出最终重写结果文本，不包含任何前缀、提示词、说明性文字或换行符。重写的疑问句以："我想知道"开头，非疑问句你自行适配\n7. 当无需重写时，直接输出原问题。\n\n【重写示例】\n历史对话：\n用户：小红鞋什么时候有货？\n系统：请问您指的是哪款小红鞋？\n用户：就是上次缺货那款\n当前问题：就是上次缺货那款\n重写结果：我想知道上次缺货的那款小红鞋什么时候有货？\n\n历史对话：\n用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：那我们还有其他计划，能投放吗？\n当前问题：那我们还有其他计划，能投放吗？\n重写结果：我想知道当账号处于限流状态时，是否可以继续开启新的广告投放计划？\n\n历史对话：\n用户：你是谁？\n当前问题：你是谁？\n重写结果：你是谁？\n\n历史对话：\n用户：开心\n当前问题：开心\n重写结果：开心\n\n历史对话：\n用户：我要买它\n系统：请问您指的是哪款商品？\n用户：之前推荐的那双\n当前问题：之前推荐的那双\n重写结果：我想要买你之前推荐的那双鞋。\n\n注意：严格按照以上风格工作，禁止任何额外输出。\n\nUser: 用户：哈哈一张图片\n系统：哎呀～您发的是图片呀，我这边暂时看不到具体内容呢😅 如果是想咨询抖音广告相关的问题，可以文字描述下需求哈～\n用户：五张图片\n系统：哎呀～您是想问图片相关的广告投放吗？比如用图片素材做信息流广告之类的？可以具体说说需求嘛～\n用户：图片\n\n当前问题：图片\n请你根据任务规则输出重写结果。\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=0.5, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 21:13:05 [async_llm.py:252] Added request chatcmpl-a9f02b7439974035b95232c568caaabe.
INFO:     127.0.0.1:58842 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-19 21:13:08 [loggers.py:111] Engine 000: Avg prompt throughput: 66.1 tokens/s, Avg generation throughput: 1.3 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 78.7%
INFO 06-19 21:13:18 [loggers.py:111] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 78.7%
INFO 06-19 21:13:28 [logger.py:39] Received request chatcmpl-3f49d3e2a3b64084b266dbfae3308577: prompt: '<｜begin▁of▁sentence｜>你是一个专业的问题重写模块，专门用于多轮对话场景下的指代补全与语义还原任务。\n请严格按照以下规则执行：\n\n【任务目标】\n1. 你的目标是准确解析用户真实意图，从历史中发掘指代和意图，使得这个独立问题尽量完整，尽可能包含所有信息关键词,特别是要补充关键的动机，指代和场景（润色后的问题至少涉及：用户面对的前置情况，什么限制，什么疑问等）。\n2. 仅当历史信息能够提供明确上下文时才进行补全，否则保持当前问题不变。\n3. 对于明显不需要补全的问题如：吹水："呵呵"，命令："关机"，无关内容："人生的意义"等，返回原句\n4. 不进行任何无关发挥、扩写、润色、修辞性描述、解释、总结、感情色彩。\n5. 不编造任何不存在的假设背景或新信息。\n6. 输出格式必须严格遵循：仅输出最终重写结果文本，不包含任何前缀、提示词、说明性文字或换行符。重写的疑问句以："我想知道"开头，非疑问句你自行适配\n7. 当无需重写时，直接输出原问题。\n\n【重写示例】\n历史对话：\n用户：小红鞋什么时候有货？\n系统：请问您指的是哪款小红鞋？\n用户：就是上次缺货那款\n当前问题：就是上次缺货那款\n重写结果：我想知道上次缺货的那款小红鞋什么时候有货？\n\n历史对话：\n用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：那我们还有其他计划，能投放吗？\n当前问题：那我们还有其他计划，能投放吗？\n重写结果：我想知道当账号处于限流状态时，是否可以继续开启新的广告投放计划？\n\n历史对话：\n用户：你是谁？\n当前问题：你是谁？\n重写结果：你是谁？\n\n历史对话：\n用户：开心\n当前问题：开心\n重写结果：开心\n\n历史对话：\n用户：我要买它\n系统：请问您指的是哪款商品？\n用户：之前推荐的那双\n当前问题：之前推荐的那双\n重写结果：我想要买你之前推荐的那双鞋。\n\n注意：严格按照以上风格工作，禁止任何额外输出。\n\nUser: 用户：125487952364\n系统：亲，您是想查询账户余额嘛？麻烦提供下业务号哈～\n用户：五张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/589a675592ab4d878cdd56b8f9dcf43a..png\n用户：五张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/a2dbe01a4d6848848935dca6e5f91911..png\n用户：五张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/2d54d547c2384735a90189966f0fcd8f..png\n用户：五张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/ffdf42977895448d8ffaa78d5cbd271b..png\n用户：五张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/c3f19ce42ba9490185fd3a622c51a918..png\n用户：你好帮我査一下下单了招募任务 达人交付完成后 怎么取消该计划\n系统：亲亲～招募任务达人交付完成后，计划会自动结束的哦～不需要手动取消呢😊\n用户：你好\n系统：亲～您刚发的图片我这边看不到具体内容呢😅 是要查账户余额还是其他问题呀？\n用户：你好帮我査一下下单了招募任务 达人交付完成后 怎么取消该计划\n系统：亲～招募任务达人交付完成后计划会自动结束的，不需要手动取消呢～如果达人已经完成交付，系统会自动关闭计划的哈😊\n用户：五张图片\n\n当前问题：五张图片\n请你根据任务规则输出重写结果。\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=0.5, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 21:13:28 [async_llm.py:252] Added request chatcmpl-3f49d3e2a3b64084b266dbfae3308577.
INFO:     127.0.0.1:33406 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-19 21:13:28 [loggers.py:111] Engine 000: Avg prompt throughput: 107.0 tokens/s, Avg generation throughput: 2.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 75.8%
INFO 06-19 21:13:38 [loggers.py:111] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 75.8%
INFO 06-19 21:14:00 [logger.py:39] Received request chatcmpl-7f80d0e1b7bc4b0c91261d64599f44b2: prompt: '<｜begin▁of▁sentence｜>你是一个专业的问题重写模块，专门用于多轮对话场景下的指代补全与语义还原任务。\n请严格按照以下规则执行：\n\n【任务目标】\n1. 你的目标是准确解析用户真实意图，从历史中发掘指代和意图，使得这个独立问题尽量完整，尽可能包含所有信息关键词,特别是要补充关键的动机，指代和场景（润色后的问题至少涉及：用户面对的前置情况，什么限制，什么疑问等）。\n2. 仅当历史信息能够提供明确上下文时才进行补全，否则保持当前问题不变。\n3. 对于明显不需要补全的问题如：吹水："呵呵"，命令："关机"，无关内容："人生的意义"等，返回原句\n4. 不进行任何无关发挥、扩写、润色、修辞性描述、解释、总结、感情色彩。\n5. 不编造任何不存在的假设背景或新信息。\n6. 输出格式必须严格遵循：仅输出最终重写结果文本，不包含任何前缀、提示词、说明性文字或换行符。重写的疑问句以："我想知道"开头，非疑问句你自行适配\n7. 当无需重写时，直接输出原问题。\n\n【重写示例】\n历史对话：\n用户：小红鞋什么时候有货？\n系统：请问您指的是哪款小红鞋？\n用户：就是上次缺货那款\n当前问题：就是上次缺货那款\n重写结果：我想知道上次缺货的那款小红鞋什么时候有货？\n\n历史对话：\n用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：那我们还有其他计划，能投放吗？\n当前问题：那我们还有其他计划，能投放吗？\n重写结果：我想知道当账号处于限流状态时，是否可以继续开启新的广告投放计划？\n\n历史对话：\n用户：你是谁？\n当前问题：你是谁？\n重写结果：你是谁？\n\n历史对话：\n用户：开心\n当前问题：开心\n重写结果：开心\n\n历史对话：\n用户：我要买它\n系统：请问您指的是哪款商品？\n用户：之前推荐的那双\n当前问题：之前推荐的那双\n重写结果：我想要买你之前推荐的那双鞋。\n\n注意：严格按照以上风格工作，禁止任何额外输出。\n\nUser: 用户：125487952364\n系统：亲，您是想查询账户余额嘛？麻烦提供下业务号哈～\n用户：五张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/589a675592ab4d878cdd56b8f9dcf43a..png\n用户：五张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/a2dbe01a4d6848848935dca6e5f91911..png\n用户：五张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/2d54d547c2384735a90189966f0fcd8f..png\n用户：五张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/ffdf42977895448d8ffaa78d5cbd271b..png\n用户：五张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/c3f19ce42ba9490185fd3a622c51a918..png\n用户：你好帮我査一下下单了招募任务 达人交付完成后 怎么取消该计划\n系统：亲亲～招募任务达人交付完成后，计划会自动结束的哦～不需要手动取消呢😊\n用户：你好\n系统：亲～您刚发的图片我这边看不到具体内容呢😅 是要查账户余额还是其他问题呀？\n用户：你好帮我査一下下单了招募任务 达人交付完成后 怎么取消该计划\n系统：亲～招募任务达人交付完成后计划会自动结束的，不需要手动取消呢～如果达人已经完成交付，系统会自动关闭计划的哈😊\n用户：五张图片\n系统：亲～招募任务达人交付完成后计划会自动结束的，不需要手动取消呢～如果达人已经完成交付，系统会自动关闭计划的哈😊\n用户：五张图片\n\n当前问题：五张图片\n请你根据任务规则输出重写结果。\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=0.5, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 21:14:00 [async_llm.py:252] Added request chatcmpl-7f80d0e1b7bc4b0c91261d64599f44b2.
INFO:     127.0.0.1:46968 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-19 21:14:08 [loggers.py:111] Engine 000: Avg prompt throughput: 111.3 tokens/s, Avg generation throughput: 2.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 77.6%
INFO 06-19 21:14:18 [loggers.py:111] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 77.6%
INFO 06-19 21:21:48 [logger.py:39] Received request chatcmpl-ab5cef28da044d5ba745ff50231f7f1e: prompt: '<｜begin▁of▁sentence｜>你是一个专业的问题重写模块，专门用于多轮对话场景下的指代补全与语义还原任务。\n请严格按照以下规则执行：\n\n【任务目标】\n1. 你的目标是准确解析用户真实意图，从历史中发掘指代和意图，使得这个独立问题尽量完整，尽可能包含所有信息关键词,特别是要补充关键的动机，指代和场景（润色后的问题至少涉及：用户面对的前置情况，什么限制，什么疑问等）。\n2. 仅当历史信息能够提供明确上下文时才进行补全，否则保持当前问题不变。\n3. 对于明显不需要补全的问题如：吹水："呵呵"，命令："关机"，无关内容："人生的意义"等，返回原句\n4. 不进行任何无关发挥、扩写、润色、修辞性描述、解释、总结、感情色彩。\n5. 不编造任何不存在的假设背景或新信息。\n6. 输出格式必须严格遵循：仅输出最终重写结果文本，不包含任何前缀、提示词、说明性文字或换行符。重写的疑问句以："我想知道"开头，非疑问句你自行适配\n7. 当无需重写时，直接输出原问题。\n\n【重写示例】\n历史对话：\n用户：小红鞋什么时候有货？\n系统：请问您指的是哪款小红鞋？\n用户：就是上次缺货那款\n当前问题：就是上次缺货那款\n重写结果：我想知道上次缺货的那款小红鞋什么时候有货？\n\n历史对话：\n用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：那我们还有其他计划，能投放吗？\n当前问题：那我们还有其他计划，能投放吗？\n重写结果：我想知道当账号处于限流状态时，是否可以继续开启新的广告投放计划？\n\n历史对话：\n用户：你是谁？\n当前问题：你是谁？\n重写结果：你是谁？\n\n历史对话：\n用户：开心\n当前问题：开心\n重写结果：开心\n\n历史对话：\n用户：我要买它\n系统：请问您指的是哪款商品？\n用户：之前推荐的那双\n当前问题：之前推荐的那双\n重写结果：我想要买你之前推荐的那双鞋。\n\n注意：严格按照以上风格工作，禁止任何额外输出。\n\nUser: 用户：我们最近打算投全域，但没经验。\n系统：全域计划适合配合大场活动，建议提前准备素材。\n用户：预算要设置多少？\n系统：大场期间建议设置充足预算，比如3000元起。\n用户：那我们之前用的那种也能一起开吗？\n\n当前问题：那我们之前用的那种也能一起开吗？\n请你根据任务规则输出重写结果。\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=0.5, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 21:21:48 [async_llm.py:252] Added request chatcmpl-ab5cef28da044d5ba745ff50231f7f1e.
INFO 06-19 21:21:48 [logger.py:39] Received request chatcmpl-cf4eae641f2245248534e3509cafc99b: prompt: '<｜begin▁of▁sentence｜>你是一个专业的问题重写模块，专门用于多轮对话场景下的指代补全与语义还原任务。\n请严格按照以下规则执行：\n\n【任务目标】\n1. 你的目标是准确解析用户真实意图，从历史中发掘指代和意图，使得这个独立问题尽量完整，尽可能包含所有信息关键词,特别是要补充关键的动机，指代和场景（润色后的问题至少涉及：用户面对的前置情况，什么限制，什么疑问等）。\n2. 仅当历史信息能够提供明确上下文时才进行补全，否则保持当前问题不变。\n3. 对于明显不需要补全的问题如：吹水："呵呵"，命令："关机"，无关内容："人生的意义"等，返回原句\n4. 不进行任何无关发挥、扩写、润色、修辞性描述、解释、总结、感情色彩。\n5. 不编造任何不存在的假设背景或新信息。\n6. 输出格式必须严格遵循：仅输出最终重写结果文本，不包含任何前缀、提示词、说明性文字或换行符。重写的疑问句以："我想知道"开头，非疑问句你自行适配\n7. 当无需重写时，直接输出原问题。\n\n【重写示例】\n历史对话：\n用户：小红鞋什么时候有货？\n系统：请问您指的是哪款小红鞋？\n用户：就是上次缺货那款\n当前问题：就是上次缺货那款\n重写结果：我想知道上次缺货的那款小红鞋什么时候有货？\n\n历史对话：\n用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：那我们还有其他计划，能投放吗？\n当前问题：那我们还有其他计划，能投放吗？\n重写结果：我想知道当账号处于限流状态时，是否可以继续开启新的广告投放计划？\n\n历史对话：\n用户：你是谁？\n当前问题：你是谁？\n重写结果：你是谁？\n\n历史对话：\n用户：开心\n当前问题：开心\n重写结果：开心\n\n历史对话：\n用户：我要买它\n系统：请问您指的是哪款商品？\n用户：之前推荐的那双\n当前问题：之前推荐的那双\n重写结果：我想要买你之前推荐的那双鞋。\n\n注意：严格按照以上风格工作，禁止任何额外输出。\n\nUser: 用户：我看到页面提示“投放内容含未成年人元素”，怎么处理？\n系统：你需要确认素材中是否涉及未成年相关内容，比如儿童模特或儿童话题。\n用户：我这边是卖儿童背包的，算违规吗？\n系统：只要内容健康、安全、没有引导风险，一般不算违规。\n用户：那我是不是不改也能过？\n\n当前问题：那我是不是不改也能过？\n请你根据任务规则输出重写结果。\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=0.5, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 21:21:48 [async_llm.py:252] Added request chatcmpl-cf4eae641f2245248534e3509cafc99b.
INFO 06-19 21:21:48 [logger.py:39] Received request chatcmpl-8eff78e8367e46b39b0a3ff7bb95054c: prompt: '<｜begin▁of▁sentence｜>你是一个专业的问题重写模块，专门用于多轮对话场景下的指代补全与语义还原任务。\n请严格按照以下规则执行：\n\n【任务目标】\n1. 你的目标是准确解析用户真实意图，从历史中发掘指代和意图，使得这个独立问题尽量完整，尽可能包含所有信息关键词,特别是要补充关键的动机，指代和场景（润色后的问题至少涉及：用户面对的前置情况，什么限制，什么疑问等）。\n2. 仅当历史信息能够提供明确上下文时才进行补全，否则保持当前问题不变。\n3. 对于明显不需要补全的问题如：吹水："呵呵"，命令："关机"，无关内容："人生的意义"等，返回原句\n4. 不进行任何无关发挥、扩写、润色、修辞性描述、解释、总结、感情色彩。\n5. 不编造任何不存在的假设背景或新信息。\n6. 输出格式必须严格遵循：仅输出最终重写结果文本，不包含任何前缀、提示词、说明性文字或换行符。重写的疑问句以："我想知道"开头，非疑问句你自行适配\n7. 当无需重写时，直接输出原问题。\n\n【重写示例】\n历史对话：\n用户：小红鞋什么时候有货？\n系统：请问您指的是哪款小红鞋？\n用户：就是上次缺货那款\n当前问题：就是上次缺货那款\n重写结果：我想知道上次缺货的那款小红鞋什么时候有货？\n\n历史对话：\n用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：那我们还有其他计划，能投放吗？\n当前问题：那我们还有其他计划，能投放吗？\n重写结果：我想知道当账号处于限流状态时，是否可以继续开启新的广告投放计划？\n\n历史对话：\n用户：你是谁？\n当前问题：你是谁？\n重写结果：你是谁？\n\n历史对话：\n用户：开心\n当前问题：开心\n重写结果：开心\n\n历史对话：\n用户：我要买它\n系统：请问您指的是哪款商品？\n用户：之前推荐的那双\n当前问题：之前推荐的那双\n重写结果：我想要买你之前推荐的那双鞋。\n\n注意：严格按照以上风格工作，禁止任何额外输出。\n\nUser: 用户：账户提示授权失效了，这是什么意思？\n系统：说明你的店铺或商品绑定信息失效，需要重新授权。\n用户：那要怎么操作？\n系统：登录账户后，进入“账户绑定”页面点击“重新授权”即可。\n用户：是不是每次登录都要授权？\n\n当前问题：是不是每次登录都要授权？\n请你根据任务规则输出重写结果。\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=0.5, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 21:21:48 [async_llm.py:252] Added request chatcmpl-8eff78e8367e46b39b0a3ff7bb95054c.
INFO 06-19 21:21:48 [logger.py:39] Received request chatcmpl-f4b32bfea1d1435385eb238362a8c2f0: prompt: '<｜begin▁of▁sentence｜>你是一个专业的问题重写模块，专门用于多轮对话场景下的指代补全与语义还原任务。\n请严格按照以下规则执行：\n\n【任务目标】\n1. 你的目标是准确解析用户真实意图，从历史中发掘指代和意图，使得这个独立问题尽量完整，尽可能包含所有信息关键词,特别是要补充关键的动机，指代和场景（润色后的问题至少涉及：用户面对的前置情况，什么限制，什么疑问等）。\n2. 仅当历史信息能够提供明确上下文时才进行补全，否则保持当前问题不变。\n3. 对于明显不需要补全的问题如：吹水："呵呵"，命令："关机"，无关内容："人生的意义"等，返回原句\n4. 不进行任何无关发挥、扩写、润色、修辞性描述、解释、总结、感情色彩。\n5. 不编造任何不存在的假设背景或新信息。\n6. 输出格式必须严格遵循：仅输出最终重写结果文本，不包含任何前缀、提示词、说明性文字或换行符。重写的疑问句以："我想知道"开头，非疑问句你自行适配\n7. 当无需重写时，直接输出原问题。\n\n【重写示例】\n历史对话：\n用户：小红鞋什么时候有货？\n系统：请问您指的是哪款小红鞋？\n用户：就是上次缺货那款\n当前问题：就是上次缺货那款\n重写结果：我想知道上次缺货的那款小红鞋什么时候有货？\n\n历史对话：\n用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：那我们还有其他计划，能投放吗？\n当前问题：那我们还有其他计划，能投放吗？\n重写结果：我想知道当账号处于限流状态时，是否可以继续开启新的广告投放计划？\n\n历史对话：\n用户：你是谁？\n当前问题：你是谁？\n重写结果：你是谁？\n\n历史对话：\n用户：开心\n当前问题：开心\n重写结果：开心\n\n历史对话：\n用户：我要买它\n系统：请问您指的是哪款商品？\n用户：之前推荐的那双\n当前问题：之前推荐的那双\n重写结果：我想要买你之前推荐的那双鞋。\n\n注意：严格按照以上风格工作，禁止任何额外输出。\n\nUser: 用户：我刚入驻平台，推广要怎么开始？\n系统：建议先使用新客引导计划，操作简单且投产比稳定。\n用户：预算方面有什么推荐？\n系统：可以从日预算200元起测，连续投放3天效果更好。\n用户：那素材还需要自己拍吗？\n系统：你可以使用平台提供的模板或达人共创内容。\n用户：那我们这种情况用哪个方式更好？\n\n当前问题：那我们这种情况用哪个方式更好？\n请你根据任务规则输出重写结果。\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=0.5, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 21:21:48 [async_llm.py:252] Added request chatcmpl-f4b32bfea1d1435385eb238362a8c2f0.
INFO:     127.0.0.1:40932 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-19 21:21:49 [logger.py:39] Received request chatcmpl-180447024b7f480a96df2d1ca86a70f0: prompt: '<｜begin▁of▁sentence｜>你是一个专业的问题重写模块，专门用于多轮对话场景下的指代补全与语义还原任务。\n请严格按照以下规则执行：\n\n【任务目标】\n1. 你的目标是准确解析用户真实意图，从历史中发掘指代和意图，使得这个独立问题尽量完整，尽可能包含所有信息关键词,特别是要补充关键的动机，指代和场景（润色后的问题至少涉及：用户面对的前置情况，什么限制，什么疑问等）。\n2. 仅当历史信息能够提供明确上下文时才进行补全，否则保持当前问题不变。\n3. 对于明显不需要补全的问题如：吹水："呵呵"，命令："关机"，无关内容："人生的意义"等，返回原句\n4. 不进行任何无关发挥、扩写、润色、修辞性描述、解释、总结、感情色彩。\n5. 不编造任何不存在的假设背景或新信息。\n6. 输出格式必须严格遵循：仅输出最终重写结果文本，不包含任何前缀、提示词、说明性文字或换行符。重写的疑问句以："我想知道"开头，非疑问句你自行适配\n7. 当无需重写时，直接输出原问题。\n\n【重写示例】\n历史对话：\n用户：小红鞋什么时候有货？\n系统：请问您指的是哪款小红鞋？\n用户：就是上次缺货那款\n当前问题：就是上次缺货那款\n重写结果：我想知道上次缺货的那款小红鞋什么时候有货？\n\n历史对话：\n用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：那我们还有其他计划，能投放吗？\n当前问题：那我们还有其他计划，能投放吗？\n重写结果：我想知道当账号处于限流状态时，是否可以继续开启新的广告投放计划？\n\n历史对话：\n用户：你是谁？\n当前问题：你是谁？\n重写结果：你是谁？\n\n历史对话：\n用户：开心\n当前问题：开心\n重写结果：开心\n\n历史对话：\n用户：我要买它\n系统：请问您指的是哪款商品？\n用户：之前推荐的那双\n当前问题：之前推荐的那双\n重写结果：我想要买你之前推荐的那双鞋。\n\n注意：严格按照以上风格工作，禁止任何额外输出。\n\nUser: 用户：我在‘调控任务’中设置了预算，怎么系统还在烧钱？\n系统：你是否设置了日预算？另外也需要检查是否有其他任务计划正在投放。\n用户：任务类型太多了我也不清楚\n系统：建议进入‘推广计划-概览’，查看每个计划的消耗情况。\n用户：那我要怎么停掉？\n\n当前问题：那我要怎么停掉？\n请你根据任务规则输出重写结果。\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=0.5, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 21:21:49 [async_llm.py:252] Added request chatcmpl-180447024b7f480a96df2d1ca86a70f0.
INFO:     127.0.0.1:40914 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-19 21:21:49 [logger.py:39] Received request chatcmpl-510d4c68c76f485e94077f0ef52839d0: prompt: '<｜begin▁of▁sentence｜>你是一个专业的问题重写模块，专门用于多轮对话场景下的指代补全与语义还原任务。\n请严格按照以下规则执行：\n\n【任务目标】\n1. 你的目标是准确解析用户真实意图，从历史中发掘指代和意图，使得这个独立问题尽量完整，尽可能包含所有信息关键词,特别是要补充关键的动机，指代和场景（润色后的问题至少涉及：用户面对的前置情况，什么限制，什么疑问等）。\n2. 仅当历史信息能够提供明确上下文时才进行补全，否则保持当前问题不变。\n3. 对于明显不需要补全的问题如：吹水："呵呵"，命令："关机"，无关内容："人生的意义"等，返回原句\n4. 不进行任何无关发挥、扩写、润色、修辞性描述、解释、总结、感情色彩。\n5. 不编造任何不存在的假设背景或新信息。\n6. 输出格式必须严格遵循：仅输出最终重写结果文本，不包含任何前缀、提示词、说明性文字或换行符。重写的疑问句以："我想知道"开头，非疑问句你自行适配\n7. 当无需重写时，直接输出原问题。\n\n【重写示例】\n历史对话：\n用户：小红鞋什么时候有货？\n系统：请问您指的是哪款小红鞋？\n用户：就是上次缺货那款\n当前问题：就是上次缺货那款\n重写结果：我想知道上次缺货的那款小红鞋什么时候有货？\n\n历史对话：\n用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：那我们还有其他计划，能投放吗？\n当前问题：那我们还有其他计划，能投放吗？\n重写结果：我想知道当账号处于限流状态时，是否可以继续开启新的广告投放计划？\n\n历史对话：\n用户：你是谁？\n当前问题：你是谁？\n重写结果：你是谁？\n\n历史对话：\n用户：开心\n当前问题：开心\n重写结果：开心\n\n历史对话：\n用户：我要买它\n系统：请问您指的是哪款商品？\n用户：之前推荐的那双\n当前问题：之前推荐的那双\n重写结果：我想要买你之前推荐的那双鞋。\n\n注意：严格按照以上风格工作，禁止任何额外输出。\n\nUser: 用户：我们账号昨天刚通过审核，今天能跑投放吗？\n系统：可以的，但建议先完成基础设置，比如绑定商品和选择投放计划。\n用户：之前我在别的平台有素材，这里能用吗？\n系统：可以复用，但需确保符合抖音平台素材规范。\n用户：那如果不符合怎么办？\n\n当前问题：那如果不符合怎么办？\n请你根据任务规则输出重写结果。\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=0.5, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 21:21:49 [async_llm.py:252] Added request chatcmpl-510d4c68c76f485e94077f0ef52839d0.
INFO:     127.0.0.1:40918 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-19 21:21:49 [logger.py:39] Received request chatcmpl-83f3ddfd94fb474ea7ef42d852b516dd: prompt: '<｜begin▁of▁sentence｜>你是一个专业的问题重写模块，专门用于多轮对话场景下的指代补全与语义还原任务。\n请严格按照以下规则执行：\n\n【任务目标】\n1. 你的目标是准确解析用户真实意图，从历史中发掘指代和意图，使得这个独立问题尽量完整，尽可能包含所有信息关键词,特别是要补充关键的动机，指代和场景（润色后的问题至少涉及：用户面对的前置情况，什么限制，什么疑问等）。\n2. 仅当历史信息能够提供明确上下文时才进行补全，否则保持当前问题不变。\n3. 对于明显不需要补全的问题如：吹水："呵呵"，命令："关机"，无关内容："人生的意义"等，返回原句\n4. 不进行任何无关发挥、扩写、润色、修辞性描述、解释、总结、感情色彩。\n5. 不编造任何不存在的假设背景或新信息。\n6. 输出格式必须严格遵循：仅输出最终重写结果文本，不包含任何前缀、提示词、说明性文字或换行符。重写的疑问句以："我想知道"开头，非疑问句你自行适配\n7. 当无需重写时，直接输出原问题。\n\n【重写示例】\n历史对话：\n用户：小红鞋什么时候有货？\n系统：请问您指的是哪款小红鞋？\n用户：就是上次缺货那款\n当前问题：就是上次缺货那款\n重写结果：我想知道上次缺货的那款小红鞋什么时候有货？\n\n历史对话：\n用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：那我们还有其他计划，能投放吗？\n当前问题：那我们还有其他计划，能投放吗？\n重写结果：我想知道当账号处于限流状态时，是否可以继续开启新的广告投放计划？\n\n历史对话：\n用户：你是谁？\n当前问题：你是谁？\n重写结果：你是谁？\n\n历史对话：\n用户：开心\n当前问题：开心\n重写结果：开心\n\n历史对话：\n用户：我要买它\n系统：请问您指的是哪款商品？\n用户：之前推荐的那双\n当前问题：之前推荐的那双\n重写结果：我想要买你之前推荐的那双鞋。\n\n注意：严格按照以上风格工作，禁止任何额外输出。\n\nUser: 用户：我们最近计划在抖音做大促，但对流程不熟。\n系统：建议你先了解平台的大促节奏，比如双十一、618等，提前准备好素材和预算。\n用户：预算应该准备多少比较合适？\n系统：建议根据以往转化率和客单价评估，一般从5000元起步较稳妥。\n用户：素材方面有哪些注意事项？\n系统：内容要突出促销点，加入限时标签、优惠码等信息可以提升点击率。\n用户：那达人投放要现在联系吗？\n系统：是的，热门达人档期紧张，建议提前至少一周确定合作。\n用户：投放时间段有没有讲究？\n系统：高峰期一般集中在中午12点和晚上8点后，可以重点投放。\n用户：听说全域计划不错，我们也能用吗？\n系统：可以，全域计划适合有多渠道曝光需求的商家，需提前设置好人群和场景。\n用户：数据监控是用平台自带的吗？\n系统：平台提供实时监控面板，你也可以接入第三方分析工具。\n用户：那之前的数据能复用吗？\n系统：可以参考历史投放效果，特别是高转化素材和人群数据。\n用户：我们产品属于教育类，会有敏感问题吗？\n系统：教育类需要避免夸大效果和绝对化表述，确保内容合规。\n用户：那素材里提到“最好成绩”可以用吗？\n系统：建议用“显著提升”“客户反馈良好”等更安全的表达方式。\n用户：如果还是被判违规怎么办？\n系统：可以提交申诉，附上真实教学场景或用户反馈作为佐证。\n用户：这么说我们得提前准备这些证明材料？\n\n当前问题：这么说我们得提前准备这些证明材料？\n请你根据任务规则输出重写结果。\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=0.5, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 21:21:49 [async_llm.py:252] Added request chatcmpl-83f3ddfd94fb474ea7ef42d852b516dd.
INFO:     127.0.0.1:40932 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-19 21:21:49 [logger.py:39] Received request chatcmpl-6b2d6fad6d1d48e98aa9d63f7d1f02d3: prompt: '<｜begin▁of▁sentence｜>你是一个专业的问题重写模块，专门用于多轮对话场景下的指代补全与语义还原任务。\n请严格按照以下规则执行：\n\n【任务目标】\n1. 你的目标是准确解析用户真实意图，从历史中发掘指代和意图，使得这个独立问题尽量完整，尽可能包含所有信息关键词,特别是要补充关键的动机，指代和场景（润色后的问题至少涉及：用户面对的前置情况，什么限制，什么疑问等）。\n2. 仅当历史信息能够提供明确上下文时才进行补全，否则保持当前问题不变。\n3. 对于明显不需要补全的问题如：吹水："呵呵"，命令："关机"，无关内容："人生的意义"等，返回原句\n4. 不进行任何无关发挥、扩写、润色、修辞性描述、解释、总结、感情色彩。\n5. 不编造任何不存在的假设背景或新信息。\n6. 输出格式必须严格遵循：仅输出最终重写结果文本，不包含任何前缀、提示词、说明性文字或换行符。重写的疑问句以："我想知道"开头，非疑问句你自行适配\n7. 当无需重写时，直接输出原问题。\n\n【重写示例】\n历史对话：\n用户：小红鞋什么时候有货？\n系统：请问您指的是哪款小红鞋？\n用户：就是上次缺货那款\n当前问题：就是上次缺货那款\n重写结果：我想知道上次缺货的那款小红鞋什么时候有货？\n\n历史对话：\n用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：那我们还有其他计划，能投放吗？\n当前问题：那我们还有其他计划，能投放吗？\n重写结果：我想知道当账号处于限流状态时，是否可以继续开启新的广告投放计划？\n\n历史对话：\n用户：你是谁？\n当前问题：你是谁？\n重写结果：你是谁？\n\n历史对话：\n用户：开心\n当前问题：开心\n重写结果：开心\n\n历史对话：\n用户：我要买它\n系统：请问您指的是哪款商品？\n用户：之前推荐的那双\n当前问题：之前推荐的那双\n重写结果：我想要买你之前推荐的那双鞋。\n\n注意：严格按照以上风格工作，禁止任何额外输出。\n\nUser: 用户：我想做短视频投放，有什么要求吗？\n系统：要求内容原创、清晰，不能有违规元素，建议时长控制在15秒左右。\n用户：那拍的时候有什么建议？\n系统：建议突出产品卖点，开头三秒抓人，结尾加上行动引导。\n用户：那像我们这种的适合吗？\n\n当前问题：那像我们这种的适合吗？\n请你根据任务规则输出重写结果。\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=0.5, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 21:21:49 [async_llm.py:252] Added request chatcmpl-6b2d6fad6d1d48e98aa9d63f7d1f02d3.
INFO:     127.0.0.1:40948 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-19 21:21:49 [logger.py:39] Received request chatcmpl-443dca210e1f4f41b5b8355713089122: prompt: '<｜begin▁of▁sentence｜>你是一个专业的问题重写模块，专门用于多轮对话场景下的指代补全与语义还原任务。\n请严格按照以下规则执行：\n\n【任务目标】\n1. 你的目标是准确解析用户真实意图，从历史中发掘指代和意图，使得这个独立问题尽量完整，尽可能包含所有信息关键词,特别是要补充关键的动机，指代和场景（润色后的问题至少涉及：用户面对的前置情况，什么限制，什么疑问等）。\n2. 仅当历史信息能够提供明确上下文时才进行补全，否则保持当前问题不变。\n3. 对于明显不需要补全的问题如：吹水："呵呵"，命令："关机"，无关内容："人生的意义"等，返回原句\n4. 不进行任何无关发挥、扩写、润色、修辞性描述、解释、总结、感情色彩。\n5. 不编造任何不存在的假设背景或新信息。\n6. 输出格式必须严格遵循：仅输出最终重写结果文本，不包含任何前缀、提示词、说明性文字或换行符。重写的疑问句以："我想知道"开头，非疑问句你自行适配\n7. 当无需重写时，直接输出原问题。\n\n【重写示例】\n历史对话：\n用户：小红鞋什么时候有货？\n系统：请问您指的是哪款小红鞋？\n用户：就是上次缺货那款\n当前问题：就是上次缺货那款\n重写结果：我想知道上次缺货的那款小红鞋什么时候有货？\n\n历史对话：\n用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：那我们还有其他计划，能投放吗？\n当前问题：那我们还有其他计划，能投放吗？\n重写结果：我想知道当账号处于限流状态时，是否可以继续开启新的广告投放计划？\n\n历史对话：\n用户：你是谁？\n当前问题：你是谁？\n重写结果：你是谁？\n\n历史对话：\n用户：开心\n当前问题：开心\n重写结果：开心\n\n历史对话：\n用户：我要买它\n系统：请问您指的是哪款商品？\n用户：之前推荐的那双\n当前问题：之前推荐的那双\n重写结果：我想要买你之前推荐的那双鞋。\n\n注意：严格按照以上风格工作，禁止任何额外输出。\n\nUser: 用户：我在投放中用了模板创意，感觉数据还行\n系统：模板创意的投产稳定，但可能不如定制素材吸引力强。\n用户：那转化会差多少？\n系统：一般差距在20%左右，但具体还得看产品类型和受众人群。\n用户：我们这种是不是不适合一直用那个？\n\n当前问题：我们这种是不是不适合一直用那个？\n请你根据任务规则输出重写结果。\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=0.5, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 21:21:49 [async_llm.py:252] Added request chatcmpl-443dca210e1f4f41b5b8355713089122.
INFO:     127.0.0.1:40914 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-19 21:21:49 [logger.py:39] Received request chatcmpl-2e6b8398fefa44cfb849f7293aba06e0: prompt: '<｜begin▁of▁sentence｜>你是一个专业的问题重写模块，专门用于多轮对话场景下的指代补全与语义还原任务。\n请严格按照以下规则执行：\n\n【任务目标】\n1. 你的目标是准确解析用户真实意图，从历史中发掘指代和意图，使得这个独立问题尽量完整，尽可能包含所有信息关键词,特别是要补充关键的动机，指代和场景（润色后的问题至少涉及：用户面对的前置情况，什么限制，什么疑问等）。\n2. 仅当历史信息能够提供明确上下文时才进行补全，否则保持当前问题不变。\n3. 对于明显不需要补全的问题如：吹水："呵呵"，命令："关机"，无关内容："人生的意义"等，返回原句\n4. 不进行任何无关发挥、扩写、润色、修辞性描述、解释、总结、感情色彩。\n5. 不编造任何不存在的假设背景或新信息。\n6. 输出格式必须严格遵循：仅输出最终重写结果文本，不包含任何前缀、提示词、说明性文字或换行符。重写的疑问句以："我想知道"开头，非疑问句你自行适配\n7. 当无需重写时，直接输出原问题。\n\n【重写示例】\n历史对话：\n用户：小红鞋什么时候有货？\n系统：请问您指的是哪款小红鞋？\n用户：就是上次缺货那款\n当前问题：就是上次缺货那款\n重写结果：我想知道上次缺货的那款小红鞋什么时候有货？\n\n历史对话：\n用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：那我们还有其他计划，能投放吗？\n当前问题：那我们还有其他计划，能投放吗？\n重写结果：我想知道当账号处于限流状态时，是否可以继续开启新的广告投放计划？\n\n历史对话：\n用户：你是谁？\n当前问题：你是谁？\n重写结果：你是谁？\n\n历史对话：\n用户：开心\n当前问题：开心\n重写结果：开心\n\n历史对话：\n用户：我要买它\n系统：请问您指的是哪款商品？\n用户：之前推荐的那双\n当前问题：之前推荐的那双\n重写结果：我想要买你之前推荐的那双鞋。\n\n注意：严格按照以上风格工作，禁止任何额外输出。\n\nUser: 用户：我们最近准备上线夏季新款，打算做一波推广，有什么建议？\n系统：建议使用季节性主题投放计划，结合短视频内容更容易吸引用户。\n用户：短视频我们自己拍可以吗？\n系统：当然可以，平台也提供达人共创服务，拍摄更专业。\n用户：预算不多，达人费用会不会很贵？\n系统：可以选择中腰部达人合作，价格更友好，性价比高。\n用户：我们主营女装，有没有对应的达人推荐？\n系统：平台有女装领域达人库，可按风格、粉丝量筛选。\n用户：我们不太懂这些，能不能帮我们选？\n系统：你可以申请投放顾问服务，由平台专员帮你定制方案。\n用户：那直播和短视频要不要同时做？\n系统：同时开启效果最好，直播转化高，短视频补充流量。\n\n当前问题：直播和短视频要不要同时做？\n请你根据任务规则输出重写结果。\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=0.5, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 21:21:49 [async_llm.py:252] Added request chatcmpl-2e6b8398fefa44cfb849f7293aba06e0.
INFO:     127.0.0.1:40918 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-19 21:21:49 [logger.py:39] Received request chatcmpl-6dbbe505cf4e470ebcd0d7018aa9d2dc: prompt: '<｜begin▁of▁sentence｜>你是一个专业的问题重写模块，专门用于多轮对话场景下的指代补全与语义还原任务。\n请严格按照以下规则执行：\n\n【任务目标】\n1. 你的目标是准确解析用户真实意图，从历史中发掘指代和意图，使得这个独立问题尽量完整，尽可能包含所有信息关键词,特别是要补充关键的动机，指代和场景（润色后的问题至少涉及：用户面对的前置情况，什么限制，什么疑问等）。\n2. 仅当历史信息能够提供明确上下文时才进行补全，否则保持当前问题不变。\n3. 对于明显不需要补全的问题如：吹水："呵呵"，命令："关机"，无关内容："人生的意义"等，返回原句\n4. 不进行任何无关发挥、扩写、润色、修辞性描述、解释、总结、感情色彩。\n5. 不编造任何不存在的假设背景或新信息。\n6. 输出格式必须严格遵循：仅输出最终重写结果文本，不包含任何前缀、提示词、说明性文字或换行符。重写的疑问句以："我想知道"开头，非疑问句你自行适配\n7. 当无需重写时，直接输出原问题。\n\n【重写示例】\n历史对话：\n用户：小红鞋什么时候有货？\n系统：请问您指的是哪款小红鞋？\n用户：就是上次缺货那款\n当前问题：就是上次缺货那款\n重写结果：我想知道上次缺货的那款小红鞋什么时候有货？\n\n历史对话：\n用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：那我们还有其他计划，能投放吗？\n当前问题：那我们还有其他计划，能投放吗？\n重写结果：我想知道当账号处于限流状态时，是否可以继续开启新的广告投放计划？\n\n历史对话：\n用户：你是谁？\n当前问题：你是谁？\n重写结果：你是谁？\n\n历史对话：\n用户：开心\n当前问题：开心\n重写结果：开心\n\n历史对话：\n用户：我要买它\n系统：请问您指的是哪款商品？\n用户：之前推荐的那双\n当前问题：之前推荐的那双\n重写结果：我想要买你之前推荐的那双鞋。\n\n注意：严格按照以上风格工作，禁止任何额外输出。\n\nUser: 用户：平台提示我存在“虚假宣传”的风险，我没理解什么意思\n系统：可能是你素材中使用了“第一”、“最强”等绝对化用词\n用户：那如果我只是说了‘很受欢迎’，也不行吗？\n系统：‘受欢迎’一般可以，但不能主观夸大或无证据证明。\n用户：那像我写的那些都要改吗？\n\n当前问题：那像我写的那些都要改吗？\n请你根据任务规则输出重写结果。\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=0.5, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 21:21:49 [async_llm.py:252] Added request chatcmpl-6dbbe505cf4e470ebcd0d7018aa9d2dc.
INFO:     127.0.0.1:40948 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-19 21:21:49 [logger.py:39] Received request chatcmpl-a51538233298444db516d5ca7735af34: prompt: '<｜begin▁of▁sentence｜>你是一个专业的问题重写模块，专门用于多轮对话场景下的指代补全与语义还原任务。\n请严格按照以下规则执行：\n\n【任务目标】\n1. 你的目标是准确解析用户真实意图，从历史中发掘指代和意图，使得这个独立问题尽量完整，尽可能包含所有信息关键词,特别是要补充关键的动机，指代和场景（润色后的问题至少涉及：用户面对的前置情况，什么限制，什么疑问等）。\n2. 仅当历史信息能够提供明确上下文时才进行补全，否则保持当前问题不变。\n3. 对于明显不需要补全的问题如：吹水："呵呵"，命令："关机"，无关内容："人生的意义"等，返回原句\n4. 不进行任何无关发挥、扩写、润色、修辞性描述、解释、总结、感情色彩。\n5. 不编造任何不存在的假设背景或新信息。\n6. 输出格式必须严格遵循：仅输出最终重写结果文本，不包含任何前缀、提示词、说明性文字或换行符。重写的疑问句以："我想知道"开头，非疑问句你自行适配\n7. 当无需重写时，直接输出原问题。\n\n【重写示例】\n历史对话：\n用户：小红鞋什么时候有货？\n系统：请问您指的是哪款小红鞋？\n用户：就是上次缺货那款\n当前问题：就是上次缺货那款\n重写结果：我想知道上次缺货的那款小红鞋什么时候有货？\n\n历史对话：\n用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：那我们还有其他计划，能投放吗？\n当前问题：那我们还有其他计划，能投放吗？\n重写结果：我想知道当账号处于限流状态时，是否可以继续开启新的广告投放计划？\n\n历史对话：\n用户：你是谁？\n当前问题：你是谁？\n重写结果：你是谁？\n\n历史对话：\n用户：开心\n当前问题：开心\n重写结果：开心\n\n历史对话：\n用户：我要买它\n系统：请问您指的是哪款商品？\n用户：之前推荐的那双\n当前问题：之前推荐的那双\n重写结果：我想要买你之前推荐的那双鞋。\n\n注意：严格按照以上风格工作，禁止任何额外输出。\n\nUser: 用户：我打算做一场新品直播，有什么建议？\n系统：建议提前3天预热，准备预售链接并邀请达人带播。\n用户：我们预算紧张，请问达人费用能省吗？\n系统：可以选择平台免费达人共创服务，或用中腰部达人进行合作。\n用户：那如果我们就是小商家呢？\n\n当前问题：那如果我们就是小商家呢？\n请你根据任务规则输出重写结果。\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=0.5, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 21:21:49 [async_llm.py:252] Added request chatcmpl-a51538233298444db516d5ca7735af34.
INFO:     127.0.0.1:40914 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-19 21:21:49 [logger.py:39] Received request chatcmpl-f5c0b74940eb42c6a7fddb9341a536c7: prompt: '<｜begin▁of▁sentence｜>你是一个专业的问题重写模块，专门用于多轮对话场景下的指代补全与语义还原任务。\n请严格按照以下规则执行：\n\n【任务目标】\n1. 你的目标是准确解析用户真实意图，从历史中发掘指代和意图，使得这个独立问题尽量完整，尽可能包含所有信息关键词,特别是要补充关键的动机，指代和场景（润色后的问题至少涉及：用户面对的前置情况，什么限制，什么疑问等）。\n2. 仅当历史信息能够提供明确上下文时才进行补全，否则保持当前问题不变。\n3. 对于明显不需要补全的问题如：吹水："呵呵"，命令："关机"，无关内容："人生的意义"等，返回原句\n4. 不进行任何无关发挥、扩写、润色、修辞性描述、解释、总结、感情色彩。\n5. 不编造任何不存在的假设背景或新信息。\n6. 输出格式必须严格遵循：仅输出最终重写结果文本，不包含任何前缀、提示词、说明性文字或换行符。重写的疑问句以："我想知道"开头，非疑问句你自行适配\n7. 当无需重写时，直接输出原问题。\n\n【重写示例】\n历史对话：\n用户：小红鞋什么时候有货？\n系统：请问您指的是哪款小红鞋？\n用户：就是上次缺货那款\n当前问题：就是上次缺货那款\n重写结果：我想知道上次缺货的那款小红鞋什么时候有货？\n\n历史对话：\n用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：那我们还有其他计划，能投放吗？\n当前问题：那我们还有其他计划，能投放吗？\n重写结果：我想知道当账号处于限流状态时，是否可以继续开启新的广告投放计划？\n\n历史对话：\n用户：你是谁？\n当前问题：你是谁？\n重写结果：你是谁？\n\n历史对话：\n用户：开心\n当前问题：开心\n重写结果：开心\n\n历史对话：\n用户：我要买它\n系统：请问您指的是哪款商品？\n用户：之前推荐的那双\n当前问题：之前推荐的那双\n重写结果：我想要买你之前推荐的那双鞋。\n\n注意：严格按照以上风格工作，禁止任何额外输出。\n\nUser: 用户：我刚接手账号，很多操作流程不熟悉\n系统：你可以参考平台的操作指引文档，帮助快速上手。\n用户：有没有视频教程？\n系统：有的，后台“帮助中心”中有详尽的视频教学。\n用户：我们之前跑的千川计划数据能看到吗？\n系统：可以在账户概览中查看历史投放效果，支持下载。\n用户：推广类型太多，我不知道该选哪个\n系统：可以根据你的产品类型和目标选择，如新品推荐适合定向引流。\n用户：我们有老客复购目标怎么办？\n系统：可以开启“老客回访计划”，提升复购率。\n用户：投放期间要每天优化吗？\n系统：建议每日观察数据波动并做微调，特别是投放初期\n用户：有没有自动优化功能？\n系统：可以开启智能调价和人群扩展，平台会自动优化。\n\n当前问题：有没有自动优化功能？\n请你根据任务规则输出重写结果。\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=0.5, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 21:21:49 [async_llm.py:252] Added request chatcmpl-f5c0b74940eb42c6a7fddb9341a536c7.
INFO:     127.0.0.1:40932 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-19 21:21:49 [logger.py:39] Received request chatcmpl-59384e0b55894ed89193a97655af93d9: prompt: '<｜begin▁of▁sentence｜>你是一个专业的问题重写模块，专门用于多轮对话场景下的指代补全与语义还原任务。\n请严格按照以下规则执行：\n\n【任务目标】\n1. 你的目标是准确解析用户真实意图，从历史中发掘指代和意图，使得这个独立问题尽量完整，尽可能包含所有信息关键词,特别是要补充关键的动机，指代和场景（润色后的问题至少涉及：用户面对的前置情况，什么限制，什么疑问等）。\n2. 仅当历史信息能够提供明确上下文时才进行补全，否则保持当前问题不变。\n3. 对于明显不需要补全的问题如：吹水："呵呵"，命令："关机"，无关内容："人生的意义"等，返回原句\n4. 不进行任何无关发挥、扩写、润色、修辞性描述、解释、总结、感情色彩。\n5. 不编造任何不存在的假设背景或新信息。\n6. 输出格式必须严格遵循：仅输出最终重写结果文本，不包含任何前缀、提示词、说明性文字或换行符。重写的疑问句以："我想知道"开头，非疑问句你自行适配\n7. 当无需重写时，直接输出原问题。\n\n【重写示例】\n历史对话：\n用户：小红鞋什么时候有货？\n系统：请问您指的是哪款小红鞋？\n用户：就是上次缺货那款\n当前问题：就是上次缺货那款\n重写结果：我想知道上次缺货的那款小红鞋什么时候有货？\n\n历史对话：\n用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：那我们还有其他计划，能投放吗？\n当前问题：那我们还有其他计划，能投放吗？\n重写结果：我想知道当账号处于限流状态时，是否可以继续开启新的广告投放计划？\n\n历史对话：\n用户：你是谁？\n当前问题：你是谁？\n重写结果：你是谁？\n\n历史对话：\n用户：开心\n当前问题：开心\n重写结果：开心\n\n历史对话：\n用户：我要买它\n系统：请问您指的是哪款商品？\n用户：之前推荐的那双\n当前问题：之前推荐的那双\n重写结果：我想要买你之前推荐的那双鞋。\n\n注意：严格按照以上风格工作，禁止任何额外输出。\n\nUser: 用户：我在千川平台上新手投放，应该怎么规划预算？\n系统：新手投放建议从日预算200元起步，根据表现逐步调整。\n用户：那我是不是可以从最低预算开始测试？\n系统：可以的，但建议至少持续3天观察数据趋势。\n用户：我们这边是做女装的，适合什么样的推广计划？\n系统：女装类适合图文结合的投放，也可以尝试短视频种草。\n用户：那达人带货和自播哪种更适合我们这种情况？\n系统：新手建议先自播，控制成本，后续再考虑达人合作。\n用户：素材方面有什么具体要求吗？\n系统：素材需高清、无水印，且不能包含夸大宣传内容。\n用户：我们用之前拍的图文可以吗？\n系统：旧素材可以参考使用，但需要检查是否符合当前标准。\n用户：不符合规范会有什么后果？\n系统：不合规素材会被拒投，严重可能影响账户健康度。\n用户：如果我只想先做曝光，哪种投放方式更合适？\n系统：曝光导向可选择CPM计费计划，适合新品冷启动。\n用户：我朋友推荐我用全域推广，这个适合我吗？\n系统：全域推广适合预算充足、追求多渠道触达的商家。\n用户：听说最近平台有补贴政策，可以用在这类计划吗？\n系统：平台补贴需要申请指定计划，目前新商家也可参与。\n用户：那这种计划和常规的计划能一起开吗？\n系统：两个计划可以并行，但需做好预算分配与计划排期。\n\n当前问题：那这种计划和常规的计划能一起开吗？\n请你根据任务规则输出重写结果。\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=0.5, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 21:21:49 [async_llm.py:252] Added request chatcmpl-59384e0b55894ed89193a97655af93d9.
INFO:     127.0.0.1:40948 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-19 21:21:49 [logger.py:39] Received request chatcmpl-ece13a2dc3a04193b4ee62259bca84fe: prompt: '<｜begin▁of▁sentence｜>你是一个专业的问题重写模块，专门用于多轮对话场景下的指代补全与语义还原任务。\n请严格按照以下规则执行：\n\n【任务目标】\n1. 你的目标是准确解析用户真实意图，从历史中发掘指代和意图，使得这个独立问题尽量完整，尽可能包含所有信息关键词,特别是要补充关键的动机，指代和场景（润色后的问题至少涉及：用户面对的前置情况，什么限制，什么疑问等）。\n2. 仅当历史信息能够提供明确上下文时才进行补全，否则保持当前问题不变。\n3. 对于明显不需要补全的问题如：吹水："呵呵"，命令："关机"，无关内容："人生的意义"等，返回原句\n4. 不进行任何无关发挥、扩写、润色、修辞性描述、解释、总结、感情色彩。\n5. 不编造任何不存在的假设背景或新信息。\n6. 输出格式必须严格遵循：仅输出最终重写结果文本，不包含任何前缀、提示词、说明性文字或换行符。重写的疑问句以："我想知道"开头，非疑问句你自行适配\n7. 当无需重写时，直接输出原问题。\n\n【重写示例】\n历史对话：\n用户：小红鞋什么时候有货？\n系统：请问您指的是哪款小红鞋？\n用户：就是上次缺货那款\n当前问题：就是上次缺货那款\n重写结果：我想知道上次缺货的那款小红鞋什么时候有货？\n\n历史对话：\n用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：那我们还有其他计划，能投放吗？\n当前问题：那我们还有其他计划，能投放吗？\n重写结果：我想知道当账号处于限流状态时，是否可以继续开启新的广告投放计划？\n\n历史对话：\n用户：你是谁？\n当前问题：你是谁？\n重写结果：你是谁？\n\n历史对话：\n用户：开心\n当前问题：开心\n重写结果：开心\n\n历史对话：\n用户：我要买它\n系统：请问您指的是哪款商品？\n用户：之前推荐的那双\n当前问题：之前推荐的那双\n重写结果：我想要买你之前推荐的那双鞋。\n\n注意：严格按照以上风格工作，禁止任何额外输出。\n\nUser: 用户：我们最近计划在抖音做大促，但对流程不熟。\n系统：建议你先了解平台的大促节奏，比如双十一、618等，提前准备好素材和预算。\n用户：预算应该准备多少比较合适？\n系统：建议根据以往转化率和客单价评估，一般从5000元起步较稳妥。\n用户：素材方面有哪些注意事项？\n系统：内容要突出促销点，加入限时标签、优惠码等信息可以提升点击率。\n用户：那达人投放要现在联系吗？\n系统：是的，热门达人档期紧张，建议提前至少一周确定合作。\n用户：投放时间段有没有讲究？\n系统：高峰期一般集中在中午12点和晚上8点后，可以重点投放。\n用户：听说全域计划不错，我们也能用吗？\n系统：可以，全域计划适合有多渠道曝光需求的商家，需提前设置好人群和场景。\n用户：数据监控是用平台自带的吗？\n系统：平台提供实时监控面板，你也可以接入第三方分析工具如蝉妈妈、新榜。\n用户：那之前的数据能复用吗？比如去年双11的投放报告。\n系统：可以参考历史投放效果，特别是高转化素材和人群画像，但需注意平台规则和用户偏好的时效性变化。\n用户：我们产品属于教育类，会有敏感问题吗？比如课程效果承诺\n系统：教育类属于高风险类目，需严格遵守《广告法》，避免使用“保过”“必提分”等绝对化表述，建议突出教学服务流程而非效果承诺。\n用户：那素材里提到“名师1对1辅导”合规吗？需要提供教师资格证吗？\n系统：“名师”需明确资质定义，若涉及具体教师需提供教师资格证编号或职称证明，建议用“资深教师团队”等模糊表述降低风险。\n用户：如果素材中出现学生成绩单截图，是否需要打码处理？\n系统：需隐去学生姓名、学校等隐私信息，仅保留成绩提升对比数据，且需获得家长/学生的肖像授权。\n用户：平台审核一般需要多久？我们怕赶不上大促档期\n系统：常规审核周期为2-4小时，但大促期间可能延长至12小时，建议提前3天提交素材预审。\n用户：万一审核被拒，有没有快速申诉通道？\n系统：可通过抖音小店后台的“内容审核申诉”入口提交，需附上课程大纲、学员合同等合规证明材料，工作日内4小时内会有复核。\n用户：这些证明材料需要提前准备哪些具体文件？\n系统：建议准备教师资质证书扫描件、学员报名协议模板、课程内容合规性自评报告，以及过往无违规记录的平台截图。\n用户：投放后的数据复盘要关注哪些核心指标？\n系统：重点关注CTR（点击率）、CVR（转化率）、ROI（投资回报率），以及观众停留时长和评论互动量，这些能反映素材吸引力和人群精准度。\n用户：如果发现某时段转化率低，该如何调整？\n系统：可先测试更换素材版本，若效果仍不佳则调整投放时段或扩大人群包，同时检查是否因竞品同期促销分流导致。\n用户：教育类大促有没有特殊的流量扶持政策？\n系统：平台对知识付费类产品有“学浪计划”扶持，需入驻学浪平台并完成ICP备案，符合条件可获得流量加权和佣金优惠。\n用户：那我们现在需要先完成ICP备案吗？流程复杂吗？\n系统：是的，经营性网站必须备案，流程包括提交营业执照、域名证书等材料，通常需要20个工作日，建议立即启动。\n用户：这么说我们得尽快准备ICP备案材料和教师资质文件了？\n\n当前问题：这么说我们得尽快准备ICP备案材料和教师资质文件了？\n请你根据任务规则输出重写结果。\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=0.5, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 21:21:49 [async_llm.py:252] Added request chatcmpl-ece13a2dc3a04193b4ee62259bca84fe.
INFO:     127.0.0.1:40918 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-19 21:21:50 [logger.py:39] Received request chatcmpl-5bb059c42a1946cfa2a00d3890736561: prompt: '<｜begin▁of▁sentence｜>你是一个专业的问题重写模块，专门用于多轮对话场景下的指代补全与语义还原任务。\n请严格按照以下规则执行：\n\n【任务目标】\n1. 你的目标是准确解析用户真实意图，从历史中发掘指代和意图，使得这个独立问题尽量完整，尽可能包含所有信息关键词,特别是要补充关键的动机，指代和场景（润色后的问题至少涉及：用户面对的前置情况，什么限制，什么疑问等）。\n2. 仅当历史信息能够提供明确上下文时才进行补全，否则保持当前问题不变。\n3. 对于明显不需要补全的问题如：吹水："呵呵"，命令："关机"，无关内容："人生的意义"等，返回原句\n4. 不进行任何无关发挥、扩写、润色、修辞性描述、解释、总结、感情色彩。\n5. 不编造任何不存在的假设背景或新信息。\n6. 输出格式必须严格遵循：仅输出最终重写结果文本，不包含任何前缀、提示词、说明性文字或换行符。重写的疑问句以："我想知道"开头，非疑问句你自行适配\n7. 当无需重写时，直接输出原问题。\n\n【重写示例】\n历史对话：\n用户：小红鞋什么时候有货？\n系统：请问您指的是哪款小红鞋？\n用户：就是上次缺货那款\n当前问题：就是上次缺货那款\n重写结果：我想知道上次缺货的那款小红鞋什么时候有货？\n\n历史对话：\n用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：那我们还有其他计划，能投放吗？\n当前问题：那我们还有其他计划，能投放吗？\n重写结果：我想知道当账号处于限流状态时，是否可以继续开启新的广告投放计划？\n\n历史对话：\n用户：你是谁？\n当前问题：你是谁？\n重写结果：你是谁？\n\n历史对话：\n用户：开心\n当前问题：开心\n重写结果：开心\n\n历史对话：\n用户：我要买它\n系统：请问您指的是哪款商品？\n用户：之前推荐的那双\n当前问题：之前推荐的那双\n重写结果：我想要买你之前推荐的那双鞋。\n\n注意：严格按照以上风格工作，禁止任何额外输出。\n\nUser: 用户：我们是线下商家，现在打算做线上引流，有推荐吗？\n系统：建议开启门店引流计划，支持设置门店范围和定向人群。\n用户：这种适合卖场类吗？\n系统：非常适合，尤其是结合地理位置定投能吸引附近用户。\n用户：那如果我们也想直播呢？\n系统：可以开设门店直播间，平台支持本地服务标签。\n用户：我们没有专业主播怎么办？\n系统：可申请官方培训，或招募兼职主播。\n用户：直播需要每天开吗？\n系统：建议固定时间开播，提升用户黏性。\n用户：我们品类比较杂，怎么规划内容？\n系统：可按品类做主题直播，如每周主推某类产品。\n\n当前问题：我们品类比较杂，怎么规划内容？\n请你根据任务规则输出重写结果。\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=0.5, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 21:21:50 [async_llm.py:252] Added request chatcmpl-5bb059c42a1946cfa2a00d3890736561.
INFO:     127.0.0.1:40914 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-19 21:21:50 [logger.py:39] Received request chatcmpl-9f22737eee864f6aaa30ce8c933548f0: prompt: '<｜begin▁of▁sentence｜>你是一个专业的问题重写模块，专门用于多轮对话场景下的指代补全与语义还原任务。\n请严格按照以下规则执行：\n\n【任务目标】\n1. 你的目标是准确解析用户真实意图，从历史中发掘指代和意图，使得这个独立问题尽量完整，尽可能包含所有信息关键词,特别是要补充关键的动机，指代和场景（润色后的问题至少涉及：用户面对的前置情况，什么限制，什么疑问等）。\n2. 仅当历史信息能够提供明确上下文时才进行补全，否则保持当前问题不变。\n3. 对于明显不需要补全的问题如：吹水："呵呵"，命令："关机"，无关内容："人生的意义"等，返回原句\n4. 不进行任何无关发挥、扩写、润色、修辞性描述、解释、总结、感情色彩。\n5. 不编造任何不存在的假设背景或新信息。\n6. 输出格式必须严格遵循：仅输出最终重写结果文本，不包含任何前缀、提示词、说明性文字或换行符。重写的疑问句以："我想知道"开头，非疑问句你自行适配\n7. 当无需重写时，直接输出原问题。\n\n【重写示例】\n历史对话：\n用户：小红鞋什么时候有货？\n系统：请问您指的是哪款小红鞋？\n用户：就是上次缺货那款\n当前问题：就是上次缺货那款\n重写结果：我想知道上次缺货的那款小红鞋什么时候有货？\n\n历史对话：\n用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：那我们还有其他计划，能投放吗？\n当前问题：那我们还有其他计划，能投放吗？\n重写结果：我想知道当账号处于限流状态时，是否可以继续开启新的广告投放计划？\n\n历史对话：\n用户：你是谁？\n当前问题：你是谁？\n重写结果：你是谁？\n\n历史对话：\n用户：开心\n当前问题：开心\n重写结果：开心\n\n历史对话：\n用户：我要买它\n系统：请问您指的是哪款商品？\n用户：之前推荐的那双\n当前问题：之前推荐的那双\n重写结果：我想要买你之前推荐的那双鞋。\n\n注意：严格按照以上风格工作，禁止任何额外输出。\n\nUser: 用户：跨境大促推进口保健品，需要啥资质？\n系统：需提供海关报关单、检验检疫证明、中文标签备案，保健食品需额外提供《进口保健食品批准证书》。\n用户：素材里能宣传“国外医院推荐”吗？\n系统：禁止使用医疗机构推荐用语，可展示“XX国家销量TOP1”等市场数据，但需标注数据来源。\n用户：保税仓发货和直邮有啥区别？\n系统：保税仓发货更快（2-3天），需提前备货；直邮周期长（7-15天），但库存压力小，可根据大促节奏选择。\n用户：关税怎么计算？会影响到手价吗？\n系统：跨境商品适用“跨境电商综合税”，税率一般为9.1%-26.3%，建议在商品页标注“税费自理”或包税政策。\n用户：用户投诉商品与描述不符怎么处理？\n系统：需提供国际物流单号、商品实拍图对比，若确属描述问题，需承担退货运费及可能的关税损失。\n用户：这么说我们得确保所有进口资质和宣传数据可溯源了？\n系统：是的，跨境商品一旦违规可能面临下架和罚款，建议建立资质台账管理制度。\n\n当前问题：这么说我们得确保所有进口资质和宣传数据可溯源了？\n请你根据任务规则输出重写结果。\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=0.5, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 21:21:50 [async_llm.py:252] Added request chatcmpl-9f22737eee864f6aaa30ce8c933548f0.
INFO:     127.0.0.1:40932 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-19 21:21:50 [logger.py:39] Received request chatcmpl-1d01d1a9f3a742aebcb1a1eace0e44e8: prompt: '<｜begin▁of▁sentence｜>你是一个专业的问题重写模块，专门用于多轮对话场景下的指代补全与语义还原任务。\n请严格按照以下规则执行：\n\n【任务目标】\n1. 你的目标是准确解析用户真实意图，从历史中发掘指代和意图，使得这个独立问题尽量完整，尽可能包含所有信息关键词,特别是要补充关键的动机，指代和场景（润色后的问题至少涉及：用户面对的前置情况，什么限制，什么疑问等）。\n2. 仅当历史信息能够提供明确上下文时才进行补全，否则保持当前问题不变。\n3. 对于明显不需要补全的问题如：吹水："呵呵"，命令："关机"，无关内容："人生的意义"等，返回原句\n4. 不进行任何无关发挥、扩写、润色、修辞性描述、解释、总结、感情色彩。\n5. 不编造任何不存在的假设背景或新信息。\n6. 输出格式必须严格遵循：仅输出最终重写结果文本，不包含任何前缀、提示词、说明性文字或换行符。重写的疑问句以："我想知道"开头，非疑问句你自行适配\n7. 当无需重写时，直接输出原问题。\n\n【重写示例】\n历史对话：\n用户：小红鞋什么时候有货？\n系统：请问您指的是哪款小红鞋？\n用户：就是上次缺货那款\n当前问题：就是上次缺货那款\n重写结果：我想知道上次缺货的那款小红鞋什么时候有货？\n\n历史对话：\n用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：那我们还有其他计划，能投放吗？\n当前问题：那我们还有其他计划，能投放吗？\n重写结果：我想知道当账号处于限流状态时，是否可以继续开启新的广告投放计划？\n\n历史对话：\n用户：你是谁？\n当前问题：你是谁？\n重写结果：你是谁？\n\n历史对话：\n用户：开心\n当前问题：开心\n重写结果：开心\n\n历史对话：\n用户：我要买它\n系统：请问您指的是哪款商品？\n用户：之前推荐的那双\n当前问题：之前推荐的那双\n重写结果：我想要买你之前推荐的那双鞋。\n\n注意：严格按照以上风格工作，禁止任何额外输出。\n\nUser: 用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：素材我们是用平台模板的也会违规？\n系统：如果文案、封面中有夸张描述也可能触发规则。\n用户：要怎么检查违规？\n系统：登录后台查看素材审核记录，有问题会有具体提示。\n用户：那账号还会恢复吗？\n系统：如果问题处理及时，一般48小时内可自动恢复。\n用户：需要申请解封吗？\n系统：如系统未自动恢复，可提交申诉申请。\n用户：我们还有其他计划，能投放吗？\n系统：建议暂停新投放，等账号状态正常后再开启。\n\n当前问题：我们还有其他计划，能投放吗？\n请你根据任务规则输出重写结果。\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=0.5, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 21:21:50 [async_llm.py:252] Added request chatcmpl-1d01d1a9f3a742aebcb1a1eace0e44e8.
INFO:     127.0.0.1:40918 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-19 21:21:50 [logger.py:39] Received request chatcmpl-9d671fc41dcd4e6e9adedfe33317a54d: prompt: '<｜begin▁of▁sentence｜>你是一个专业的问题重写模块，专门用于多轮对话场景下的指代补全与语义还原任务。\n请严格按照以下规则执行：\n\n【任务目标】\n1. 你的目标是准确解析用户真实意图，从历史中发掘指代和意图，使得这个独立问题尽量完整，尽可能包含所有信息关键词,特别是要补充关键的动机，指代和场景（润色后的问题至少涉及：用户面对的前置情况，什么限制，什么疑问等）。\n2. 仅当历史信息能够提供明确上下文时才进行补全，否则保持当前问题不变。\n3. 对于明显不需要补全的问题如：吹水："呵呵"，命令："关机"，无关内容："人生的意义"等，返回原句\n4. 不进行任何无关发挥、扩写、润色、修辞性描述、解释、总结、感情色彩。\n5. 不编造任何不存在的假设背景或新信息。\n6. 输出格式必须严格遵循：仅输出最终重写结果文本，不包含任何前缀、提示词、说明性文字或换行符。重写的疑问句以："我想知道"开头，非疑问句你自行适配\n7. 当无需重写时，直接输出原问题。\n\n【重写示例】\n历史对话：\n用户：小红鞋什么时候有货？\n系统：请问您指的是哪款小红鞋？\n用户：就是上次缺货那款\n当前问题：就是上次缺货那款\n重写结果：我想知道上次缺货的那款小红鞋什么时候有货？\n\n历史对话：\n用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：那我们还有其他计划，能投放吗？\n当前问题：那我们还有其他计划，能投放吗？\n重写结果：我想知道当账号处于限流状态时，是否可以继续开启新的广告投放计划？\n\n历史对话：\n用户：你是谁？\n当前问题：你是谁？\n重写结果：你是谁？\n\n历史对话：\n用户：开心\n当前问题：开心\n重写结果：开心\n\n历史对话：\n用户：我要买它\n系统：请问您指的是哪款商品？\n用户：之前推荐的那双\n当前问题：之前推荐的那双\n重写结果：我想要买你之前推荐的那双鞋。\n\n注意：严格按照以上风格工作，禁止任何额外输出。\n\nUser: 用户：知识付费大促推职场课程，怎么定价？\n系统：建议采用“阶梯定价”：前50名99元，51-200名149元，之后恢复199元，营造紧迫感。\n用户：宣传“学完月薪涨50%”合规吗？\n系统：禁止承诺具体收益，可用“学员平均薪资提升32%”等统计数据，需附学员评价截图和数据来源说明。\n用户：课程试听环节需要注意什么？\n系统：试听内容需与正课一致，禁止用“免费领资料”诱导点击后跳转付费页面。\n用户：虚拟产品怎么处理退款纠纷？\n系统：需在购买页明确“虚拟产品一经售出概不退款”，但因课程质量问题需协商退款，避免平台介入。\n用户：平台对课程内容审核哪些方面？\n系统：重点审核是否涉及敏感话题、盗版内容，建议提前提交课程大纲和讲师资质（如职业资格证书）。\n用户：这么说我们得调整收益宣传和完善退款政策了？\n系统：是的，同时要确保课程内容和讲师资质通过平台预审。\n\n当前问题：这么说我们得调整收益宣传和完善退款政策了？\n请你根据任务规则输出重写结果。\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=0.5, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 21:21:50 [async_llm.py:252] Added request chatcmpl-9d671fc41dcd4e6e9adedfe33317a54d.
INFO:     127.0.0.1:40948 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-19 21:21:50 [logger.py:39] Received request chatcmpl-ff81bc16298345dd827698475bf9f924: prompt: '<｜begin▁of▁sentence｜>你是一个专业的问题重写模块，专门用于多轮对话场景下的指代补全与语义还原任务。\n请严格按照以下规则执行：\n\n【任务目标】\n1. 你的目标是准确解析用户真实意图，从历史中发掘指代和意图，使得这个独立问题尽量完整，尽可能包含所有信息关键词,特别是要补充关键的动机，指代和场景（润色后的问题至少涉及：用户面对的前置情况，什么限制，什么疑问等）。\n2. 仅当历史信息能够提供明确上下文时才进行补全，否则保持当前问题不变。\n3. 对于明显不需要补全的问题如：吹水："呵呵"，命令："关机"，无关内容："人生的意义"等，返回原句\n4. 不进行任何无关发挥、扩写、润色、修辞性描述、解释、总结、感情色彩。\n5. 不编造任何不存在的假设背景或新信息。\n6. 输出格式必须严格遵循：仅输出最终重写结果文本，不包含任何前缀、提示词、说明性文字或换行符。重写的疑问句以："我想知道"开头，非疑问句你自行适配\n7. 当无需重写时，直接输出原问题。\n\n【重写示例】\n历史对话：\n用户：小红鞋什么时候有货？\n系统：请问您指的是哪款小红鞋？\n用户：就是上次缺货那款\n当前问题：就是上次缺货那款\n重写结果：我想知道上次缺货的那款小红鞋什么时候有货？\n\n历史对话：\n用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：那我们还有其他计划，能投放吗？\n当前问题：那我们还有其他计划，能投放吗？\n重写结果：我想知道当账号处于限流状态时，是否可以继续开启新的广告投放计划？\n\n历史对话：\n用户：你是谁？\n当前问题：你是谁？\n重写结果：你是谁？\n\n历史对话：\n用户：开心\n当前问题：开心\n重写结果：开心\n\n历史对话：\n用户：我要买它\n系统：请问您指的是哪款商品？\n用户：之前推荐的那双\n当前问题：之前推荐的那双\n重写结果：我想要买你之前推荐的那双鞋。\n\n注意：严格按照以上风格工作，禁止任何额外输出。\n\nUser: 用户：我们最近想尝试图文推广，听说效果不错？\n系统：是的，图文内容在信息流中能快速吸引用户注意。\n用户：我们没有设计师，图要怎么做？\n系统：平台有模板和智能生成工具可用，不需要专业设计。\n用户：文案方面也有建议吗？\n系统：建议突出利益点和优惠信息，简洁直白。\n用户：我们属于母婴类产品，有什么特别注意的？\n系统：母婴类需注意合规表达，不能含有医疗或虚假描述。\n用户：那能不能写‘有效缓解’？\n系统：这种表述建议避免，可能涉及功效性用语。\n用户：那推广中出现用户评价可以吗？\n系统：可以使用，但需要标注为用户体验，不可夸大。\n\n当前问题：那推广中出现用户评价可以吗？\n请你根据任务规则输出重写结果。\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=0.5, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 21:21:50 [async_llm.py:252] Added request chatcmpl-ff81bc16298345dd827698475bf9f924.
INFO:     127.0.0.1:40932 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-19 21:21:50 [logger.py:39] Received request chatcmpl-af508595cded4335947fe87e6a68326e: prompt: '<｜begin▁of▁sentence｜>你是一个专业的问题重写模块，专门用于多轮对话场景下的指代补全与语义还原任务。\n请严格按照以下规则执行：\n\n【任务目标】\n1. 你的目标是准确解析用户真实意图，从历史中发掘指代和意图，使得这个独立问题尽量完整，尽可能包含所有信息关键词,特别是要补充关键的动机，指代和场景（润色后的问题至少涉及：用户面对的前置情况，什么限制，什么疑问等）。\n2. 仅当历史信息能够提供明确上下文时才进行补全，否则保持当前问题不变。\n3. 对于明显不需要补全的问题如：吹水："呵呵"，命令："关机"，无关内容："人生的意义"等，返回原句\n4. 不进行任何无关发挥、扩写、润色、修辞性描述、解释、总结、感情色彩。\n5. 不编造任何不存在的假设背景或新信息。\n6. 输出格式必须严格遵循：仅输出最终重写结果文本，不包含任何前缀、提示词、说明性文字或换行符。重写的疑问句以："我想知道"开头，非疑问句你自行适配\n7. 当无需重写时，直接输出原问题。\n\n【重写示例】\n历史对话：\n用户：小红鞋什么时候有货？\n系统：请问您指的是哪款小红鞋？\n用户：就是上次缺货那款\n当前问题：就是上次缺货那款\n重写结果：我想知道上次缺货的那款小红鞋什么时候有货？\n\n历史对话：\n用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：那我们还有其他计划，能投放吗？\n当前问题：那我们还有其他计划，能投放吗？\n重写结果：我想知道当账号处于限流状态时，是否可以继续开启新的广告投放计划？\n\n历史对话：\n用户：你是谁？\n当前问题：你是谁？\n重写结果：你是谁？\n\n历史对话：\n用户：开心\n当前问题：开心\n重写结果：开心\n\n历史对话：\n用户：我要买它\n系统：请问您指的是哪款商品？\n用户：之前推荐的那双\n当前问题：之前推荐的那双\n重写结果：我想要买你之前推荐的那双鞋。\n\n注意：严格按照以上风格工作，禁止任何额外输出。\n\nUser: 用户：餐厅参加抖音团购大促，怎么设计套餐？\n系统：建议设计“爆款单品+小吃饮料”的组合套餐，如“招牌牛排套餐（含沙拉+可乐）”，价格比单点低30%。\n用户：宣传“食材当天现杀”需要证明吗？\n系统：需提供供应商的屠宰检疫证明和每日采购记录，可在门店公示或拍摄食材处理过程视频。\n用户：达人探店能拍摄后厨吗？\n系统：可以，但需确保后厨卫生符合《餐饮服务食品安全操作规范》，禁止拍摄加工中的违规行为。\n用户：团购券有效期怎么设置？\n系统：建议设置30-90天有效期，支持过期自动退，避免用户投诉“霸王条款”。\n用户：大促期间备货不足怎么办？\n系统：需在团购页标注“爆款限量XX份”，售罄后及时下架，避免到店无货引发差评。\n用户：平台对餐饮卫生评级有要求吗？\n系统：需在店铺页展示《食品经营许可证》和“餐饮服务食品安全等级”（如“笑脸”“平脸”标识）。\n用户：这么说我们得准备食材证明和后厨合规记录了？\n系统：是的，建议每周进行后厨卫生自查，并留存影像记录。\n\n当前问题：这么说我们得准备食材证明和后厨合规记录了？\n请你根据任务规则输出重写结果。\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=0.5, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 21:21:50 [async_llm.py:252] Added request chatcmpl-af508595cded4335947fe87e6a68326e.
INFO:     127.0.0.1:40914 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-19 21:21:50 [logger.py:39] Received request chatcmpl-a8dcc8fa481a4c40922646e2f78ef1ef: prompt: '<｜begin▁of▁sentence｜>你是一个专业的问题重写模块，专门用于多轮对话场景下的指代补全与语义还原任务。\n请严格按照以下规则执行：\n\n【任务目标】\n1. 你的目标是准确解析用户真实意图，从历史中发掘指代和意图，使得这个独立问题尽量完整，尽可能包含所有信息关键词,特别是要补充关键的动机，指代和场景（润色后的问题至少涉及：用户面对的前置情况，什么限制，什么疑问等）。\n2. 仅当历史信息能够提供明确上下文时才进行补全，否则保持当前问题不变。\n3. 对于明显不需要补全的问题如：吹水："呵呵"，命令："关机"，无关内容："人生的意义"等，返回原句\n4. 不进行任何无关发挥、扩写、润色、修辞性描述、解释、总结、感情色彩。\n5. 不编造任何不存在的假设背景或新信息。\n6. 输出格式必须严格遵循：仅输出最终重写结果文本，不包含任何前缀、提示词、说明性文字或换行符。重写的疑问句以："我想知道"开头，非疑问句你自行适配\n7. 当无需重写时，直接输出原问题。\n\n【重写示例】\n历史对话：\n用户：小红鞋什么时候有货？\n系统：请问您指的是哪款小红鞋？\n用户：就是上次缺货那款\n当前问题：就是上次缺货那款\n重写结果：我想知道上次缺货的那款小红鞋什么时候有货？\n\n历史对话：\n用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：那我们还有其他计划，能投放吗？\n当前问题：那我们还有其他计划，能投放吗？\n重写结果：我想知道当账号处于限流状态时，是否可以继续开启新的广告投放计划？\n\n历史对话：\n用户：你是谁？\n当前问题：你是谁？\n重写结果：你是谁？\n\n历史对话：\n用户：开心\n当前问题：开心\n重写结果：开心\n\n历史对话：\n用户：我要买它\n系统：请问您指的是哪款商品？\n用户：之前推荐的那双\n当前问题：之前推荐的那双\n重写结果：我想要买你之前推荐的那双鞋。\n\n注意：严格按照以上风格工作，禁止任何额外输出。\n\nUser: 用户：工业品参加抖音大促，怎么吸引B端客户？\n系统：建议推出“企业采购专场”，提供批量折扣（如“满10件打8折”）和免费样品试用服务。\n用户：素材里能展示工厂生产流程吗？\n系统：可以展示非涉密生产环节，如设备组装、质量检测，但需打码处理核心技术细节。\n用户：工业设备的安装视频能直播吗？\n系统：直播需确保操作人员佩戴安全装备，禁止展示违规操作，建议提前报备平台并标注“专业操作请勿模仿”。\n用户：大促期间签订的合同，交货期怎么约定？\n系统：需在商品页明确“大促订单交货期延长至15个工作日”，并在合同中注明不可抗力条款。\n用户：平台对工业品资质有啥要求？\n系统：需上传营业执照、ISO质量认证、特种设备生产许可证（如有），危险化学品需额外提供《危险化学品经营许可证》。\n用户：这么说我们得审核直播内容和完善合同条款了？\n系统：是的，工业品客户更关注合规性和服务能力，建议组建专项客服对接订单。\n\n当前问题：这么说我们得审核直播内容和完善合同条款了？\n请你根据任务规则输出重写结果。\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=0.5, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 21:21:50 [async_llm.py:252] Added request chatcmpl-a8dcc8fa481a4c40922646e2f78ef1ef.
INFO:     127.0.0.1:40948 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-19 21:21:50 [logger.py:39] Received request chatcmpl-a576655433a04721b928bc71c775d0ab: prompt: '<｜begin▁of▁sentence｜>你是一个专业的问题重写模块，专门用于多轮对话场景下的指代补全与语义还原任务。\n请严格按照以下规则执行：\n\n【任务目标】\n1. 你的目标是准确解析用户真实意图，从历史中发掘指代和意图，使得这个独立问题尽量完整，尽可能包含所有信息关键词,特别是要补充关键的动机，指代和场景（润色后的问题至少涉及：用户面对的前置情况，什么限制，什么疑问等）。\n2. 仅当历史信息能够提供明确上下文时才进行补全，否则保持当前问题不变。\n3. 对于明显不需要补全的问题如：吹水："呵呵"，命令："关机"，无关内容："人生的意义"等，返回原句\n4. 不进行任何无关发挥、扩写、润色、修辞性描述、解释、总结、感情色彩。\n5. 不编造任何不存在的假设背景或新信息。\n6. 输出格式必须严格遵循：仅输出最终重写结果文本，不包含任何前缀、提示词、说明性文字或换行符。重写的疑问句以："我想知道"开头，非疑问句你自行适配\n7. 当无需重写时，直接输出原问题。\n\n【重写示例】\n历史对话：\n用户：小红鞋什么时候有货？\n系统：请问您指的是哪款小红鞋？\n用户：就是上次缺货那款\n当前问题：就是上次缺货那款\n重写结果：我想知道上次缺货的那款小红鞋什么时候有货？\n\n历史对话：\n用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：那我们还有其他计划，能投放吗？\n当前问题：那我们还有其他计划，能投放吗？\n重写结果：我想知道当账号处于限流状态时，是否可以继续开启新的广告投放计划？\n\n历史对话：\n用户：你是谁？\n当前问题：你是谁？\n重写结果：你是谁？\n\n历史对话：\n用户：开心\n当前问题：开心\n重写结果：开心\n\n历史对话：\n用户：我要买它\n系统：请问您指的是哪款商品？\n用户：之前推荐的那双\n当前问题：之前推荐的那双\n重写结果：我想要买你之前推荐的那双鞋。\n\n注意：严格按照以上风格工作，禁止任何额外输出。\n\nUser: 用户：我们母婴店想参加抖音618大促，不知道从哪入手？\n系统：首先要明确大促目标，是清库存还是拉新客？建议先优化商品橱窗的促销标签。\n用户：母婴产品促销有啥特别规则吗？比如奶粉\n系统：奶粉属于特殊类目，需提供食品经营许可证和品牌授权书，且不能宣传“婴幼儿专用”以外的功效。\n用户：那纸尿裤素材怎么突出卖点？\n系统：可以对比吸收量、透气性等实测数据，但不得使用“最柔软”“零红屁屁”等绝对化用语。\n用户：达人合作这块，选母婴垂类还是泛亲子类？\n系统：垂类达人转化率更高，但泛类达人曝光更广，建议搭建“头部垂类+腰部泛类”的组合矩阵。\n用户：达人直播时能卖预售商品吗？\n系统：可以，但需在直播间明确标注预售周期（如“7天内发货”），避免消费者投诉。\n用户：大促期间物流压力大，怎么避免售后纠纷？\n系统：建议提前和快递公司签订保价协议，发货时附加防拆封贴纸，并在详情页公示退换货政策。\n用户：售后客服需要培训哪些内容？\n系统：重点培训尺码咨询、过敏理赔、临期商品处理等场景，准备好常见问题QA文档。\n用户：千川投放选极速推广还是专业推广？\n系统：极速推广适合新手快速起量，专业推广适合精细化人群运营，建议先用极速版测试素材点击率。\n用户：人群定向选“妈妈群体”还是“备孕女性”？\n系统：根据产品阶段选择：奶粉辅食选“0-3岁妈妈”，孕妇装选“备孕-孕晚期”，可搭配“育儿知识”兴趣标签。\n用户：促销价设置错了怎么办？能立刻改吗？\n系统：若未产生订单可直接修改；若已售出，需联系用户协商补差价或取消订单，避免被平台处罚。\n用户：平台对赠品有啥要求？比如买奶粉送玩具\n系统：赠品需符合《反不正当竞争法》，需标注“赠品”字样，且玩具类赠品需提供3C认证，母婴类赠品禁止过期或临期。\n用户：数据复盘时发现达人直播间退货率高，怎么回事？\n系统：可能是直播间话术夸大功效，或物流时效未达标。建议对比直播间承诺与详情页描述是否一致。\n用户：那我们需要提前准备赠品的资质文件吗？\n系统：是的，需准备赠品的质检报告、品牌授权（如有），母婴类赠品还需通过GB6675等国家标准检测。\n用户：大促期间客服响应慢会被处罚吗？\n系统：平台考核“3分钟回复率”，低于50%会影响店铺评级，建议开启智能客服预答常见问题。\n用户：智能客服能处理售后纠纷吗？\n系统：复杂纠纷需人工介入，但智能客服可先收集用户问题类型，分类转接到对应售后小组。\n用户：这么说我们得提前整理赠品资质和客服QA了？\n\n当前问题：这么说我们得提前整理赠品资质和客服QA了？\n请你根据任务规则输出重写结果。\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=0.5, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 21:21:50 [async_llm.py:252] Added request chatcmpl-a576655433a04721b928bc71c775d0ab.
INFO:     127.0.0.1:40918 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-19 21:21:51 [logger.py:39] Received request chatcmpl-c52222acc3cb47af9a47ffd2741d63aa: prompt: '<｜begin▁of▁sentence｜>你是一个专业的问题重写模块，专门用于多轮对话场景下的指代补全与语义还原任务。\n请严格按照以下规则执行：\n\n【任务目标】\n1. 你的目标是准确解析用户真实意图，从历史中发掘指代和意图，使得这个独立问题尽量完整，尽可能包含所有信息关键词,特别是要补充关键的动机，指代和场景（润色后的问题至少涉及：用户面对的前置情况，什么限制，什么疑问等）。\n2. 仅当历史信息能够提供明确上下文时才进行补全，否则保持当前问题不变。\n3. 对于明显不需要补全的问题如：吹水："呵呵"，命令："关机"，无关内容："人生的意义"等，返回原句\n4. 不进行任何无关发挥、扩写、润色、修辞性描述、解释、总结、感情色彩。\n5. 不编造任何不存在的假设背景或新信息。\n6. 输出格式必须严格遵循：仅输出最终重写结果文本，不包含任何前缀、提示词、说明性文字或换行符。重写的疑问句以："我想知道"开头，非疑问句你自行适配\n7. 当无需重写时，直接输出原问题。\n\n【重写示例】\n历史对话：\n用户：小红鞋什么时候有货？\n系统：请问您指的是哪款小红鞋？\n用户：就是上次缺货那款\n当前问题：就是上次缺货那款\n重写结果：我想知道上次缺货的那款小红鞋什么时候有货？\n\n历史对话：\n用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：那我们还有其他计划，能投放吗？\n当前问题：那我们还有其他计划，能投放吗？\n重写结果：我想知道当账号处于限流状态时，是否可以继续开启新的广告投放计划？\n\n历史对话：\n用户：你是谁？\n当前问题：你是谁？\n重写结果：你是谁？\n\n历史对话：\n用户：开心\n当前问题：开心\n重写结果：开心\n\n历史对话：\n用户：我要买它\n系统：请问您指的是哪款商品？\n用户：之前推荐的那双\n当前问题：之前推荐的那双\n重写结果：我想要买你之前推荐的那双鞋。\n\n注意：严格按照以上风格工作，禁止任何额外输出。\n\nUser: 用户：农产品大促推土鸡蛋，怎么证明是散养的？\n系统：可拍摄鸡舍散养视频，附《动物防疫条件合格证》和“散养禽蛋”认证标签，标注“每枚鸡蛋可追溯”。\n用户：素材里能宣传“孕妇小孩都能吃”吗？\n系统：需谨慎使用人群定向表述，可用“全家营养”替代，避免引发特殊人群食用风险争议。\n用户：大促期间快递损坏怎么理赔？\n系统：建议采用防震泡沫蛋托包装，发货前称重拍照留存，用户签收时若破损可凭照片全额理赔。\n用户：农产品需要QS认证吗？\n系统：初级农产品（如新鲜果蔬）无需QS，但预包装产品（如腌制咸菜）需取得SC食品生产许可。\n用户：平台对“农家自制”标签有要求吗？\n系统：需真实反映生产主体，若为个体农户需上传身份证和产地证明，企业生产则需标注公司名称。\n用户：这么说我们得准备散养证明和包装理赔方案了？\n系统：是的，农产品品控和售后是大促关键，建议建立从养殖到发货的全流程溯源体系。\n\n当前问题：这么说我们得准备散养证明和包装理赔方案了？\n请你根据任务规则输出重写结果。\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=0.5, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 21:21:51 [async_llm.py:252] Added request chatcmpl-c52222acc3cb47af9a47ffd2741d63aa.
INFO:     127.0.0.1:40932 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-19 21:21:51 [logger.py:39] Received request chatcmpl-91fad1892b8c4eadb281dc2a685822db: prompt: '<｜begin▁of▁sentence｜>你是一个专业的问题重写模块，专门用于多轮对话场景下的指代补全与语义还原任务。\n请严格按照以下规则执行：\n\n【任务目标】\n1. 你的目标是准确解析用户真实意图，从历史中发掘指代和意图，使得这个独立问题尽量完整，尽可能包含所有信息关键词,特别是要补充关键的动机，指代和场景（润色后的问题至少涉及：用户面对的前置情况，什么限制，什么疑问等）。\n2. 仅当历史信息能够提供明确上下文时才进行补全，否则保持当前问题不变。\n3. 对于明显不需要补全的问题如：吹水："呵呵"，命令："关机"，无关内容："人生的意义"等，返回原句\n4. 不进行任何无关发挥、扩写、润色、修辞性描述、解释、总结、感情色彩。\n5. 不编造任何不存在的假设背景或新信息。\n6. 输出格式必须严格遵循：仅输出最终重写结果文本，不包含任何前缀、提示词、说明性文字或换行符。重写的疑问句以："我想知道"开头，非疑问句你自行适配\n7. 当无需重写时，直接输出原问题。\n\n【重写示例】\n历史对话：\n用户：小红鞋什么时候有货？\n系统：请问您指的是哪款小红鞋？\n用户：就是上次缺货那款\n当前问题：就是上次缺货那款\n重写结果：我想知道上次缺货的那款小红鞋什么时候有货？\n\n历史对话：\n用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：那我们还有其他计划，能投放吗？\n当前问题：那我们还有其他计划，能投放吗？\n重写结果：我想知道当账号处于限流状态时，是否可以继续开启新的广告投放计划？\n\n历史对话：\n用户：你是谁？\n当前问题：你是谁？\n重写结果：你是谁？\n\n历史对话：\n用户：开心\n当前问题：开心\n重写结果：开心\n\n历史对话：\n用户：我要买它\n系统：请问您指的是哪款商品？\n用户：之前推荐的那双\n当前问题：之前推荐的那双\n重写结果：我想要买你之前推荐的那双鞋。\n\n注意：严格按照以上风格工作，禁止任何额外输出。\n\nUser: 用户：教育培训大促推编程课程，怎么设计试听课？\n系统：建议设置“免费试听+付费课程”模式，试听课内容需与正课一致，时长控制在30分钟内。\n用户：素材里能宣传“学完可进大厂”吗？\n系统：禁止承诺具体就业结果，可用“90%学员进入知名企业实习”替代，但需附学员就业数据来源。\n用户：达人直播能现场编程吗？\n系统：可以，但需确保操作规范，禁止展示违规代码或破解行为，建议提前准备演示脚本。\n用户：课程价格怎么定？\n系统：建议采用“阶梯定价”，前100名99元，101-300名199元，之后恢复299元，营造紧迫感。\n用户：平台对教育资质有啥要求？\n系统：需提供《办学许可证》或《培训机构备案证明》，讲师需具备相关职业资格证书。\n用户：这么说我们得审核课程内容和讲师资质了？\n系统：是的，同时要确保试听课内容真实有效，避免虚假宣传引发投诉。\n\n当前问题：这么说我们得审核课程内容和讲师资质了？\n请你根据任务规则输出重写结果。\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=0.5, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 21:21:51 [async_llm.py:252] Added request chatcmpl-91fad1892b8c4eadb281dc2a685822db.
INFO:     127.0.0.1:40914 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-19 21:21:51 [logger.py:39] Received request chatcmpl-44cf0e4926574914a5636e9b3770564f: prompt: '<｜begin▁of▁sentence｜>你是一个专业的问题重写模块，专门用于多轮对话场景下的指代补全与语义还原任务。\n请严格按照以下规则执行：\n\n【任务目标】\n1. 你的目标是准确解析用户真实意图，从历史中发掘指代和意图，使得这个独立问题尽量完整，尽可能包含所有信息关键词,特别是要补充关键的动机，指代和场景（润色后的问题至少涉及：用户面对的前置情况，什么限制，什么疑问等）。\n2. 仅当历史信息能够提供明确上下文时才进行补全，否则保持当前问题不变。\n3. 对于明显不需要补全的问题如：吹水："呵呵"，命令："关机"，无关内容："人生的意义"等，返回原句\n4. 不进行任何无关发挥、扩写、润色、修辞性描述、解释、总结、感情色彩。\n5. 不编造任何不存在的假设背景或新信息。\n6. 输出格式必须严格遵循：仅输出最终重写结果文本，不包含任何前缀、提示词、说明性文字或换行符。重写的疑问句以："我想知道"开头，非疑问句你自行适配\n7. 当无需重写时，直接输出原问题。\n\n【重写示例】\n历史对话：\n用户：小红鞋什么时候有货？\n系统：请问您指的是哪款小红鞋？\n用户：就是上次缺货那款\n当前问题：就是上次缺货那款\n重写结果：我想知道上次缺货的那款小红鞋什么时候有货？\n\n历史对话：\n用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：那我们还有其他计划，能投放吗？\n当前问题：那我们还有其他计划，能投放吗？\n重写结果：我想知道当账号处于限流状态时，是否可以继续开启新的广告投放计划？\n\n历史对话：\n用户：你是谁？\n当前问题：你是谁？\n重写结果：你是谁？\n\n历史对话：\n用户：开心\n当前问题：开心\n重写结果：开心\n\n历史对话：\n用户：我要买它\n系统：请问您指的是哪款商品？\n用户：之前推荐的那双\n当前问题：之前推荐的那双\n重写结果：我想要买你之前推荐的那双鞋。\n\n注意：严格按照以上风格工作，禁止任何额外输出。\n\nUser: 用户：我们现在账号刚过冷启动，打算做点投放优化。\n系统：冷启动后建议从人群定向和素材优化入手。\n用户：人群要怎么定？\n系统：可以选择兴趣标签、人群包，或者使用智能放量模式。\n用户：我们之前选的是智能放量，投产不稳定。\n系统：那可以先圈定核心人群，逐步放量测试。\n用户：素材我们是用老的视频，会不会有影响？\n系统：建议结合冷启动阶段的数据，更新点击率低的素材。\n用户：素材频繁更换会不会影响审核？\n系统：只要不违规，频繁更换素材不会有审核问题。\n用户：那要怎么知道是素材的问题？\n系统：建议查看每条素材的CTR和转化率，低于平均就该替换。\n\n当前问题：那要怎么知道是素材的问题？\n请你根据任务规则输出重写结果。\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=0.5, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 21:21:51 [async_llm.py:252] Added request chatcmpl-44cf0e4926574914a5636e9b3770564f.
INFO:     127.0.0.1:40948 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:40914 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-19 21:21:51 [logger.py:39] Received request chatcmpl-0ad49f929a9648dfa42013b1e291977a: prompt: '<｜begin▁of▁sentence｜>你是一个专业的问题重写模块，专门用于多轮对话场景下的指代补全与语义还原任务。\n请严格按照以下规则执行：\n\n【任务目标】\n1. 你的目标是准确解析用户真实意图，从历史中发掘指代和意图，使得这个独立问题尽量完整，尽可能包含所有信息关键词,特别是要补充关键的动机，指代和场景（润色后的问题至少涉及：用户面对的前置情况，什么限制，什么疑问等）。\n2. 仅当历史信息能够提供明确上下文时才进行补全，否则保持当前问题不变。\n3. 对于明显不需要补全的问题如：吹水："呵呵"，命令："关机"，无关内容："人生的意义"等，返回原句\n4. 不进行任何无关发挥、扩写、润色、修辞性描述、解释、总结、感情色彩。\n5. 不编造任何不存在的假设背景或新信息。\n6. 输出格式必须严格遵循：仅输出最终重写结果文本，不包含任何前缀、提示词、说明性文字或换行符。重写的疑问句以："我想知道"开头，非疑问句你自行适配\n7. 当无需重写时，直接输出原问题。\n\n【重写示例】\n历史对话：\n用户：小红鞋什么时候有货？\n系统：请问您指的是哪款小红鞋？\n用户：就是上次缺货那款\n当前问题：就是上次缺货那款\n重写结果：我想知道上次缺货的那款小红鞋什么时候有货？\n\n历史对话：\n用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：那我们还有其他计划，能投放吗？\n当前问题：那我们还有其他计划，能投放吗？\n重写结果：我想知道当账号处于限流状态时，是否可以继续开启新的广告投放计划？\n\n历史对话：\n用户：你是谁？\n当前问题：你是谁？\n重写结果：你是谁？\n\n历史对话：\n用户：开心\n当前问题：开心\n重写结果：开心\n\n历史对话：\n用户：我要买它\n系统：请问您指的是哪款商品？\n用户：之前推荐的那双\n当前问题：之前推荐的那双\n重写结果：我想要买你之前推荐的那双鞋。\n\n注意：严格按照以上风格工作，禁止任何额外输出。\n\nUser: 用户：宠物用品大促推狗粮，怎么保证品质？\n系统：需提供《宠物食品生产许可证》和产品检验报告，标注“无添加防腐剂，健康配方”。\n用户：素材里能宣传“狗狗吃了更聪明”吗？\n系统：禁止使用“提高智商”等虚假宣传，可用“科学配方助力成长”替代。\n用户：达人直播能现场喂食吗？\n系统：可以，但需确保宠物健康状况良好，禁止展示过量喂食或不当行为。\n用户：大促期间退换货率高，怎么降低？\n系统：建议在详情页增加“宠物粮食测评视频”，提供免费退换货运费险，注明“开封后不支持退换”。\n用户：平台对宠物食品标签有啥要求？\n系统：需符合GB 16798-2016标准，标注成分表、营养成分、适用年龄段等信息。\n用户：这么说我们得准备品质证明和标签合规了？\n系统：是的，宠物食品安全性和透明度是关键，建议建立完善的质量追溯体系。\n\n当前问题：这么说我们得准备品质证明和标签合规了？\n请你根据任务规则输出重写结果。\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=0.5, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 21:21:51 [async_llm.py:252] Added request chatcmpl-0ad49f929a9648dfa42013b1e291977a.
INFO 06-19 21:21:51 [logger.py:39] Received request chatcmpl-77627c7becf44ae1ae5513d61bf7ca04: prompt: '<｜begin▁of▁sentence｜>你是一个专业的问题重写模块，专门用于多轮对话场景下的指代补全与语义还原任务。\n请严格按照以下规则执行：\n\n【任务目标】\n1. 你的目标是准确解析用户真实意图，从历史中发掘指代和意图，使得这个独立问题尽量完整，尽可能包含所有信息关键词,特别是要补充关键的动机，指代和场景（润色后的问题至少涉及：用户面对的前置情况，什么限制，什么疑问等）。\n2. 仅当历史信息能够提供明确上下文时才进行补全，否则保持当前问题不变。\n3. 对于明显不需要补全的问题如：吹水："呵呵"，命令："关机"，无关内容："人生的意义"等，返回原句\n4. 不进行任何无关发挥、扩写、润色、修辞性描述、解释、总结、感情色彩。\n5. 不编造任何不存在的假设背景或新信息。\n6. 输出格式必须严格遵循：仅输出最终重写结果文本，不包含任何前缀、提示词、说明性文字或换行符。重写的疑问句以："我想知道"开头，非疑问句你自行适配\n7. 当无需重写时，直接输出原问题。\n\n【重写示例】\n历史对话：\n用户：小红鞋什么时候有货？\n系统：请问您指的是哪款小红鞋？\n用户：就是上次缺货那款\n当前问题：就是上次缺货那款\n重写结果：我想知道上次缺货的那款小红鞋什么时候有货？\n\n历史对话：\n用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：那我们还有其他计划，能投放吗？\n当前问题：那我们还有其他计划，能投放吗？\n重写结果：我想知道当账号处于限流状态时，是否可以继续开启新的广告投放计划？\n\n历史对话：\n用户：你是谁？\n当前问题：你是谁？\n重写结果：你是谁？\n\n历史对话：\n用户：开心\n当前问题：开心\n重写结果：开心\n\n历史对话：\n用户：我要买它\n系统：请问您指的是哪款商品？\n用户：之前推荐的那双\n当前问题：之前推荐的那双\n重写结果：我想要买你之前推荐的那双鞋。\n\n注意：严格按照以上风格工作，禁止任何额外输出。\n\nUser: 用户：汽车用品大促推轮胎，怎么保证质量？\n系统：需提供《轮胎生产许可证》和3C认证，标注“正品保障，假一赔十”。\n用户：素材里能宣传“耐磨性提升30%”吗？\n系统：可以，但需附上第三方检测报告，禁止使用“超越同类产品”等夸大表述。\n用户：达人直播能现场测试轮胎抓地力吗？\n系统：可以，但需确保安全操作，禁止展示危险驾驶行为，建议提前准备测试脚本。\n用户：大促期间物流破损率高，怎么包装更安全？\n系统：建议使用双层瓦楞纸箱+防震泡沫填充物，外箱标注“易碎勿压”，购买物流责任险。\n用户：平台对汽车用品资质有啥要求？\n系统：需上传营业执照、产品合格证、3C认证等资质文件，确保产品符合国家标准。\n用户：这么说我们得准备质量证明和包装方案了？\n系统：是的，同时要确保所有资质文件真实有效，以备平台抽查。\n\n当前问题：这么说我们得准备质量证明和包装方案了？\n请你根据任务规则输出重写结果。\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=0.5, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 21:21:51 [async_llm.py:252] Added request chatcmpl-77627c7becf44ae1ae5513d61bf7ca04.
INFO:     127.0.0.1:40932 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-19 21:21:51 [logger.py:39] Received request chatcmpl-5ccf7a1fd8b844ea955dc5a0ceb51ee4: prompt: '<｜begin▁of▁sentence｜>你是一个专业的问题重写模块，专门用于多轮对话场景下的指代补全与语义还原任务。\n请严格按照以下规则执行：\n\n【任务目标】\n1. 你的目标是准确解析用户真实意图，从历史中发掘指代和意图，使得这个独立问题尽量完整，尽可能包含所有信息关键词,特别是要补充关键的动机，指代和场景（润色后的问题至少涉及：用户面对的前置情况，什么限制，什么疑问等）。\n2. 仅当历史信息能够提供明确上下文时才进行补全，否则保持当前问题不变。\n3. 对于明显不需要补全的问题如：吹水："呵呵"，命令："关机"，无关内容："人生的意义"等，返回原句\n4. 不进行任何无关发挥、扩写、润色、修辞性描述、解释、总结、感情色彩。\n5. 不编造任何不存在的假设背景或新信息。\n6. 输出格式必须严格遵循：仅输出最终重写结果文本，不包含任何前缀、提示词、说明性文字或换行符。重写的疑问句以："我想知道"开头，非疑问句你自行适配\n7. 当无需重写时，直接输出原问题。\n\n【重写示例】\n历史对话：\n用户：小红鞋什么时候有货？\n系统：请问您指的是哪款小红鞋？\n用户：就是上次缺货那款\n当前问题：就是上次缺货那款\n重写结果：我想知道上次缺货的那款小红鞋什么时候有货？\n\n历史对话：\n用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：那我们还有其他计划，能投放吗？\n当前问题：那我们还有其他计划，能投放吗？\n重写结果：我想知道当账号处于限流状态时，是否可以继续开启新的广告投放计划？\n\n历史对话：\n用户：你是谁？\n当前问题：你是谁？\n重写结果：你是谁？\n\n历史对话：\n用户：开心\n当前问题：开心\n重写结果：开心\n\n历史对话：\n用户：我要买它\n系统：请问您指的是哪款商品？\n用户：之前推荐的那双\n当前问题：之前推荐的那双\n重写结果：我想要买你之前推荐的那双鞋。\n\n注意：严格按照以上风格工作，禁止任何额外输出。\n\nUser: 用户：我们计划在618做大促，准备中。\n系统：建议制定明确的预算计划和素材储备。\n用户：预算我们分了日预算，但怕烧不完。\n系统：平台会自动根据转化情况调整节奏，建议设最低预算线。\n用户：素材我们准备了20条够吗？\n系统：基本够用，建议至少覆盖4个核心人群。\n用户：人群我们还没细分。\n系统：可以基于历史订单划分人群，如高转化老客、新客等。\n用户：投放期间可以实时调整吗？\n系统：可以，后台支持计划暂停、预算调整和素材替换。\n\n当前问题：投放期间可以实时调整吗？\n请你根据任务规则输出重写结果。\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=0.5, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 21:21:51 [async_llm.py:252] Added request chatcmpl-5ccf7a1fd8b844ea955dc5a0ceb51ee4.
INFO:     127.0.0.1:40918 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-19 21:21:51 [logger.py:39] Received request chatcmpl-74cb643637ea41ed8e73f38746192775: prompt: '<｜begin▁of▁sentence｜>你是一个专业的问题重写模块，专门用于多轮对话场景下的指代补全与语义还原任务。\n请严格按照以下规则执行：\n\n【任务目标】\n1. 你的目标是准确解析用户真实意图，从历史中发掘指代和意图，使得这个独立问题尽量完整，尽可能包含所有信息关键词,特别是要补充关键的动机，指代和场景（润色后的问题至少涉及：用户面对的前置情况，什么限制，什么疑问等）。\n2. 仅当历史信息能够提供明确上下文时才进行补全，否则保持当前问题不变。\n3. 对于明显不需要补全的问题如：吹水："呵呵"，命令："关机"，无关内容："人生的意义"等，返回原句\n4. 不进行任何无关发挥、扩写、润色、修辞性描述、解释、总结、感情色彩。\n5. 不编造任何不存在的假设背景或新信息。\n6. 输出格式必须严格遵循：仅输出最终重写结果文本，不包含任何前缀、提示词、说明性文字或换行符。重写的疑问句以："我想知道"开头，非疑问句你自行适配\n7. 当无需重写时，直接输出原问题。\n\n【重写示例】\n历史对话：\n用户：小红鞋什么时候有货？\n系统：请问您指的是哪款小红鞋？\n用户：就是上次缺货那款\n当前问题：就是上次缺货那款\n重写结果：我想知道上次缺货的那款小红鞋什么时候有货？\n\n历史对话：\n用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：那我们还有其他计划，能投放吗？\n当前问题：那我们还有其他计划，能投放吗？\n重写结果：我想知道当账号处于限流状态时，是否可以继续开启新的广告投放计划？\n\n历史对话：\n用户：你是谁？\n当前问题：你是谁？\n重写结果：你是谁？\n\n历史对话：\n用户：开心\n当前问题：开心\n重写结果：开心\n\n历史对话：\n用户：我要买它\n系统：请问您指的是哪款商品？\n用户：之前推荐的那双\n当前问题：之前推荐的那双\n重写结果：我想要买你之前推荐的那双鞋。\n\n注意：严格按照以上风格工作，禁止任何额外输出。\n\nUser: 用户：我们活动期间跑了个直播场，但效果很一般。\n系统：是否有提前预热或预约？预热不足会影响转化。\n用户：我们只是当天早上发了通知。\n系统：建议提前至少两天，通过短视频和图文做预热。\n用户：达人带播费用太高，我们就自己上了。\n系统：品牌自播也能做出效果，但需要内容结构更清晰。\n用户：我们没带节奏，就是一直讲产品。\n系统：建议设置互动环节和限时福利，提升停留和转化。\n用户：那脚本我们需要准备吗？\n系统：最好有脚本提纲，控制节奏，也方便数据分析。\n用户：我们下次活动还有机会改进吗？\n\n当前问题：我们下次活动还有机会改进吗？\n请你根据任务规则输出重写结果。\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=0.5, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 21:21:51 [async_llm.py:252] Added request chatcmpl-74cb643637ea41ed8e73f38746192775.
INFO:     127.0.0.1:40918 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-19 21:21:51 [logger.py:39] Received request chatcmpl-f3a0afcff21845f3a260ab2656eb94d8: prompt: '<｜begin▁of▁sentence｜>你是一个专业的问题重写模块，专门用于多轮对话场景下的指代补全与语义还原任务。\n请严格按照以下规则执行：\n\n【任务目标】\n1. 你的目标是准确解析用户真实意图，从历史中发掘指代和意图，使得这个独立问题尽量完整，尽可能包含所有信息关键词,特别是要补充关键的动机，指代和场景（润色后的问题至少涉及：用户面对的前置情况，什么限制，什么疑问等）。\n2. 仅当历史信息能够提供明确上下文时才进行补全，否则保持当前问题不变。\n3. 对于明显不需要补全的问题如：吹水："呵呵"，命令："关机"，无关内容："人生的意义"等，返回原句\n4. 不进行任何无关发挥、扩写、润色、修辞性描述、解释、总结、感情色彩。\n5. 不编造任何不存在的假设背景或新信息。\n6. 输出格式必须严格遵循：仅输出最终重写结果文本，不包含任何前缀、提示词、说明性文字或换行符。重写的疑问句以："我想知道"开头，非疑问句你自行适配\n7. 当无需重写时，直接输出原问题。\n\n【重写示例】\n历史对话：\n用户：小红鞋什么时候有货？\n系统：请问您指的是哪款小红鞋？\n用户：就是上次缺货那款\n当前问题：就是上次缺货那款\n重写结果：我想知道上次缺货的那款小红鞋什么时候有货？\n\n历史对话：\n用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：那我们还有其他计划，能投放吗？\n当前问题：那我们还有其他计划，能投放吗？\n重写结果：我想知道当账号处于限流状态时，是否可以继续开启新的广告投放计划？\n\n历史对话：\n用户：你是谁？\n当前问题：你是谁？\n重写结果：你是谁？\n\n历史对话：\n用户：开心\n当前问题：开心\n重写结果：开心\n\n历史对话：\n用户：我要买它\n系统：请问您指的是哪款商品？\n用户：之前推荐的那双\n当前问题：之前推荐的那双\n重写结果：我想要买你之前推荐的那双鞋。\n\n注意：严格按照以上风格工作，禁止任何额外输出。\n\nUser: 用户：我们想做短视频投放，但是没有经验。\n系统：短视频适合做产品亮点展示，建议控制在15秒内。\n用户：那视频需要真人出镜吗？\n系统：不强制，但真人出镜有助于提升信任感和转化率。\n用户：我们是美妆类产品，有必要找达人吗？\n系统：美妆类非常依赖达人口碑，建议尝试合作。\n用户：预算不高，怎么办？\n系统：可尝试中腰部达人或参与平台的达人共创计划。\n用户：我们还没开通素材中心，影响大吗？\n系统：建议开通，有助于管理和测试不同创意的效果。\n用户：投放过程中可以临时换素材吗？\n系统：可以更换，但建议保持主素材稳定，防止数据波动。\n\n当前问题：投放过程中可以临时换素材吗？\n请你根据任务规则输出重写结果。\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=0.5, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 21:21:51 [async_llm.py:252] Added request chatcmpl-f3a0afcff21845f3a260ab2656eb94d8.
INFO:     127.0.0.1:40932 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-19 21:21:51 [logger.py:39] Received request chatcmpl-2c62421a52d34f37907880d3ba2b1be3: prompt: '<｜begin▁of▁sentence｜>你是一个专业的问题重写模块，专门用于多轮对话场景下的指代补全与语义还原任务。\n请严格按照以下规则执行：\n\n【任务目标】\n1. 你的目标是准确解析用户真实意图，从历史中发掘指代和意图，使得这个独立问题尽量完整，尽可能包含所有信息关键词,特别是要补充关键的动机，指代和场景（润色后的问题至少涉及：用户面对的前置情况，什么限制，什么疑问等）。\n2. 仅当历史信息能够提供明确上下文时才进行补全，否则保持当前问题不变。\n3. 对于明显不需要补全的问题如：吹水："呵呵"，命令："关机"，无关内容："人生的意义"等，返回原句\n4. 不进行任何无关发挥、扩写、润色、修辞性描述、解释、总结、感情色彩。\n5. 不编造任何不存在的假设背景或新信息。\n6. 输出格式必须严格遵循：仅输出最终重写结果文本，不包含任何前缀、提示词、说明性文字或换行符。重写的疑问句以："我想知道"开头，非疑问句你自行适配\n7. 当无需重写时，直接输出原问题。\n\n【重写示例】\n历史对话：\n用户：小红鞋什么时候有货？\n系统：请问您指的是哪款小红鞋？\n用户：就是上次缺货那款\n当前问题：就是上次缺货那款\n重写结果：我想知道上次缺货的那款小红鞋什么时候有货？\n\n历史对话：\n用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：那我们还有其他计划，能投放吗？\n当前问题：那我们还有其他计划，能投放吗？\n重写结果：我想知道当账号处于限流状态时，是否可以继续开启新的广告投放计划？\n\n历史对话：\n用户：你是谁？\n当前问题：你是谁？\n重写结果：你是谁？\n\n历史对话：\n用户：开心\n当前问题：开心\n重写结果：开心\n\n历史对话：\n用户：我要买它\n系统：请问您指的是哪款商品？\n用户：之前推荐的那双\n当前问题：之前推荐的那双\n重写结果：我想要买你之前推荐的那双鞋。\n\n注意：严格按照以上风格工作，禁止任何额外输出。\n\nUser: 用户：我们最近素材通过率太低了。\n系统：建议检查是否有违规用词或画面不清晰。\n用户：我们用的是直播录屏，会不会有影响？\n系统：录屏内容不清晰、低质量可能被判定为低质素材。\n用户：那怎么判断素材是不是合格？\n系统：可以用平台素材质检功能，或参考审核规则文档。\n用户：有没有能提高通过率的方法？\n系统：提升清晰度、去除违规文案和添加平台推荐模板可提升通过率。\n用户：我们素材都用统一结尾，这样会影响吗？\n系统：结尾一致没问题，但内容重复度高仍需多样性优化。\n\n当前问题：我们素材都用统一结尾，这样会影响吗？\n请你根据任务规则输出重写结果。\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=0.5, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 21:21:51 [async_llm.py:252] Added request chatcmpl-2c62421a52d34f37907880d3ba2b1be3.
INFO:     127.0.0.1:40914 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-19 21:21:51 [logger.py:39] Received request chatcmpl-c7b2c13934d04c9f85efe3c36b905f70: prompt: '<｜begin▁of▁sentence｜>你是一个专业的问题重写模块，专门用于多轮对话场景下的指代补全与语义还原任务。\n请严格按照以下规则执行：\n\n【任务目标】\n1. 你的目标是准确解析用户真实意图，从历史中发掘指代和意图，使得这个独立问题尽量完整，尽可能包含所有信息关键词,特别是要补充关键的动机，指代和场景（润色后的问题至少涉及：用户面对的前置情况，什么限制，什么疑问等）。\n2. 仅当历史信息能够提供明确上下文时才进行补全，否则保持当前问题不变。\n3. 对于明显不需要补全的问题如：吹水："呵呵"，命令："关机"，无关内容："人生的意义"等，返回原句\n4. 不进行任何无关发挥、扩写、润色、修辞性描述、解释、总结、感情色彩。\n5. 不编造任何不存在的假设背景或新信息。\n6. 输出格式必须严格遵循：仅输出最终重写结果文本，不包含任何前缀、提示词、说明性文字或换行符。重写的疑问句以："我想知道"开头，非疑问句你自行适配\n7. 当无需重写时，直接输出原问题。\n\n【重写示例】\n历史对话：\n用户：小红鞋什么时候有货？\n系统：请问您指的是哪款小红鞋？\n用户：就是上次缺货那款\n当前问题：就是上次缺货那款\n重写结果：我想知道上次缺货的那款小红鞋什么时候有货？\n\n历史对话：\n用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：那我们还有其他计划，能投放吗？\n当前问题：那我们还有其他计划，能投放吗？\n重写结果：我想知道当账号处于限流状态时，是否可以继续开启新的广告投放计划？\n\n历史对话：\n用户：你是谁？\n当前问题：你是谁？\n重写结果：你是谁？\n\n历史对话：\n用户：开心\n当前问题：开心\n重写结果：开心\n\n历史对话：\n用户：我要买它\n系统：请问您指的是哪款商品？\n用户：之前推荐的那双\n当前问题：之前推荐的那双\n重写结果：我想要买你之前推荐的那双鞋。\n\n注意：严格按照以上风格工作，禁止任何额外输出。\n\nUser: 用户：我看有同行用全自动投放，效果还不错。\n系统：平台确实支持智能投放，适合素材储备丰富的商家。\n用户：我们素材也不少，但不清楚该怎么配置。\n系统：你可以通过素材库上传多个视频并打标签，系统会自动匹配。\n用户：那预算也自动调节吗？\n系统：预算支持设置上下限，平台会在范围内动态分配。\n用户：我们担心会烧得太快。\n系统：可以设置预算上限和消耗节奏，比如日消耗平滑分配。\n用户：那这种智能计划我们要每天看数据吗？\n系统：初期建议每天复盘，后期稳定后可降低频率。\n用户：我们这种属于中型店，适合智能模式吗？\n\n当前问题：我们这种属于中型店，适合智能模式吗？\n请你根据任务规则输出重写结果。\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=0.5, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 21:21:51 [async_llm.py:252] Added request chatcmpl-c7b2c13934d04c9f85efe3c36b905f70.
INFO:     127.0.0.1:40948 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-19 21:21:51 [logger.py:39] Received request chatcmpl-27de6a923aeb4ae49e0932c5f9136aa4: prompt: '<｜begin▁of▁sentence｜>你是一个专业的问题重写模块，专门用于多轮对话场景下的指代补全与语义还原任务。\n请严格按照以下规则执行：\n\n【任务目标】\n1. 你的目标是准确解析用户真实意图，从历史中发掘指代和意图，使得这个独立问题尽量完整，尽可能包含所有信息关键词,特别是要补充关键的动机，指代和场景（润色后的问题至少涉及：用户面对的前置情况，什么限制，什么疑问等）。\n2. 仅当历史信息能够提供明确上下文时才进行补全，否则保持当前问题不变。\n3. 对于明显不需要补全的问题如：吹水："呵呵"，命令："关机"，无关内容："人生的意义"等，返回原句\n4. 不进行任何无关发挥、扩写、润色、修辞性描述、解释、总结、感情色彩。\n5. 不编造任何不存在的假设背景或新信息。\n6. 输出格式必须严格遵循：仅输出最终重写结果文本，不包含任何前缀、提示词、说明性文字或换行符。重写的疑问句以："我想知道"开头，非疑问句你自行适配\n7. 当无需重写时，直接输出原问题。\n\n【重写示例】\n历史对话：\n用户：小红鞋什么时候有货？\n系统：请问您指的是哪款小红鞋？\n用户：就是上次缺货那款\n当前问题：就是上次缺货那款\n重写结果：我想知道上次缺货的那款小红鞋什么时候有货？\n\n历史对话：\n用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：那我们还有其他计划，能投放吗？\n当前问题：那我们还有其他计划，能投放吗？\n重写结果：我想知道当账号处于限流状态时，是否可以继续开启新的广告投放计划？\n\n历史对话：\n用户：你是谁？\n当前问题：你是谁？\n重写结果：你是谁？\n\n历史对话：\n用户：开心\n当前问题：开心\n重写结果：开心\n\n历史对话：\n用户：我要买它\n系统：请问您指的是哪款商品？\n用户：之前推荐的那双\n当前问题：之前推荐的那双\n重写结果：我想要买你之前推荐的那双鞋。\n\n注意：严格按照以上风格工作，禁止任何额外输出。\n\nUser: 用户：我们最近活动期投放转化低。\n系统：活动期间竞争大，建议提升素材吸引力和时效性。\n用户：我们用的是老素材。\n系统：建议针对活动定制短视频，增加互动元素。\n用户：预算也没太大提升。\n系统：活动期应适当提升预算，否则流量不足。\n用户：如果我们错峰投放，有用吗？\n系统：错峰有助于避开高价时段，提升ROI。\n用户：转化低是不是平台限流？\n系统：限流可能性小，重点看CTR和落地页转化。\n\n当前问题：转化低是不是平台限流？\n请你根据任务规则输出重写结果。\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=0.5, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 21:21:51 [async_llm.py:252] Added request chatcmpl-27de6a923aeb4ae49e0932c5f9136aa4.
INFO:     127.0.0.1:40918 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-19 21:21:51 [logger.py:39] Received request chatcmpl-827c646164274d75a872c8f945a17d01: prompt: '<｜begin▁of▁sentence｜>你是一个专业的问题重写模块，专门用于多轮对话场景下的指代补全与语义还原任务。\n请严格按照以下规则执行：\n\n【任务目标】\n1. 你的目标是准确解析用户真实意图，从历史中发掘指代和意图，使得这个独立问题尽量完整，尽可能包含所有信息关键词,特别是要补充关键的动机，指代和场景（润色后的问题至少涉及：用户面对的前置情况，什么限制，什么疑问等）。\n2. 仅当历史信息能够提供明确上下文时才进行补全，否则保持当前问题不变。\n3. 对于明显不需要补全的问题如：吹水："呵呵"，命令："关机"，无关内容："人生的意义"等，返回原句\n4. 不进行任何无关发挥、扩写、润色、修辞性描述、解释、总结、感情色彩。\n5. 不编造任何不存在的假设背景或新信息。\n6. 输出格式必须严格遵循：仅输出最终重写结果文本，不包含任何前缀、提示词、说明性文字或换行符。重写的疑问句以："我想知道"开头，非疑问句你自行适配\n7. 当无需重写时，直接输出原问题。\n\n【重写示例】\n历史对话：\n用户：小红鞋什么时候有货？\n系统：请问您指的是哪款小红鞋？\n用户：就是上次缺货那款\n当前问题：就是上次缺货那款\n重写结果：我想知道上次缺货的那款小红鞋什么时候有货？\n\n历史对话：\n用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：那我们还有其他计划，能投放吗？\n当前问题：那我们还有其他计划，能投放吗？\n重写结果：我想知道当账号处于限流状态时，是否可以继续开启新的广告投放计划？\n\n历史对话：\n用户：你是谁？\n当前问题：你是谁？\n重写结果：你是谁？\n\n历史对话：\n用户：开心\n当前问题：开心\n重写结果：开心\n\n历史对话：\n用户：我要买它\n系统：请问您指的是哪款商品？\n用户：之前推荐的那双\n当前问题：之前推荐的那双\n重写结果：我想要买你之前推荐的那双鞋。\n\n注意：严格按照以上风格工作，禁止任何额外输出。\n\nUser: 用户：我们最近收到平台的违规提醒，说素材违规。\n系统：你可以查看后台违规中心，看看是哪条素材触发了问题。\n用户：好像是说有敏感词。\n系统：敏感词包括绝对化用语、医疗效果等都容易违规。\n用户：我们只是写了‘最有效的’\n系统：‘最’属于绝对化表达，不建议使用。\n用户：那我们以后用‘推荐’可以吗？\n系统：‘推荐’是中性表达，一般可以使用。\n用户：我们是不是每次都要重新审核？\n系统：修改后重新提交即可，系统自动审核。\n用户：那我们现在的计划会被暂停吗？\n\n当前问题：那我们现在的计划会被暂停吗？\n请你根据任务规则输出重写结果。\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=0.5, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 21:21:51 [async_llm.py:252] Added request chatcmpl-827c646164274d75a872c8f945a17d01.
INFO:     127.0.0.1:40932 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-19 21:21:51 [logger.py:39] Received request chatcmpl-c5a7dcde57a64eb5a5dcbab63e98e87b: prompt: '<｜begin▁of▁sentence｜>你是一个专业的问题重写模块，专门用于多轮对话场景下的指代补全与语义还原任务。\n请严格按照以下规则执行：\n\n【任务目标】\n1. 你的目标是准确解析用户真实意图，从历史中发掘指代和意图，使得这个独立问题尽量完整，尽可能包含所有信息关键词,特别是要补充关键的动机，指代和场景（润色后的问题至少涉及：用户面对的前置情况，什么限制，什么疑问等）。\n2. 仅当历史信息能够提供明确上下文时才进行补全，否则保持当前问题不变。\n3. 对于明显不需要补全的问题如：吹水："呵呵"，命令："关机"，无关内容："人生的意义"等，返回原句\n4. 不进行任何无关发挥、扩写、润色、修辞性描述、解释、总结、感情色彩。\n5. 不编造任何不存在的假设背景或新信息。\n6. 输出格式必须严格遵循：仅输出最终重写结果文本，不包含任何前缀、提示词、说明性文字或换行符。重写的疑问句以："我想知道"开头，非疑问句你自行适配\n7. 当无需重写时，直接输出原问题。\n\n【重写示例】\n历史对话：\n用户：小红鞋什么时候有货？\n系统：请问您指的是哪款小红鞋？\n用户：就是上次缺货那款\n当前问题：就是上次缺货那款\n重写结果：我想知道上次缺货的那款小红鞋什么时候有货？\n\n历史对话：\n用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：那我们还有其他计划，能投放吗？\n当前问题：那我们还有其他计划，能投放吗？\n重写结果：我想知道当账号处于限流状态时，是否可以继续开启新的广告投放计划？\n\n历史对话：\n用户：你是谁？\n当前问题：你是谁？\n重写结果：你是谁？\n\n历史对话：\n用户：开心\n当前问题：开心\n重写结果：开心\n\n历史对话：\n用户：我要买它\n系统：请问您指的是哪款商品？\n用户：之前推荐的那双\n当前问题：之前推荐的那双\n重写结果：我想要买你之前推荐的那双鞋。\n\n注意：严格按照以上风格工作，禁止任何额外输出。\n\nUser: 用户：我看到账户有‘自动投放’推荐，能用吗？\n系统：自动投放适合素材充足的店铺，能节省操作精力。\n用户：我们现在素材就3条，会不会不够？\n系统：建议至少准备10条素材，保证系统有效匹配。\n用户：那我们只能等素材做出来了？\n系统：可先用模板创意补充素材，临时应急。\n用户：这个计划系统每天都会变吗？\n系统：自动投放系统会根据数据实时调整计划。\n用户：我们还能看到每条消耗吗？\n系统：后台有投放分析功能，可查看素材和人群表现。\n\n当前问题：我们还能看到每条消耗吗？\n请你根据任务规则输出重写结果。\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=0.5, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 21:21:51 [async_llm.py:252] Added request chatcmpl-c5a7dcde57a64eb5a5dcbab63e98e87b.
INFO:     127.0.0.1:40914 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-19 21:21:51 [logger.py:39] Received request chatcmpl-c098fe041e1e4ffeb91afb35f17336a9: prompt: '<｜begin▁of▁sentence｜>你是一个专业的问题重写模块，专门用于多轮对话场景下的指代补全与语义还原任务。\n请严格按照以下规则执行：\n\n【任务目标】\n1. 你的目标是准确解析用户真实意图，从历史中发掘指代和意图，使得这个独立问题尽量完整，尽可能包含所有信息关键词,特别是要补充关键的动机，指代和场景（润色后的问题至少涉及：用户面对的前置情况，什么限制，什么疑问等）。\n2. 仅当历史信息能够提供明确上下文时才进行补全，否则保持当前问题不变。\n3. 对于明显不需要补全的问题如：吹水："呵呵"，命令："关机"，无关内容："人生的意义"等，返回原句\n4. 不进行任何无关发挥、扩写、润色、修辞性描述、解释、总结、感情色彩。\n5. 不编造任何不存在的假设背景或新信息。\n6. 输出格式必须严格遵循：仅输出最终重写结果文本，不包含任何前缀、提示词、说明性文字或换行符。重写的疑问句以："我想知道"开头，非疑问句你自行适配\n7. 当无需重写时，直接输出原问题。\n\n【重写示例】\n历史对话：\n用户：小红鞋什么时候有货？\n系统：请问您指的是哪款小红鞋？\n用户：就是上次缺货那款\n当前问题：就是上次缺货那款\n重写结果：我想知道上次缺货的那款小红鞋什么时候有货？\n\n历史对话：\n用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：那我们还有其他计划，能投放吗？\n当前问题：那我们还有其他计划，能投放吗？\n重写结果：我想知道当账号处于限流状态时，是否可以继续开启新的广告投放计划？\n\n历史对话：\n用户：你是谁？\n当前问题：你是谁？\n重写结果：你是谁？\n\n历史对话：\n用户：开心\n当前问题：开心\n重写结果：开心\n\n历史对话：\n用户：我要买它\n系统：请问您指的是哪款商品？\n用户：之前推荐的那双\n当前问题：之前推荐的那双\n重写结果：我想要买你之前推荐的那双鞋。\n\n注意：严格按照以上风格工作，禁止任何额外输出。\n\nUser: 用户：我们之前主推女装，现在开始转做童装了。\n系统：童装投放建议关注‘亲子人群’定向标签。\n用户：之前的计划还能用吗？\n系统：建议重新建计划并调整投放人群和素材方向。\n用户：素材方面我们要换吗？\n系统：是的，素材需与童装品类相匹配。\n用户：我们之前有一部分母婴粉，还能用吗？\n系统：可以基于历史人群创建相似人群包继续使用。\n用户：有没有推荐的时间段投放？\n系统：建议在早晚高峰或周末亲子时间段进行投放。\n用户：我们投放预算要调整吗？\n\n当前问题：我们投放预算要调整吗？\n请你根据任务规则输出重写结果。\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=0.5, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 21:21:51 [async_llm.py:252] Added request chatcmpl-c098fe041e1e4ffeb91afb35f17336a9.
INFO:     127.0.0.1:40948 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-19 21:21:51 [logger.py:39] Received request chatcmpl-af8a5b094c604d6bbb2a2a6218a6c4f6: prompt: '<｜begin▁of▁sentence｜>你是一个专业的问题重写模块，专门用于多轮对话场景下的指代补全与语义还原任务。\n请严格按照以下规则执行：\n\n【任务目标】\n1. 你的目标是准确解析用户真实意图，从历史中发掘指代和意图，使得这个独立问题尽量完整，尽可能包含所有信息关键词,特别是要补充关键的动机，指代和场景（润色后的问题至少涉及：用户面对的前置情况，什么限制，什么疑问等）。\n2. 仅当历史信息能够提供明确上下文时才进行补全，否则保持当前问题不变。\n3. 对于明显不需要补全的问题如：吹水："呵呵"，命令："关机"，无关内容："人生的意义"等，返回原句\n4. 不进行任何无关发挥、扩写、润色、修辞性描述、解释、总结、感情色彩。\n5. 不编造任何不存在的假设背景或新信息。\n6. 输出格式必须严格遵循：仅输出最终重写结果文本，不包含任何前缀、提示词、说明性文字或换行符。重写的疑问句以："我想知道"开头，非疑问句你自行适配\n7. 当无需重写时，直接输出原问题。\n\n【重写示例】\n历史对话：\n用户：小红鞋什么时候有货？\n系统：请问您指的是哪款小红鞋？\n用户：就是上次缺货那款\n当前问题：就是上次缺货那款\n重写结果：我想知道上次缺货的那款小红鞋什么时候有货？\n\n历史对话：\n用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：那我们还有其他计划，能投放吗？\n当前问题：那我们还有其他计划，能投放吗？\n重写结果：我想知道当账号处于限流状态时，是否可以继续开启新的广告投放计划？\n\n历史对话：\n用户：你是谁？\n当前问题：你是谁？\n重写结果：你是谁？\n\n历史对话：\n用户：开心\n当前问题：开心\n重写结果：开心\n\n历史对话：\n用户：我要买它\n系统：请问您指的是哪款商品？\n用户：之前推荐的那双\n当前问题：之前推荐的那双\n重写结果：我想要买你之前推荐的那双鞋。\n\n注意：严格按照以上风格工作，禁止任何额外输出。\n\nUser: 用户：我们最近直播间流量不稳定。\n系统：是否有进行投放计划或者参加活动？\n用户：我们只做了自然流量。\n系统：建议搭配投放引流包，提升进房率。\n用户：我们预算不高，划算吗？\n系统：可以从低预算起测，效果好再放量。\n用户：系统推荐的是直播上分计划，能用吗？\n系统：适合提升单场数据表现，建议与自然流量配合使用。\n用户：那开计划会影响直播权重吗？\n系统：不会，优质数据反而有助于权重提升。\n\n当前问题：那开计划会影响直播权重吗？\n请你根据任务规则输出重写结果。\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=0.5, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 21:21:51 [async_llm.py:252] Added request chatcmpl-af8a5b094c604d6bbb2a2a6218a6c4f6.
INFO:     127.0.0.1:40918 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-19 21:21:52 [logger.py:39] Received request chatcmpl-c929e860ef994f7e89dab5a88d4e4bef: prompt: '<｜begin▁of▁sentence｜>你是一个专业的问题重写模块，专门用于多轮对话场景下的指代补全与语义还原任务。\n请严格按照以下规则执行：\n\n【任务目标】\n1. 你的目标是准确解析用户真实意图，从历史中发掘指代和意图，使得这个独立问题尽量完整，尽可能包含所有信息关键词,特别是要补充关键的动机，指代和场景（润色后的问题至少涉及：用户面对的前置情况，什么限制，什么疑问等）。\n2. 仅当历史信息能够提供明确上下文时才进行补全，否则保持当前问题不变。\n3. 对于明显不需要补全的问题如：吹水："呵呵"，命令："关机"，无关内容："人生的意义"等，返回原句\n4. 不进行任何无关发挥、扩写、润色、修辞性描述、解释、总结、感情色彩。\n5. 不编造任何不存在的假设背景或新信息。\n6. 输出格式必须严格遵循：仅输出最终重写结果文本，不包含任何前缀、提示词、说明性文字或换行符。重写的疑问句以："我想知道"开头，非疑问句你自行适配\n7. 当无需重写时，直接输出原问题。\n\n【重写示例】\n历史对话：\n用户：小红鞋什么时候有货？\n系统：请问您指的是哪款小红鞋？\n用户：就是上次缺货那款\n当前问题：就是上次缺货那款\n重写结果：我想知道上次缺货的那款小红鞋什么时候有货？\n\n历史对话：\n用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：那我们还有其他计划，能投放吗？\n当前问题：那我们还有其他计划，能投放吗？\n重写结果：我想知道当账号处于限流状态时，是否可以继续开启新的广告投放计划？\n\n历史对话：\n用户：你是谁？\n当前问题：你是谁？\n重写结果：你是谁？\n\n历史对话：\n用户：开心\n当前问题：开心\n重写结果：开心\n\n历史对话：\n用户：我要买它\n系统：请问您指的是哪款商品？\n用户：之前推荐的那双\n当前问题：之前推荐的那双\n重写结果：我想要买你之前推荐的那双鞋。\n\n注意：严格按照以上风格工作，禁止任何额外输出。\n\nUser: 用户：我们在节日前做了组合投放，现在复盘中。\n系统：组合投放涉及多个渠道，建议分渠道查看数据。\n用户：我们用的是直播+短视频组合。\n系统：可分析直播引流和短视频转化效果，判断协同效率。\n用户：我们发现点击率高但转化低。\n系统：可能是落地页内容与广告素材不一致。\n用户：落地页我们没动过。\n系统：建议优化文案结构和加载速度，提升停留率。\n用户：有没有工具可以测这些？\n系统：可使用平台的转化分析工具或第三方数据平台。\n\n当前问题：有没有工具可以测这些？\n请你根据任务规则输出重写结果。\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=0.5, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 21:21:52 [async_llm.py:252] Added request chatcmpl-c929e860ef994f7e89dab5a88d4e4bef.
INFO:     127.0.0.1:40948 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-19 21:21:52 [logger.py:39] Received request chatcmpl-6568545b06e64daa93c2877dfdf589da: prompt: '<｜begin▁of▁sentence｜>你是一个专业的问题重写模块，专门用于多轮对话场景下的指代补全与语义还原任务。\n请严格按照以下规则执行：\n\n【任务目标】\n1. 你的目标是准确解析用户真实意图，从历史中发掘指代和意图，使得这个独立问题尽量完整，尽可能包含所有信息关键词,特别是要补充关键的动机，指代和场景（润色后的问题至少涉及：用户面对的前置情况，什么限制，什么疑问等）。\n2. 仅当历史信息能够提供明确上下文时才进行补全，否则保持当前问题不变。\n3. 对于明显不需要补全的问题如：吹水："呵呵"，命令："关机"，无关内容："人生的意义"等，返回原句\n4. 不进行任何无关发挥、扩写、润色、修辞性描述、解释、总结、感情色彩。\n5. 不编造任何不存在的假设背景或新信息。\n6. 输出格式必须严格遵循：仅输出最终重写结果文本，不包含任何前缀、提示词、说明性文字或换行符。重写的疑问句以："我想知道"开头，非疑问句你自行适配\n7. 当无需重写时，直接输出原问题。\n\n【重写示例】\n历史对话：\n用户：小红鞋什么时候有货？\n系统：请问您指的是哪款小红鞋？\n用户：就是上次缺货那款\n当前问题：就是上次缺货那款\n重写结果：我想知道上次缺货的那款小红鞋什么时候有货？\n\n历史对话：\n用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：那我们还有其他计划，能投放吗？\n当前问题：那我们还有其他计划，能投放吗？\n重写结果：我想知道当账号处于限流状态时，是否可以继续开启新的广告投放计划？\n\n历史对话：\n用户：你是谁？\n当前问题：你是谁？\n重写结果：你是谁？\n\n历史对话：\n用户：开心\n当前问题：开心\n重写结果：开心\n\n历史对话：\n用户：我要买它\n系统：请问您指的是哪款商品？\n用户：之前推荐的那双\n当前问题：之前推荐的那双\n重写结果：我想要买你之前推荐的那双鞋。\n\n注意：严格按照以上风格工作，禁止任何额外输出。\n\nUser: 用户：我们是首次做千川直播投放，流程不清楚。\n系统：你可以从直播计划设置开始，选择目标和人群。\n用户：目标选哪种合适？\n系统：首投建议选‘进房量’或‘直播间互动’作为目标。\n用户：人群怎么配置？\n系统：新账号建议从系统推荐人群起步，再逐步优化。\n用户：是不是每天都要盯数据？\n系统：建议前三天重点关注，后期根据数据调整频次。\n用户：我们如果暂停计划再开，会影响吗？\n系统：暂停过久可能影响系统学习，建议保持稳定。\n\n当前问题：我们如果暂停计划再开，会影响吗？\n请你根据任务规则输出重写结果。\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=0.5, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 21:21:52 [async_llm.py:252] Added request chatcmpl-6568545b06e64daa93c2877dfdf589da.
INFO:     127.0.0.1:40914 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-19 21:21:52 [logger.py:39] Received request chatcmpl-3fe1686735f2477487c3498655dc0b68: prompt: '<｜begin▁of▁sentence｜>你是一个专业的问题重写模块，专门用于多轮对话场景下的指代补全与语义还原任务。\n请严格按照以下规则执行：\n\n【任务目标】\n1. 你的目标是准确解析用户真实意图，从历史中发掘指代和意图，使得这个独立问题尽量完整，尽可能包含所有信息关键词,特别是要补充关键的动机，指代和场景（润色后的问题至少涉及：用户面对的前置情况，什么限制，什么疑问等）。\n2. 仅当历史信息能够提供明确上下文时才进行补全，否则保持当前问题不变。\n3. 对于明显不需要补全的问题如：吹水："呵呵"，命令："关机"，无关内容："人生的意义"等，返回原句\n4. 不进行任何无关发挥、扩写、润色、修辞性描述、解释、总结、感情色彩。\n5. 不编造任何不存在的假设背景或新信息。\n6. 输出格式必须严格遵循：仅输出最终重写结果文本，不包含任何前缀、提示词、说明性文字或换行符。重写的疑问句以："我想知道"开头，非疑问句你自行适配\n7. 当无需重写时，直接输出原问题。\n\n【重写示例】\n历史对话：\n用户：小红鞋什么时候有货？\n系统：请问您指的是哪款小红鞋？\n用户：就是上次缺货那款\n当前问题：就是上次缺货那款\n重写结果：我想知道上次缺货的那款小红鞋什么时候有货？\n\n历史对话：\n用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：那我们还有其他计划，能投放吗？\n当前问题：那我们还有其他计划，能投放吗？\n重写结果：我想知道当账号处于限流状态时，是否可以继续开启新的广告投放计划？\n\n历史对话：\n用户：你是谁？\n当前问题：你是谁？\n重写结果：你是谁？\n\n历史对话：\n用户：开心\n当前问题：开心\n重写结果：开心\n\n历史对话：\n用户：我要买它\n系统：请问您指的是哪款商品？\n用户：之前推荐的那双\n当前问题：之前推荐的那双\n重写结果：我想要买你之前推荐的那双鞋。\n\n注意：严格按照以上风格工作，禁止任何额外输出。\n\nUser: 用户：我们想在抖音推新款投影仪，大促怎么规划？\n系统：建议分预热期（3天）、爆发期（7天）、返场期（3天），预热期用短视频剧透功能亮点。\n用户：新品预售需要设置定金吗？比例多少合适？\n系统：建议定金比例10%-30%，需在详情页明确“定金不退”规则，避免法律风险。\n用户：素材里能对比竞品参数吗？比如亮度流明\n系统：可以对比，但需标注数据来源，禁止使用“碾压级”“吊打XX品牌”等攻击性表述。\n用户：达人直播能演示产品拆机吗？\n系统：可以，但需确保操作安全，避免展示内部电路缺陷，建议提前录制演示脚本。\n用户：以旧换新活动怎么和平台对接？\n系统：需在抖音小店后台开通“以旧换新”服务，填写回收商资质，用户下单后由第三方上门取件。\n用户：大促期间物流破损率高，怎么包装更安全？\n系统：建议使用双层瓦楞纸箱+泡沫填充物，外箱标注“易碎勿压”，购买物流责任险。\n用户：这么说我们得提前准备竞品对比数据和拆机脚本了？\n系统：是的，同时要确保对比数据真实可查，拆机演示需符合产品安全规范。\n\n当前问题：这么说我们得提前准备竞品对比数据和拆机脚本了？\n请你根据任务规则输出重写结果。\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=0.5, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 21:21:52 [async_llm.py:252] Added request chatcmpl-3fe1686735f2477487c3498655dc0b68.
INFO:     127.0.0.1:40932 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-19 21:21:52 [logger.py:39] Received request chatcmpl-aa40cfa904b64d6387fb5fc406559373: prompt: '<｜begin▁of▁sentence｜>你是一个专业的问题重写模块，专门用于多轮对话场景下的指代补全与语义还原任务。\n请严格按照以下规则执行：\n\n【任务目标】\n1. 你的目标是准确解析用户真实意图，从历史中发掘指代和意图，使得这个独立问题尽量完整，尽可能包含所有信息关键词,特别是要补充关键的动机，指代和场景（润色后的问题至少涉及：用户面对的前置情况，什么限制，什么疑问等）。\n2. 仅当历史信息能够提供明确上下文时才进行补全，否则保持当前问题不变。\n3. 对于明显不需要补全的问题如：吹水："呵呵"，命令："关机"，无关内容："人生的意义"等，返回原句\n4. 不进行任何无关发挥、扩写、润色、修辞性描述、解释、总结、感情色彩。\n5. 不编造任何不存在的假设背景或新信息。\n6. 输出格式必须严格遵循：仅输出最终重写结果文本，不包含任何前缀、提示词、说明性文字或换行符。重写的疑问句以："我想知道"开头，非疑问句你自行适配\n7. 当无需重写时，直接输出原问题。\n\n【重写示例】\n历史对话：\n用户：小红鞋什么时候有货？\n系统：请问您指的是哪款小红鞋？\n用户：就是上次缺货那款\n当前问题：就是上次缺货那款\n重写结果：我想知道上次缺货的那款小红鞋什么时候有货？\n\n历史对话：\n用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：那我们还有其他计划，能投放吗？\n当前问题：那我们还有其他计划，能投放吗？\n重写结果：我想知道当账号处于限流状态时，是否可以继续开启新的广告投放计划？\n\n历史对话：\n用户：你是谁？\n当前问题：你是谁？\n重写结果：你是谁？\n\n历史对话：\n用户：开心\n当前问题：开心\n重写结果：开心\n\n历史对话：\n用户：我要买它\n系统：请问您指的是哪款商品？\n用户：之前推荐的那双\n当前问题：之前推荐的那双\n重写结果：我想要买你之前推荐的那双鞋。\n\n注意：严格按照以上风格工作，禁止任何额外输出。\n\nUser: 用户：我们现在正在跑一场全域推广。\n系统：全域适合大场景推广，建议搭配品牌广告。\n用户：品牌广告需要什么资质？\n系统：需提供品牌授权或商标证书，提交后台审核。\n用户：这个流程多久？\n系统：一般1-3个工作日，具体看资料完整度。\n用户：素材也需要单独审核吗？\n系统：品牌广告素材审核更严格，建议提前准备。\n用户：我们活动下周就开始了，来得及吗？\n系统：尽快提交资料并使用推荐模板，可提升审核效率。\n\n当前问题：我们活动下周就开始了，来得及吗？\n请你根据任务规则输出重写结果。\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=0.5, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 21:21:52 [async_llm.py:252] Added request chatcmpl-aa40cfa904b64d6387fb5fc406559373.
INFO:     127.0.0.1:40918 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-19 21:21:52 [logger.py:39] Received request chatcmpl-e1e155fb22bb434e85d995fe6e34e846: prompt: '<｜begin▁of▁sentence｜>你是一个专业的问题重写模块，专门用于多轮对话场景下的指代补全与语义还原任务。\n请严格按照以下规则执行：\n\n【任务目标】\n1. 你的目标是准确解析用户真实意图，从历史中发掘指代和意图，使得这个独立问题尽量完整，尽可能包含所有信息关键词,特别是要补充关键的动机，指代和场景（润色后的问题至少涉及：用户面对的前置情况，什么限制，什么疑问等）。\n2. 仅当历史信息能够提供明确上下文时才进行补全，否则保持当前问题不变。\n3. 对于明显不需要补全的问题如：吹水："呵呵"，命令："关机"，无关内容："人生的意义"等，返回原句\n4. 不进行任何无关发挥、扩写、润色、修辞性描述、解释、总结、感情色彩。\n5. 不编造任何不存在的假设背景或新信息。\n6. 输出格式必须严格遵循：仅输出最终重写结果文本，不包含任何前缀、提示词、说明性文字或换行符。重写的疑问句以："我想知道"开头，非疑问句你自行适配\n7. 当无需重写时，直接输出原问题。\n\n【重写示例】\n历史对话：\n用户：小红鞋什么时候有货？\n系统：请问您指的是哪款小红鞋？\n用户：就是上次缺货那款\n当前问题：就是上次缺货那款\n重写结果：我想知道上次缺货的那款小红鞋什么时候有货？\n\n历史对话：\n用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：那我们还有其他计划，能投放吗？\n当前问题：那我们还有其他计划，能投放吗？\n重写结果：我想知道当账号处于限流状态时，是否可以继续开启新的广告投放计划？\n\n历史对话：\n用户：你是谁？\n当前问题：你是谁？\n重写结果：你是谁？\n\n历史对话：\n用户：开心\n当前问题：开心\n重写结果：开心\n\n历史对话：\n用户：我要买它\n系统：请问您指的是哪款商品？\n用户：之前推荐的那双\n当前问题：之前推荐的那双\n重写结果：我想要买你之前推荐的那双鞋。\n\n注意：严格按照以上风格工作，禁止任何额外输出。\n\nUser: 用户：美妆大促推精华液，素材怎么突出抗老效果？\n系统：可展示临床测试数据（如“28天淡化细纹23%”），但需标注“实验室数据，效果因人而异”。\n用户：成分表需要全部展示吗？比如视黄醇浓度\n系统：核心成分需标注，浓度超过1%需特别提醒，避免引发敏感肌用户投诉。\n用户：达人直播能现场测试过敏吗？\n系统：禁止现场做过敏测试，建议用斑贴试验报告代替，或强调“建议先做耳后测试”。\n用户：赠品小样的保质期怎么标注？\n系统：需在包装醒目位置标注“赠品”和“限用日期”，临期小样（<6个月）禁止赠送。\n用户：千川投放选“美妆爱好者”还是“成分党”人群？\n系统：抗老精华选“30+女性+成分党”组合，可叠加“护肤技巧”“抗老话题”兴趣标签。\n用户：平台对“医美级”表述有限制吗？\n系统：“医美级”“医学护肤”属于敏感词，可用“实验室研发”“皮肤科医生推荐”替代。\n用户：这么说我们得调整素材里的敏感表述和赠品日期了？\n系统：是的，同时要确保临床数据有完整检测报告存档。\n\n当前问题：这么说我们得调整素材里的敏感表述和赠品日期了？\n请你根据任务规则输出重写结果。\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=0.5, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 21:21:52 [async_llm.py:252] Added request chatcmpl-e1e155fb22bb434e85d995fe6e34e846.
INFO:     127.0.0.1:40948 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-19 21:21:52 [logger.py:39] Received request chatcmpl-8c0c0387b71449119b49e5d37b362a32: prompt: '<｜begin▁of▁sentence｜>你是一个专业的问题重写模块，专门用于多轮对话场景下的指代补全与语义还原任务。\n请严格按照以下规则执行：\n\n【任务目标】\n1. 你的目标是准确解析用户真实意图，从历史中发掘指代和意图，使得这个独立问题尽量完整，尽可能包含所有信息关键词,特别是要补充关键的动机，指代和场景（润色后的问题至少涉及：用户面对的前置情况，什么限制，什么疑问等）。\n2. 仅当历史信息能够提供明确上下文时才进行补全，否则保持当前问题不变。\n3. 对于明显不需要补全的问题如：吹水："呵呵"，命令："关机"，无关内容："人生的意义"等，返回原句\n4. 不进行任何无关发挥、扩写、润色、修辞性描述、解释、总结、感情色彩。\n5. 不编造任何不存在的假设背景或新信息。\n6. 输出格式必须严格遵循：仅输出最终重写结果文本，不包含任何前缀、提示词、说明性文字或换行符。重写的疑问句以："我想知道"开头，非疑问句你自行适配\n7. 当无需重写时，直接输出原问题。\n\n【重写示例】\n历史对话：\n用户：小红鞋什么时候有货？\n系统：请问您指的是哪款小红鞋？\n用户：就是上次缺货那款\n当前问题：就是上次缺货那款\n重写结果：我想知道上次缺货的那款小红鞋什么时候有货？\n\n历史对话：\n用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：那我们还有其他计划，能投放吗？\n当前问题：那我们还有其他计划，能投放吗？\n重写结果：我想知道当账号处于限流状态时，是否可以继续开启新的广告投放计划？\n\n历史对话：\n用户：你是谁？\n当前问题：你是谁？\n重写结果：你是谁？\n\n历史对话：\n用户：开心\n当前问题：开心\n重写结果：开心\n\n历史对话：\n用户：我要买它\n系统：请问您指的是哪款商品？\n用户：之前推荐的那双\n当前问题：之前推荐的那双\n重写结果：我想要买你之前推荐的那双鞋。\n\n注意：严格按照以上风格工作，禁止任何额外输出。\n\nUser: 用户：家纺大促想推四件套，怎么提升客单价？\n系统：可设置“床品套餐”（床单+被罩+枕套），搭配“满300减50”跨店满减。\n用户：素材里能宣传“抗菌防螨”吗？需要检测报告吗？\n系统：可以宣传，但需在详情页附省级以上机构的抗菌检测报告，标注“检测范围：面料表层”。\n用户：达人直播展示洗涤测试，需要注意什么？\n系统：洗涤次数需与实际产品说明一致，禁止演示“暴力搓洗不变形”等夸大场景。\n用户：大促期间退换货率高，怎么降低？\n系统：建议在详情页增加“尺寸测量视频”，提供免费退换货运费险，注明“水洗后不支持退换”。\n用户：平台对“纯棉”材质有认证要求吗？\n系统：需符合GB/T 29862-2013标准，含棉量≥95%可标注“纯棉”，否则需标注具体比例。\n用户：这么说我们得准备抗菌报告和材质检测文件了？\n系统：是的，建议将报告上传至抖音小店“资质中心”，以备平台抽查。\n\n当前问题：这么说我们得准备抗菌报告和材质检测文件了？\n请你根据任务规则输出重写结果。\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=0.5, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 21:21:52 [async_llm.py:252] Added request chatcmpl-8c0c0387b71449119b49e5d37b362a32.
INFO:     127.0.0.1:40932 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-19 21:21:52 [logger.py:39] Received request chatcmpl-e658728521384857b1359555d3cfb8e0: prompt: '<｜begin▁of▁sentence｜>你是一个专业的问题重写模块，专门用于多轮对话场景下的指代补全与语义还原任务。\n请严格按照以下规则执行：\n\n【任务目标】\n1. 你的目标是准确解析用户真实意图，从历史中发掘指代和意图，使得这个独立问题尽量完整，尽可能包含所有信息关键词,特别是要补充关键的动机，指代和场景（润色后的问题至少涉及：用户面对的前置情况，什么限制，什么疑问等）。\n2. 仅当历史信息能够提供明确上下文时才进行补全，否则保持当前问题不变。\n3. 对于明显不需要补全的问题如：吹水："呵呵"，命令："关机"，无关内容："人生的意义"等，返回原句\n4. 不进行任何无关发挥、扩写、润色、修辞性描述、解释、总结、感情色彩。\n5. 不编造任何不存在的假设背景或新信息。\n6. 输出格式必须严格遵循：仅输出最终重写结果文本，不包含任何前缀、提示词、说明性文字或换行符。重写的疑问句以："我想知道"开头，非疑问句你自行适配\n7. 当无需重写时，直接输出原问题。\n\n【重写示例】\n历史对话：\n用户：小红鞋什么时候有货？\n系统：请问您指的是哪款小红鞋？\n用户：就是上次缺货那款\n当前问题：就是上次缺货那款\n重写结果：我想知道上次缺货的那款小红鞋什么时候有货？\n\n历史对话：\n用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：那我们还有其他计划，能投放吗？\n当前问题：那我们还有其他计划，能投放吗？\n重写结果：我想知道当账号处于限流状态时，是否可以继续开启新的广告投放计划？\n\n历史对话：\n用户：你是谁？\n当前问题：你是谁？\n重写结果：你是谁？\n\n历史对话：\n用户：开心\n当前问题：开心\n重写结果：开心\n\n历史对话：\n用户：我要买它\n系统：请问您指的是哪款商品？\n用户：之前推荐的那双\n当前问题：之前推荐的那双\n重写结果：我想要买你之前推荐的那双鞋。\n\n注意：严格按照以上风格工作，禁止任何额外输出。\n\nUser: 用户：生鲜大促推草莓，怎么保证运输新鲜度？\n系统：建议采用航空冷链运输，包装内放置冰袋+透气孔，标注“48小时直达”。\n用户：宣传“有机认证”需要什么材料？\n系统：需提供《有机产品认证证书》，且认证范围包含具体品类（如“草莓”），有效期内。\n用户：素材里能展示农药残留检测结果吗？\n系统：可以，但需完整展示检测机构名称、样品编号和未检出项目，禁止截取部分数据。\n用户：达人直播能试吃未清洗的水果吗？\n系统：禁止试吃生食材，建议展示清洗过程或提供加工后的试吃场景（如草莓沙拉）。\n用户：临期食品怎么处理？能打折促销吗？\n系统：保质期≤1年的食品，临期界定为到期前45天，需在标题标注“临期特惠”并公示剩余保质期。\n用户：平台对食品添加剂有啥要求？\n系统：需在详情页完整列出添加剂名称，符合GB 2760-2014标准，禁止使用“零添加”等模糊表述。\n用户：这么说我们得提前备案有机证书和检测报告了？\n系统：是的，生鲜类目审核严格，建议所有资质提前15天提交平台审核。\n\n当前问题：这么说我们得提前备案有机证书和检测报告了？\n请你根据任务规则输出重写结果。\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=0.5, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 21:21:52 [async_llm.py:252] Added request chatcmpl-e658728521384857b1359555d3cfb8e0.
INFO:     127.0.0.1:40914 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:40918 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:40948 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:40932 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-19 21:21:58 [loggers.py:111] Engine 000: Avg prompt throughput: 3531.7 tokens/s, Avg generation throughput: 92.1 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 72.4%
INFO 06-19 21:22:08 [loggers.py:111] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 72.4%
INFO 06-19 21:25:40 [logger.py:39] Received request chatcmpl-1f72d9e480ec44c9aa60f56383c0c133: prompt: '<｜begin▁of▁sentence｜>你是一个专业的问题重写模块，专门用于多轮对话场景下的指代补全与语义还原任务。\n请严格按照以下规则执行：\n\n【任务目标】\n1. 你的目标是准确解析用户真实意图，从历史中发掘指代和意图，使得这个独立问题尽量完整，尽可能包含所有信息关键词,特别是要补充关键的动机，指代和场景（润色后的问题至少涉及：用户面对的前置情况，什么限制，什么疑问等）。\n2. 仅当历史信息能够提供明确上下文时才进行补全，否则保持当前问题不变。\n3. 对于明显不需要补全的问题如：吹水："呵呵"，命令："关机"，无关内容："人生的意义"等，返回原句\n4. 不进行任何无关发挥、扩写、润色、修辞性描述、解释、总结、感情色彩。\n5. 不编造任何不存在的假设背景或新信息。\n6. 输出格式必须严格遵循：仅输出最终重写结果文本，不包含任何前缀、提示词、说明性文字或换行符。重写的疑问句以："我想知道"开头，非疑问句你自行适配\n7. 当无需重写时，直接输出原问题。\n\n【重写示例】\n历史对话：\n用户：小红鞋什么时候有货？\n系统：请问您指的是哪款小红鞋？\n用户：就是上次缺货那款\n当前问题：就是上次缺货那款\n重写结果：我想知道上次缺货的那款小红鞋什么时候有货？\n\n历史对话：\n用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：那我们还有其他计划，能投放吗？\n当前问题：那我们还有其他计划，能投放吗？\n重写结果：我想知道当账号处于限流状态时，是否可以继续开启新的广告投放计划？\n\n历史对话：\n用户：你是谁？\n当前问题：你是谁？\n重写结果：你是谁？\n\n历史对话：\n用户：开心\n当前问题：开心\n重写结果：开心\n\n历史对话：\n用户：我要买它\n系统：请问您指的是哪款商品？\n用户：之前推荐的那双\n当前问题：之前推荐的那双\n重写结果：我想要买你之前推荐的那双鞋。\n\n注意：严格按照以上风格工作，禁止任何额外输出。\n\nUser: 用户：我想做短视频投放，有什么要求吗？\n系统：要求内容原创、清晰，不能有违规元素，建议时长控制在15秒左右。\n用户：那拍的时候有什么建议？\n系统：建议突出产品卖点，开头三秒抓人，结尾加上行动引导。\n用户：那像我们这种的适合吗？\n\n当前问题：那像我们这种的适合吗？\n请你根据任务规则输出重写结果。\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=0.5, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 21:25:40 [async_llm.py:252] Added request chatcmpl-1f72d9e480ec44c9aa60f56383c0c133.
INFO 06-19 21:25:40 [logger.py:39] Received request chatcmpl-f1729d5be14d44a2bb5f70db1ac1a11e: prompt: '<｜begin▁of▁sentence｜>你是一个专业的问题重写模块，专门用于多轮对话场景下的指代补全与语义还原任务。\n请严格按照以下规则执行：\n\n【任务目标】\n1. 你的目标是准确解析用户真实意图，从历史中发掘指代和意图，使得这个独立问题尽量完整，尽可能包含所有信息关键词,特别是要补充关键的动机，指代和场景（润色后的问题至少涉及：用户面对的前置情况，什么限制，什么疑问等）。\n2. 仅当历史信息能够提供明确上下文时才进行补全，否则保持当前问题不变。\n3. 对于明显不需要补全的问题如：吹水："呵呵"，命令："关机"，无关内容："人生的意义"等，返回原句\n4. 不进行任何无关发挥、扩写、润色、修辞性描述、解释、总结、感情色彩。\n5. 不编造任何不存在的假设背景或新信息。\n6. 输出格式必须严格遵循：仅输出最终重写结果文本，不包含任何前缀、提示词、说明性文字或换行符。重写的疑问句以："我想知道"开头，非疑问句你自行适配\n7. 当无需重写时，直接输出原问题。\n\n【重写示例】\n历史对话：\n用户：小红鞋什么时候有货？\n系统：请问您指的是哪款小红鞋？\n用户：就是上次缺货那款\n当前问题：就是上次缺货那款\n重写结果：我想知道上次缺货的那款小红鞋什么时候有货？\n\n历史对话：\n用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：那我们还有其他计划，能投放吗？\n当前问题：那我们还有其他计划，能投放吗？\n重写结果：我想知道当账号处于限流状态时，是否可以继续开启新的广告投放计划？\n\n历史对话：\n用户：你是谁？\n当前问题：你是谁？\n重写结果：你是谁？\n\n历史对话：\n用户：开心\n当前问题：开心\n重写结果：开心\n\n历史对话：\n用户：我要买它\n系统：请问您指的是哪款商品？\n用户：之前推荐的那双\n当前问题：之前推荐的那双\n重写结果：我想要买你之前推荐的那双鞋。\n\n注意：严格按照以上风格工作，禁止任何额外输出。\n\nUser: 用户：我们最近打算投全域，但没经验。\n系统：全域计划适合配合大场活动，建议提前准备素材。\n用户：预算要设置多少？\n系统：大场期间建议设置充足预算，比如3000元起。\n用户：那我们之前用的那种也能一起开吗？\n\n当前问题：那我们之前用的那种也能一起开吗？\n请你根据任务规则输出重写结果。\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=0.5, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 21:25:40 [async_llm.py:252] Added request chatcmpl-f1729d5be14d44a2bb5f70db1ac1a11e.
INFO 06-19 21:25:40 [logger.py:39] Received request chatcmpl-4c7035e2c9d1475d85bfd67cbf645df3: prompt: '<｜begin▁of▁sentence｜>你是一个专业的问题重写模块，专门用于多轮对话场景下的指代补全与语义还原任务。\n请严格按照以下规则执行：\n\n【任务目标】\n1. 你的目标是准确解析用户真实意图，从历史中发掘指代和意图，使得这个独立问题尽量完整，尽可能包含所有信息关键词,特别是要补充关键的动机，指代和场景（润色后的问题至少涉及：用户面对的前置情况，什么限制，什么疑问等）。\n2. 仅当历史信息能够提供明确上下文时才进行补全，否则保持当前问题不变。\n3. 对于明显不需要补全的问题如：吹水："呵呵"，命令："关机"，无关内容："人生的意义"等，返回原句\n4. 不进行任何无关发挥、扩写、润色、修辞性描述、解释、总结、感情色彩。\n5. 不编造任何不存在的假设背景或新信息。\n6. 输出格式必须严格遵循：仅输出最终重写结果文本，不包含任何前缀、提示词、说明性文字或换行符。重写的疑问句以："我想知道"开头，非疑问句你自行适配\n7. 当无需重写时，直接输出原问题。\n\n【重写示例】\n历史对话：\n用户：小红鞋什么时候有货？\n系统：请问您指的是哪款小红鞋？\n用户：就是上次缺货那款\n当前问题：就是上次缺货那款\n重写结果：我想知道上次缺货的那款小红鞋什么时候有货？\n\n历史对话：\n用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：那我们还有其他计划，能投放吗？\n当前问题：那我们还有其他计划，能投放吗？\n重写结果：我想知道当账号处于限流状态时，是否可以继续开启新的广告投放计划？\n\n历史对话：\n用户：你是谁？\n当前问题：你是谁？\n重写结果：你是谁？\n\n历史对话：\n用户：开心\n当前问题：开心\n重写结果：开心\n\n历史对话：\n用户：我要买它\n系统：请问您指的是哪款商品？\n用户：之前推荐的那双\n当前问题：之前推荐的那双\n重写结果：我想要买你之前推荐的那双鞋。\n\n注意：严格按照以上风格工作，禁止任何额外输出。\n\nUser: 用户：账户提示授权失效了，这是什么意思？\n系统：说明你的店铺或商品绑定信息失效，需要重新授权。\n用户：那要怎么操作？\n系统：登录账户后，进入“账户绑定”页面点击“重新授权”即可。\n用户：是不是每次登录都要授权？\n\n当前问题：是不是每次登录都要授权？\n请你根据任务规则输出重写结果。\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=0.5, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 21:25:40 [async_llm.py:252] Added request chatcmpl-4c7035e2c9d1475d85bfd67cbf645df3.
INFO 06-19 21:25:40 [logger.py:39] Received request chatcmpl-538077c5c52e404bb6a9baf45ebbdb43: prompt: '<｜begin▁of▁sentence｜>你是一个专业的问题重写模块，专门用于多轮对话场景下的指代补全与语义还原任务。\n请严格按照以下规则执行：\n\n【任务目标】\n1. 你的目标是准确解析用户真实意图，从历史中发掘指代和意图，使得这个独立问题尽量完整，尽可能包含所有信息关键词,特别是要补充关键的动机，指代和场景（润色后的问题至少涉及：用户面对的前置情况，什么限制，什么疑问等）。\n2. 仅当历史信息能够提供明确上下文时才进行补全，否则保持当前问题不变。\n3. 对于明显不需要补全的问题如：吹水："呵呵"，命令："关机"，无关内容："人生的意义"等，返回原句\n4. 不进行任何无关发挥、扩写、润色、修辞性描述、解释、总结、感情色彩。\n5. 不编造任何不存在的假设背景或新信息。\n6. 输出格式必须严格遵循：仅输出最终重写结果文本，不包含任何前缀、提示词、说明性文字或换行符。重写的疑问句以："我想知道"开头，非疑问句你自行适配\n7. 当无需重写时，直接输出原问题。\n\n【重写示例】\n历史对话：\n用户：小红鞋什么时候有货？\n系统：请问您指的是哪款小红鞋？\n用户：就是上次缺货那款\n当前问题：就是上次缺货那款\n重写结果：我想知道上次缺货的那款小红鞋什么时候有货？\n\n历史对话：\n用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：那我们还有其他计划，能投放吗？\n当前问题：那我们还有其他计划，能投放吗？\n重写结果：我想知道当账号处于限流状态时，是否可以继续开启新的广告投放计划？\n\n历史对话：\n用户：你是谁？\n当前问题：你是谁？\n重写结果：你是谁？\n\n历史对话：\n用户：开心\n当前问题：开心\n重写结果：开心\n\n历史对话：\n用户：我要买它\n系统：请问您指的是哪款商品？\n用户：之前推荐的那双\n当前问题：之前推荐的那双\n重写结果：我想要买你之前推荐的那双鞋。\n\n注意：严格按照以上风格工作，禁止任何额外输出。\n\nUser: 用户：我刚入驻平台，推广要怎么开始？\n系统：建议先使用新客引导计划，操作简单且投产比稳定。\n用户：预算方面有什么推荐？\n系统：可以从日预算200元起测，连续投放3天效果更好。\n用户：那素材还需要自己拍吗？\n系统：你可以使用平台提供的模板或达人共创内容。\n用户：那我们这种情况用哪个方式更好？\n\n当前问题：那我们这种情况用哪个方式更好？\n请你根据任务规则输出重写结果。\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=0.5, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 21:25:40 [async_llm.py:252] Added request chatcmpl-538077c5c52e404bb6a9baf45ebbdb43.
INFO:     127.0.0.1:45866 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-19 21:25:40 [logger.py:39] Received request chatcmpl-8d9274175ce24655b46aae1a01a05ed7: prompt: '<｜begin▁of▁sentence｜>你是一个专业的问题重写模块，专门用于多轮对话场景下的指代补全与语义还原任务。\n请严格按照以下规则执行：\n\n【任务目标】\n1. 你的目标是准确解析用户真实意图，从历史中发掘指代和意图，使得这个独立问题尽量完整，尽可能包含所有信息关键词,特别是要补充关键的动机，指代和场景（润色后的问题至少涉及：用户面对的前置情况，什么限制，什么疑问等）。\n2. 仅当历史信息能够提供明确上下文时才进行补全，否则保持当前问题不变。\n3. 对于明显不需要补全的问题如：吹水："呵呵"，命令："关机"，无关内容："人生的意义"等，返回原句\n4. 不进行任何无关发挥、扩写、润色、修辞性描述、解释、总结、感情色彩。\n5. 不编造任何不存在的假设背景或新信息。\n6. 输出格式必须严格遵循：仅输出最终重写结果文本，不包含任何前缀、提示词、说明性文字或换行符。重写的疑问句以："我想知道"开头，非疑问句你自行适配\n7. 当无需重写时，直接输出原问题。\n\n【重写示例】\n历史对话：\n用户：小红鞋什么时候有货？\n系统：请问您指的是哪款小红鞋？\n用户：就是上次缺货那款\n当前问题：就是上次缺货那款\n重写结果：我想知道上次缺货的那款小红鞋什么时候有货？\n\n历史对话：\n用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：那我们还有其他计划，能投放吗？\n当前问题：那我们还有其他计划，能投放吗？\n重写结果：我想知道当账号处于限流状态时，是否可以继续开启新的广告投放计划？\n\n历史对话：\n用户：你是谁？\n当前问题：你是谁？\n重写结果：你是谁？\n\n历史对话：\n用户：开心\n当前问题：开心\n重写结果：开心\n\n历史对话：\n用户：我要买它\n系统：请问您指的是哪款商品？\n用户：之前推荐的那双\n当前问题：之前推荐的那双\n重写结果：我想要买你之前推荐的那双鞋。\n\n注意：严格按照以上风格工作，禁止任何额外输出。\n\nUser: 用户：我看到页面提示“投放内容含未成年人元素”，怎么处理？\n系统：你需要确认素材中是否涉及未成年相关内容，比如儿童模特或儿童话题。\n用户：我这边是卖儿童背包的，算违规吗？\n系统：只要内容健康、安全、没有引导风险，一般不算违规。\n用户：那我是不是不改也能过？\n\n当前问题：那我是不是不改也能过？\n请你根据任务规则输出重写结果。\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=0.5, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 21:25:40 [async_llm.py:252] Added request chatcmpl-8d9274175ce24655b46aae1a01a05ed7.
INFO:     127.0.0.1:45864 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-19 21:25:40 [logger.py:39] Received request chatcmpl-a4c647fe9c4949dc9a41e2fad2da1184: prompt: '<｜begin▁of▁sentence｜>你是一个专业的问题重写模块，专门用于多轮对话场景下的指代补全与语义还原任务。\n请严格按照以下规则执行：\n\n【任务目标】\n1. 你的目标是准确解析用户真实意图，从历史中发掘指代和意图，使得这个独立问题尽量完整，尽可能包含所有信息关键词,特别是要补充关键的动机，指代和场景（润色后的问题至少涉及：用户面对的前置情况，什么限制，什么疑问等）。\n2. 仅当历史信息能够提供明确上下文时才进行补全，否则保持当前问题不变。\n3. 对于明显不需要补全的问题如：吹水："呵呵"，命令："关机"，无关内容："人生的意义"等，返回原句\n4. 不进行任何无关发挥、扩写、润色、修辞性描述、解释、总结、感情色彩。\n5. 不编造任何不存在的假设背景或新信息。\n6. 输出格式必须严格遵循：仅输出最终重写结果文本，不包含任何前缀、提示词、说明性文字或换行符。重写的疑问句以："我想知道"开头，非疑问句你自行适配\n7. 当无需重写时，直接输出原问题。\n\n【重写示例】\n历史对话：\n用户：小红鞋什么时候有货？\n系统：请问您指的是哪款小红鞋？\n用户：就是上次缺货那款\n当前问题：就是上次缺货那款\n重写结果：我想知道上次缺货的那款小红鞋什么时候有货？\n\n历史对话：\n用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：那我们还有其他计划，能投放吗？\n当前问题：那我们还有其他计划，能投放吗？\n重写结果：我想知道当账号处于限流状态时，是否可以继续开启新的广告投放计划？\n\n历史对话：\n用户：你是谁？\n当前问题：你是谁？\n重写结果：你是谁？\n\n历史对话：\n用户：开心\n当前问题：开心\n重写结果：开心\n\n历史对话：\n用户：我要买它\n系统：请问您指的是哪款商品？\n用户：之前推荐的那双\n当前问题：之前推荐的那双\n重写结果：我想要买你之前推荐的那双鞋。\n\n注意：严格按照以上风格工作，禁止任何额外输出。\n\nUser: 用户：我在投放中用了模板创意，感觉数据还行\n系统：模板创意的投产稳定，但可能不如定制素材吸引力强。\n用户：那转化会差多少？\n系统：一般差距在20%左右，但具体还得看产品类型和受众人群。\n用户：我们这种是不是不适合一直用那个？\n\n当前问题：我们这种是不是不适合一直用那个？\n请你根据任务规则输出重写结果。\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=0.5, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 21:25:40 [async_llm.py:252] Added request chatcmpl-a4c647fe9c4949dc9a41e2fad2da1184.
INFO:     127.0.0.1:45860 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-19 21:25:40 [logger.py:39] Received request chatcmpl-a41f8312e7384f3ebacf7b198d6f1f88: prompt: '<｜begin▁of▁sentence｜>你是一个专业的问题重写模块，专门用于多轮对话场景下的指代补全与语义还原任务。\n请严格按照以下规则执行：\n\n【任务目标】\n1. 你的目标是准确解析用户真实意图，从历史中发掘指代和意图，使得这个独立问题尽量完整，尽可能包含所有信息关键词,特别是要补充关键的动机，指代和场景（润色后的问题至少涉及：用户面对的前置情况，什么限制，什么疑问等）。\n2. 仅当历史信息能够提供明确上下文时才进行补全，否则保持当前问题不变。\n3. 对于明显不需要补全的问题如：吹水："呵呵"，命令："关机"，无关内容："人生的意义"等，返回原句\n4. 不进行任何无关发挥、扩写、润色、修辞性描述、解释、总结、感情色彩。\n5. 不编造任何不存在的假设背景或新信息。\n6. 输出格式必须严格遵循：仅输出最终重写结果文本，不包含任何前缀、提示词、说明性文字或换行符。重写的疑问句以："我想知道"开头，非疑问句你自行适配\n7. 当无需重写时，直接输出原问题。\n\n【重写示例】\n历史对话：\n用户：小红鞋什么时候有货？\n系统：请问您指的是哪款小红鞋？\n用户：就是上次缺货那款\n当前问题：就是上次缺货那款\n重写结果：我想知道上次缺货的那款小红鞋什么时候有货？\n\n历史对话：\n用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：那我们还有其他计划，能投放吗？\n当前问题：那我们还有其他计划，能投放吗？\n重写结果：我想知道当账号处于限流状态时，是否可以继续开启新的广告投放计划？\n\n历史对话：\n用户：你是谁？\n当前问题：你是谁？\n重写结果：你是谁？\n\n历史对话：\n用户：开心\n当前问题：开心\n重写结果：开心\n\n历史对话：\n用户：我要买它\n系统：请问您指的是哪款商品？\n用户：之前推荐的那双\n当前问题：之前推荐的那双\n重写结果：我想要买你之前推荐的那双鞋。\n\n注意：严格按照以上风格工作，禁止任何额外输出。\n\nUser: 用户：我在‘调控任务’中设置了预算，怎么系统还在烧钱？\n系统：你是否设置了日预算？另外也需要检查是否有其他任务计划正在投放。\n用户：任务类型太多了我也不清楚\n系统：建议进入‘推广计划-概览’，查看每个计划的消耗情况。\n用户：那我要怎么停掉？\n\n当前问题：那我要怎么停掉？\n请你根据任务规则输出重写结果。\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=0.5, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 21:25:40 [async_llm.py:252] Added request chatcmpl-a41f8312e7384f3ebacf7b198d6f1f88.
INFO:     127.0.0.1:45866 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-19 21:25:40 [logger.py:39] Received request chatcmpl-0e6550323449479fabe05c906162c0ed: prompt: '<｜begin▁of▁sentence｜>你是一个专业的问题重写模块，专门用于多轮对话场景下的指代补全与语义还原任务。\n请严格按照以下规则执行：\n\n【任务目标】\n1. 你的目标是准确解析用户真实意图，从历史中发掘指代和意图，使得这个独立问题尽量完整，尽可能包含所有信息关键词,特别是要补充关键的动机，指代和场景（润色后的问题至少涉及：用户面对的前置情况，什么限制，什么疑问等）。\n2. 仅当历史信息能够提供明确上下文时才进行补全，否则保持当前问题不变。\n3. 对于明显不需要补全的问题如：吹水："呵呵"，命令："关机"，无关内容："人生的意义"等，返回原句\n4. 不进行任何无关发挥、扩写、润色、修辞性描述、解释、总结、感情色彩。\n5. 不编造任何不存在的假设背景或新信息。\n6. 输出格式必须严格遵循：仅输出最终重写结果文本，不包含任何前缀、提示词、说明性文字或换行符。重写的疑问句以："我想知道"开头，非疑问句你自行适配\n7. 当无需重写时，直接输出原问题。\n\n【重写示例】\n历史对话：\n用户：小红鞋什么时候有货？\n系统：请问您指的是哪款小红鞋？\n用户：就是上次缺货那款\n当前问题：就是上次缺货那款\n重写结果：我想知道上次缺货的那款小红鞋什么时候有货？\n\n历史对话：\n用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：那我们还有其他计划，能投放吗？\n当前问题：那我们还有其他计划，能投放吗？\n重写结果：我想知道当账号处于限流状态时，是否可以继续开启新的广告投放计划？\n\n历史对话：\n用户：你是谁？\n当前问题：你是谁？\n重写结果：你是谁？\n\n历史对话：\n用户：开心\n当前问题：开心\n重写结果：开心\n\n历史对话：\n用户：我要买它\n系统：请问您指的是哪款商品？\n用户：之前推荐的那双\n当前问题：之前推荐的那双\n重写结果：我想要买你之前推荐的那双鞋。\n\n注意：严格按照以上风格工作，禁止任何额外输出。\n\nUser: 用户：我们账号昨天刚通过审核，今天能跑投放吗？\n系统：可以的，但建议先完成基础设置，比如绑定商品和选择投放计划。\n用户：之前我在别的平台有素材，这里能用吗？\n系统：可以复用，但需确保符合抖音平台素材规范。\n用户：那如果不符合怎么办？\n\n当前问题：那如果不符合怎么办？\n请你根据任务规则输出重写结果。\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=0.5, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 21:25:40 [async_llm.py:252] Added request chatcmpl-0e6550323449479fabe05c906162c0ed.
INFO:     127.0.0.1:45880 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-19 21:25:40 [logger.py:39] Received request chatcmpl-f0da5b0b3fc94fba8f1deb48fa87bfcd: prompt: '<｜begin▁of▁sentence｜>你是一个专业的问题重写模块，专门用于多轮对话场景下的指代补全与语义还原任务。\n请严格按照以下规则执行：\n\n【任务目标】\n1. 你的目标是准确解析用户真实意图，从历史中发掘指代和意图，使得这个独立问题尽量完整，尽可能包含所有信息关键词,特别是要补充关键的动机，指代和场景（润色后的问题至少涉及：用户面对的前置情况，什么限制，什么疑问等）。\n2. 仅当历史信息能够提供明确上下文时才进行补全，否则保持当前问题不变。\n3. 对于明显不需要补全的问题如：吹水："呵呵"，命令："关机"，无关内容："人生的意义"等，返回原句\n4. 不进行任何无关发挥、扩写、润色、修辞性描述、解释、总结、感情色彩。\n5. 不编造任何不存在的假设背景或新信息。\n6. 输出格式必须严格遵循：仅输出最终重写结果文本，不包含任何前缀、提示词、说明性文字或换行符。重写的疑问句以："我想知道"开头，非疑问句你自行适配\n7. 当无需重写时，直接输出原问题。\n\n【重写示例】\n历史对话：\n用户：小红鞋什么时候有货？\n系统：请问您指的是哪款小红鞋？\n用户：就是上次缺货那款\n当前问题：就是上次缺货那款\n重写结果：我想知道上次缺货的那款小红鞋什么时候有货？\n\n历史对话：\n用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：那我们还有其他计划，能投放吗？\n当前问题：那我们还有其他计划，能投放吗？\n重写结果：我想知道当账号处于限流状态时，是否可以继续开启新的广告投放计划？\n\n历史对话：\n用户：你是谁？\n当前问题：你是谁？\n重写结果：你是谁？\n\n历史对话：\n用户：开心\n当前问题：开心\n重写结果：开心\n\n历史对话：\n用户：我要买它\n系统：请问您指的是哪款商品？\n用户：之前推荐的那双\n当前问题：之前推荐的那双\n重写结果：我想要买你之前推荐的那双鞋。\n\n注意：严格按照以上风格工作，禁止任何额外输出。\n\nUser: 用户：平台提示我存在“虚假宣传”的风险，我没理解什么意思\n系统：可能是你素材中使用了“第一”、“最强”等绝对化用词\n用户：那如果我只是说了‘很受欢迎’，也不行吗？\n系统：‘受欢迎’一般可以，但不能主观夸大或无证据证明。\n用户：那像我写的那些都要改吗？\n\n当前问题：那像我写的那些都要改吗？\n请你根据任务规则输出重写结果。\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=0.5, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 21:25:40 [async_llm.py:252] Added request chatcmpl-f0da5b0b3fc94fba8f1deb48fa87bfcd.
INFO:     127.0.0.1:45864 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-19 21:25:40 [logger.py:39] Received request chatcmpl-05aa3784097842738c2ec21715960b42: prompt: '<｜begin▁of▁sentence｜>你是一个专业的问题重写模块，专门用于多轮对话场景下的指代补全与语义还原任务。\n请严格按照以下规则执行：\n\n【任务目标】\n1. 你的目标是准确解析用户真实意图，从历史中发掘指代和意图，使得这个独立问题尽量完整，尽可能包含所有信息关键词,特别是要补充关键的动机，指代和场景（润色后的问题至少涉及：用户面对的前置情况，什么限制，什么疑问等）。\n2. 仅当历史信息能够提供明确上下文时才进行补全，否则保持当前问题不变。\n3. 对于明显不需要补全的问题如：吹水："呵呵"，命令："关机"，无关内容："人生的意义"等，返回原句\n4. 不进行任何无关发挥、扩写、润色、修辞性描述、解释、总结、感情色彩。\n5. 不编造任何不存在的假设背景或新信息。\n6. 输出格式必须严格遵循：仅输出最终重写结果文本，不包含任何前缀、提示词、说明性文字或换行符。重写的疑问句以："我想知道"开头，非疑问句你自行适配\n7. 当无需重写时，直接输出原问题。\n\n【重写示例】\n历史对话：\n用户：小红鞋什么时候有货？\n系统：请问您指的是哪款小红鞋？\n用户：就是上次缺货那款\n当前问题：就是上次缺货那款\n重写结果：我想知道上次缺货的那款小红鞋什么时候有货？\n\n历史对话：\n用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：那我们还有其他计划，能投放吗？\n当前问题：那我们还有其他计划，能投放吗？\n重写结果：我想知道当账号处于限流状态时，是否可以继续开启新的广告投放计划？\n\n历史对话：\n用户：你是谁？\n当前问题：你是谁？\n重写结果：你是谁？\n\n历史对话：\n用户：开心\n当前问题：开心\n重写结果：开心\n\n历史对话：\n用户：我要买它\n系统：请问您指的是哪款商品？\n用户：之前推荐的那双\n当前问题：之前推荐的那双\n重写结果：我想要买你之前推荐的那双鞋。\n\n注意：严格按照以上风格工作，禁止任何额外输出。\n\nUser: 用户：我打算做一场新品直播，有什么建议？\n系统：建议提前3天预热，准备预售链接并邀请达人带播。\n用户：我们预算紧张，请问达人费用能省吗？\n系统：可以选择平台免费达人共创服务，或用中腰部达人进行合作。\n用户：那如果我们就是小商家呢？\n\n当前问题：那如果我们就是小商家呢？\n请你根据任务规则输出重写结果。\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=0.5, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 21:25:40 [async_llm.py:252] Added request chatcmpl-05aa3784097842738c2ec21715960b42.
INFO:     127.0.0.1:45866 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-19 21:25:40 [logger.py:39] Received request chatcmpl-8ee9e653c435458f98cdf2c144fbad4e: prompt: '<｜begin▁of▁sentence｜>你是一个专业的问题重写模块，专门用于多轮对话场景下的指代补全与语义还原任务。\n请严格按照以下规则执行：\n\n【任务目标】\n1. 你的目标是准确解析用户真实意图，从历史中发掘指代和意图，使得这个独立问题尽量完整，尽可能包含所有信息关键词,特别是要补充关键的动机，指代和场景（润色后的问题至少涉及：用户面对的前置情况，什么限制，什么疑问等）。\n2. 仅当历史信息能够提供明确上下文时才进行补全，否则保持当前问题不变。\n3. 对于明显不需要补全的问题如：吹水："呵呵"，命令："关机"，无关内容："人生的意义"等，返回原句\n4. 不进行任何无关发挥、扩写、润色、修辞性描述、解释、总结、感情色彩。\n5. 不编造任何不存在的假设背景或新信息。\n6. 输出格式必须严格遵循：仅输出最终重写结果文本，不包含任何前缀、提示词、说明性文字或换行符。重写的疑问句以："我想知道"开头，非疑问句你自行适配\n7. 当无需重写时，直接输出原问题。\n\n【重写示例】\n历史对话：\n用户：小红鞋什么时候有货？\n系统：请问您指的是哪款小红鞋？\n用户：就是上次缺货那款\n当前问题：就是上次缺货那款\n重写结果：我想知道上次缺货的那款小红鞋什么时候有货？\n\n历史对话：\n用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：那我们还有其他计划，能投放吗？\n当前问题：那我们还有其他计划，能投放吗？\n重写结果：我想知道当账号处于限流状态时，是否可以继续开启新的广告投放计划？\n\n历史对话：\n用户：你是谁？\n当前问题：你是谁？\n重写结果：你是谁？\n\n历史对话：\n用户：开心\n当前问题：开心\n重写结果：开心\n\n历史对话：\n用户：我要买它\n系统：请问您指的是哪款商品？\n用户：之前推荐的那双\n当前问题：之前推荐的那双\n重写结果：我想要买你之前推荐的那双鞋。\n\n注意：严格按照以上风格工作，禁止任何额外输出。\n\nUser: 用户：我在千川平台上新手投放，应该怎么规划预算？\n系统：新手投放建议从日预算200元起步，根据表现逐步调整。\n用户：那我是不是可以从最低预算开始测试？\n系统：可以的，但建议至少持续3天观察数据趋势。\n用户：我们这边是做女装的，适合什么样的推广计划？\n系统：女装类适合图文结合的投放，也可以尝试短视频种草。\n用户：那达人带货和自播哪种更适合我们这种情况？\n系统：新手建议先自播，控制成本，后续再考虑达人合作。\n用户：素材方面有什么具体要求吗？\n系统：素材需高清、无水印，且不能包含夸大宣传内容。\n用户：我们用之前拍的图文可以吗？\n系统：旧素材可以参考使用，但需要检查是否符合当前标准。\n用户：不符合规范会有什么后果？\n系统：不合规素材会被拒投，严重可能影响账户健康度。\n用户：如果我只想先做曝光，哪种投放方式更合适？\n系统：曝光导向可选择CPM计费计划，适合新品冷启动。\n用户：我朋友推荐我用全域推广，这个适合我吗？\n系统：全域推广适合预算充足、追求多渠道触达的商家。\n用户：听说最近平台有补贴政策，可以用在这类计划吗？\n系统：平台补贴需要申请指定计划，目前新商家也可参与。\n用户：那这种计划和常规的计划能一起开吗？\n系统：两个计划可以并行，但需做好预算分配与计划排期。\n\n当前问题：那这种计划和常规的计划能一起开吗？\n请你根据任务规则输出重写结果。\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=0.5, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 21:25:40 [async_llm.py:252] Added request chatcmpl-8ee9e653c435458f98cdf2c144fbad4e.
INFO:     127.0.0.1:45860 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-19 21:25:40 [logger.py:39] Received request chatcmpl-fa088456b89d4b7c91aaa0427f6a0c83: prompt: '<｜begin▁of▁sentence｜>你是一个专业的问题重写模块，专门用于多轮对话场景下的指代补全与语义还原任务。\n请严格按照以下规则执行：\n\n【任务目标】\n1. 你的目标是准确解析用户真实意图，从历史中发掘指代和意图，使得这个独立问题尽量完整，尽可能包含所有信息关键词,特别是要补充关键的动机，指代和场景（润色后的问题至少涉及：用户面对的前置情况，什么限制，什么疑问等）。\n2. 仅当历史信息能够提供明确上下文时才进行补全，否则保持当前问题不变。\n3. 对于明显不需要补全的问题如：吹水："呵呵"，命令："关机"，无关内容："人生的意义"等，返回原句\n4. 不进行任何无关发挥、扩写、润色、修辞性描述、解释、总结、感情色彩。\n5. 不编造任何不存在的假设背景或新信息。\n6. 输出格式必须严格遵循：仅输出最终重写结果文本，不包含任何前缀、提示词、说明性文字或换行符。重写的疑问句以："我想知道"开头，非疑问句你自行适配\n7. 当无需重写时，直接输出原问题。\n\n【重写示例】\n历史对话：\n用户：小红鞋什么时候有货？\n系统：请问您指的是哪款小红鞋？\n用户：就是上次缺货那款\n当前问题：就是上次缺货那款\n重写结果：我想知道上次缺货的那款小红鞋什么时候有货？\n\n历史对话：\n用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：那我们还有其他计划，能投放吗？\n当前问题：那我们还有其他计划，能投放吗？\n重写结果：我想知道当账号处于限流状态时，是否可以继续开启新的广告投放计划？\n\n历史对话：\n用户：你是谁？\n当前问题：你是谁？\n重写结果：你是谁？\n\n历史对话：\n用户：开心\n当前问题：开心\n重写结果：开心\n\n历史对话：\n用户：我要买它\n系统：请问您指的是哪款商品？\n用户：之前推荐的那双\n当前问题：之前推荐的那双\n重写结果：我想要买你之前推荐的那双鞋。\n\n注意：严格按照以上风格工作，禁止任何额外输出。\n\nUser: 用户：我们最近计划在抖音做大促，但对流程不熟。\n系统：建议你先了解平台的大促节奏，比如双十一、618等，提前准备好素材和预算。\n用户：预算应该准备多少比较合适？\n系统：建议根据以往转化率和客单价评估，一般从5000元起步较稳妥。\n用户：素材方面有哪些注意事项？\n系统：内容要突出促销点，加入限时标签、优惠码等信息可以提升点击率。\n用户：那达人投放要现在联系吗？\n系统：是的，热门达人档期紧张，建议提前至少一周确定合作。\n用户：投放时间段有没有讲究？\n系统：高峰期一般集中在中午12点和晚上8点后，可以重点投放。\n用户：听说全域计划不错，我们也能用吗？\n系统：可以，全域计划适合有多渠道曝光需求的商家，需提前设置好人群和场景。\n用户：数据监控是用平台自带的吗？\n系统：平台提供实时监控面板，你也可以接入第三方分析工具如蝉妈妈、新榜。\n用户：那之前的数据能复用吗？比如去年双11的投放报告。\n系统：可以参考历史投放效果，特别是高转化素材和人群画像，但需注意平台规则和用户偏好的时效性变化。\n用户：我们产品属于教育类，会有敏感问题吗？比如课程效果承诺\n系统：教育类属于高风险类目，需严格遵守《广告法》，避免使用“保过”“必提分”等绝对化表述，建议突出教学服务流程而非效果承诺。\n用户：那素材里提到“名师1对1辅导”合规吗？需要提供教师资格证吗？\n系统：“名师”需明确资质定义，若涉及具体教师需提供教师资格证编号或职称证明，建议用“资深教师团队”等模糊表述降低风险。\n用户：如果素材中出现学生成绩单截图，是否需要打码处理？\n系统：需隐去学生姓名、学校等隐私信息，仅保留成绩提升对比数据，且需获得家长/学生的肖像授权。\n用户：平台审核一般需要多久？我们怕赶不上大促档期\n系统：常规审核周期为2-4小时，但大促期间可能延长至12小时，建议提前3天提交素材预审。\n用户：万一审核被拒，有没有快速申诉通道？\n系统：可通过抖音小店后台的“内容审核申诉”入口提交，需附上课程大纲、学员合同等合规证明材料，工作日内4小时内会有复核。\n用户：这些证明材料需要提前准备哪些具体文件？\n系统：建议准备教师资质证书扫描件、学员报名协议模板、课程内容合规性自评报告，以及过往无违规记录的平台截图。\n用户：投放后的数据复盘要关注哪些核心指标？\n系统：重点关注CTR（点击率）、CVR（转化率）、ROI（投资回报率），以及观众停留时长和评论互动量，这些能反映素材吸引力和人群精准度。\n用户：如果发现某时段转化率低，该如何调整？\n系统：可先测试更换素材版本，若效果仍不佳则调整投放时段或扩大人群包，同时检查是否因竞品同期促销分流导致。\n用户：教育类大促有没有特殊的流量扶持政策？\n系统：平台对知识付费类产品有“学浪计划”扶持，需入驻学浪平台并完成ICP备案，符合条件可获得流量加权和佣金优惠。\n用户：那我们现在需要先完成ICP备案吗？流程复杂吗？\n系统：是的，经营性网站必须备案，流程包括提交营业执照、域名证书等材料，通常需要20个工作日，建议立即启动。\n用户：这么说我们得尽快准备ICP备案材料和教师资质文件了？\n\n当前问题：这么说我们得尽快准备ICP备案材料和教师资质文件了？\n请你根据任务规则输出重写结果。\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=0.5, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 21:25:40 [async_llm.py:252] Added request chatcmpl-fa088456b89d4b7c91aaa0427f6a0c83.
INFO:     127.0.0.1:45864 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-19 21:25:41 [logger.py:39] Received request chatcmpl-0f7e166465b3482e987f8375c69a2c74: prompt: '<｜begin▁of▁sentence｜>你是一个专业的问题重写模块，专门用于多轮对话场景下的指代补全与语义还原任务。\n请严格按照以下规则执行：\n\n【任务目标】\n1. 你的目标是准确解析用户真实意图，从历史中发掘指代和意图，使得这个独立问题尽量完整，尽可能包含所有信息关键词,特别是要补充关键的动机，指代和场景（润色后的问题至少涉及：用户面对的前置情况，什么限制，什么疑问等）。\n2. 仅当历史信息能够提供明确上下文时才进行补全，否则保持当前问题不变。\n3. 对于明显不需要补全的问题如：吹水："呵呵"，命令："关机"，无关内容："人生的意义"等，返回原句\n4. 不进行任何无关发挥、扩写、润色、修辞性描述、解释、总结、感情色彩。\n5. 不编造任何不存在的假设背景或新信息。\n6. 输出格式必须严格遵循：仅输出最终重写结果文本，不包含任何前缀、提示词、说明性文字或换行符。重写的疑问句以："我想知道"开头，非疑问句你自行适配\n7. 当无需重写时，直接输出原问题。\n\n【重写示例】\n历史对话：\n用户：小红鞋什么时候有货？\n系统：请问您指的是哪款小红鞋？\n用户：就是上次缺货那款\n当前问题：就是上次缺货那款\n重写结果：我想知道上次缺货的那款小红鞋什么时候有货？\n\n历史对话：\n用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：那我们还有其他计划，能投放吗？\n当前问题：那我们还有其他计划，能投放吗？\n重写结果：我想知道当账号处于限流状态时，是否可以继续开启新的广告投放计划？\n\n历史对话：\n用户：你是谁？\n当前问题：你是谁？\n重写结果：你是谁？\n\n历史对话：\n用户：开心\n当前问题：开心\n重写结果：开心\n\n历史对话：\n用户：我要买它\n系统：请问您指的是哪款商品？\n用户：之前推荐的那双\n当前问题：之前推荐的那双\n重写结果：我想要买你之前推荐的那双鞋。\n\n注意：严格按照以上风格工作，禁止任何额外输出。\n\nUser: 用户：我们最近计划在抖音做大促，但对流程不熟。\n系统：建议你先了解平台的大促节奏，比如双十一、618等，提前准备好素材和预算。\n用户：预算应该准备多少比较合适？\n系统：建议根据以往转化率和客单价评估，一般从5000元起步较稳妥。\n用户：素材方面有哪些注意事项？\n系统：内容要突出促销点，加入限时标签、优惠码等信息可以提升点击率。\n用户：那达人投放要现在联系吗？\n系统：是的，热门达人档期紧张，建议提前至少一周确定合作。\n用户：投放时间段有没有讲究？\n系统：高峰期一般集中在中午12点和晚上8点后，可以重点投放。\n用户：听说全域计划不错，我们也能用吗？\n系统：可以，全域计划适合有多渠道曝光需求的商家，需提前设置好人群和场景。\n用户：数据监控是用平台自带的吗？\n系统：平台提供实时监控面板，你也可以接入第三方分析工具。\n用户：那之前的数据能复用吗？\n系统：可以参考历史投放效果，特别是高转化素材和人群数据。\n用户：我们产品属于教育类，会有敏感问题吗？\n系统：教育类需要避免夸大效果和绝对化表述，确保内容合规。\n用户：那素材里提到“最好成绩”可以用吗？\n系统：建议用“显著提升”“客户反馈良好”等更安全的表达方式。\n用户：如果还是被判违规怎么办？\n系统：可以提交申诉，附上真实教学场景或用户反馈作为佐证。\n用户：这么说我们得提前准备这些证明材料？\n\n当前问题：这么说我们得提前准备这些证明材料？\n请你根据任务规则输出重写结果。\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=0.5, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 21:25:41 [async_llm.py:252] Added request chatcmpl-0f7e166465b3482e987f8375c69a2c74.
INFO:     127.0.0.1:45880 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-19 21:25:41 [logger.py:39] Received request chatcmpl-235cf77aec66464bb4f1ac5b80b9968c: prompt: '<｜begin▁of▁sentence｜>你是一个专业的问题重写模块，专门用于多轮对话场景下的指代补全与语义还原任务。\n请严格按照以下规则执行：\n\n【任务目标】\n1. 你的目标是准确解析用户真实意图，从历史中发掘指代和意图，使得这个独立问题尽量完整，尽可能包含所有信息关键词,特别是要补充关键的动机，指代和场景（润色后的问题至少涉及：用户面对的前置情况，什么限制，什么疑问等）。\n2. 仅当历史信息能够提供明确上下文时才进行补全，否则保持当前问题不变。\n3. 对于明显不需要补全的问题如：吹水："呵呵"，命令："关机"，无关内容："人生的意义"等，返回原句\n4. 不进行任何无关发挥、扩写、润色、修辞性描述、解释、总结、感情色彩。\n5. 不编造任何不存在的假设背景或新信息。\n6. 输出格式必须严格遵循：仅输出最终重写结果文本，不包含任何前缀、提示词、说明性文字或换行符。重写的疑问句以："我想知道"开头，非疑问句你自行适配\n7. 当无需重写时，直接输出原问题。\n\n【重写示例】\n历史对话：\n用户：小红鞋什么时候有货？\n系统：请问您指的是哪款小红鞋？\n用户：就是上次缺货那款\n当前问题：就是上次缺货那款\n重写结果：我想知道上次缺货的那款小红鞋什么时候有货？\n\n历史对话：\n用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：那我们还有其他计划，能投放吗？\n当前问题：那我们还有其他计划，能投放吗？\n重写结果：我想知道当账号处于限流状态时，是否可以继续开启新的广告投放计划？\n\n历史对话：\n用户：你是谁？\n当前问题：你是谁？\n重写结果：你是谁？\n\n历史对话：\n用户：开心\n当前问题：开心\n重写结果：开心\n\n历史对话：\n用户：我要买它\n系统：请问您指的是哪款商品？\n用户：之前推荐的那双\n当前问题：之前推荐的那双\n重写结果：我想要买你之前推荐的那双鞋。\n\n注意：严格按照以上风格工作，禁止任何额外输出。\n\nUser: 用户：我们最近收到平台的违规提醒，说素材违规。\n系统：你可以查看后台违规中心，看看是哪条素材触发了问题。\n用户：好像是说有敏感词。\n系统：敏感词包括绝对化用语、医疗效果等都容易违规。\n用户：我们只是写了‘最有效的’\n系统：‘最’属于绝对化表达，不建议使用。\n用户：那我们以后用‘推荐’可以吗？\n系统：‘推荐’是中性表达，一般可以使用。\n用户：我们是不是每次都要重新审核？\n系统：修改后重新提交即可，系统自动审核。\n用户：那我们现在的计划会被暂停吗？\n\n当前问题：那我们现在的计划会被暂停吗？\n请你根据任务规则输出重写结果。\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=0.5, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 21:25:41 [async_llm.py:252] Added request chatcmpl-235cf77aec66464bb4f1ac5b80b9968c.
INFO:     127.0.0.1:45866 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-19 21:25:41 [logger.py:39] Received request chatcmpl-9821bda0be0d4125ab1061e1594f38eb: prompt: '<｜begin▁of▁sentence｜>你是一个专业的问题重写模块，专门用于多轮对话场景下的指代补全与语义还原任务。\n请严格按照以下规则执行：\n\n【任务目标】\n1. 你的目标是准确解析用户真实意图，从历史中发掘指代和意图，使得这个独立问题尽量完整，尽可能包含所有信息关键词,特别是要补充关键的动机，指代和场景（润色后的问题至少涉及：用户面对的前置情况，什么限制，什么疑问等）。\n2. 仅当历史信息能够提供明确上下文时才进行补全，否则保持当前问题不变。\n3. 对于明显不需要补全的问题如：吹水："呵呵"，命令："关机"，无关内容："人生的意义"等，返回原句\n4. 不进行任何无关发挥、扩写、润色、修辞性描述、解释、总结、感情色彩。\n5. 不编造任何不存在的假设背景或新信息。\n6. 输出格式必须严格遵循：仅输出最终重写结果文本，不包含任何前缀、提示词、说明性文字或换行符。重写的疑问句以："我想知道"开头，非疑问句你自行适配\n7. 当无需重写时，直接输出原问题。\n\n【重写示例】\n历史对话：\n用户：小红鞋什么时候有货？\n系统：请问您指的是哪款小红鞋？\n用户：就是上次缺货那款\n当前问题：就是上次缺货那款\n重写结果：我想知道上次缺货的那款小红鞋什么时候有货？\n\n历史对话：\n用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：那我们还有其他计划，能投放吗？\n当前问题：那我们还有其他计划，能投放吗？\n重写结果：我想知道当账号处于限流状态时，是否可以继续开启新的广告投放计划？\n\n历史对话：\n用户：你是谁？\n当前问题：你是谁？\n重写结果：你是谁？\n\n历史对话：\n用户：开心\n当前问题：开心\n重写结果：开心\n\n历史对话：\n用户：我要买它\n系统：请问您指的是哪款商品？\n用户：之前推荐的那双\n当前问题：之前推荐的那双\n重写结果：我想要买你之前推荐的那双鞋。\n\n注意：严格按照以上风格工作，禁止任何额外输出。\n\nUser: 用户：我们最近准备上线夏季新款，打算做一波推广，有什么建议？\n系统：建议使用季节性主题投放计划，结合短视频内容更容易吸引用户。\n用户：短视频我们自己拍可以吗？\n系统：当然可以，平台也提供达人共创服务，拍摄更专业。\n用户：预算不多，达人费用会不会很贵？\n系统：可以选择中腰部达人合作，价格更友好，性价比高。\n用户：我们主营女装，有没有对应的达人推荐？\n系统：平台有女装领域达人库，可按风格、粉丝量筛选。\n用户：我们不太懂这些，能不能帮我们选？\n系统：你可以申请投放顾问服务，由平台专员帮你定制方案。\n用户：那直播和短视频要不要同时做？\n系统：同时开启效果最好，直播转化高，短视频补充流量。\n\n当前问题：直播和短视频要不要同时做？\n请你根据任务规则输出重写结果。\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=0.5, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 21:25:41 [async_llm.py:252] Added request chatcmpl-9821bda0be0d4125ab1061e1594f38eb.
INFO:     127.0.0.1:45860 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-19 21:25:41 [logger.py:39] Received request chatcmpl-be151899cf7b4ed4a92795ccfb5763a5: prompt: '<｜begin▁of▁sentence｜>你是一个专业的问题重写模块，专门用于多轮对话场景下的指代补全与语义还原任务。\n请严格按照以下规则执行：\n\n【任务目标】\n1. 你的目标是准确解析用户真实意图，从历史中发掘指代和意图，使得这个独立问题尽量完整，尽可能包含所有信息关键词,特别是要补充关键的动机，指代和场景（润色后的问题至少涉及：用户面对的前置情况，什么限制，什么疑问等）。\n2. 仅当历史信息能够提供明确上下文时才进行补全，否则保持当前问题不变。\n3. 对于明显不需要补全的问题如：吹水："呵呵"，命令："关机"，无关内容："人生的意义"等，返回原句\n4. 不进行任何无关发挥、扩写、润色、修辞性描述、解释、总结、感情色彩。\n5. 不编造任何不存在的假设背景或新信息。\n6. 输出格式必须严格遵循：仅输出最终重写结果文本，不包含任何前缀、提示词、说明性文字或换行符。重写的疑问句以："我想知道"开头，非疑问句你自行适配\n7. 当无需重写时，直接输出原问题。\n\n【重写示例】\n历史对话：\n用户：小红鞋什么时候有货？\n系统：请问您指的是哪款小红鞋？\n用户：就是上次缺货那款\n当前问题：就是上次缺货那款\n重写结果：我想知道上次缺货的那款小红鞋什么时候有货？\n\n历史对话：\n用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：那我们还有其他计划，能投放吗？\n当前问题：那我们还有其他计划，能投放吗？\n重写结果：我想知道当账号处于限流状态时，是否可以继续开启新的广告投放计划？\n\n历史对话：\n用户：你是谁？\n当前问题：你是谁？\n重写结果：你是谁？\n\n历史对话：\n用户：开心\n当前问题：开心\n重写结果：开心\n\n历史对话：\n用户：我要买它\n系统：请问您指的是哪款商品？\n用户：之前推荐的那双\n当前问题：之前推荐的那双\n重写结果：我想要买你之前推荐的那双鞋。\n\n注意：严格按照以上风格工作，禁止任何额外输出。\n\nUser: 用户：我刚接手账号，很多操作流程不熟悉\n系统：你可以参考平台的操作指引文档，帮助快速上手。\n用户：有没有视频教程？\n系统：有的，后台“帮助中心”中有详尽的视频教学。\n用户：我们之前跑的千川计划数据能看到吗？\n系统：可以在账户概览中查看历史投放效果，支持下载。\n用户：推广类型太多，我不知道该选哪个\n系统：可以根据你的产品类型和目标选择，如新品推荐适合定向引流。\n用户：我们有老客复购目标怎么办？\n系统：可以开启“老客回访计划”，提升复购率。\n用户：投放期间要每天优化吗？\n系统：建议每日观察数据波动并做微调，特别是投放初期\n用户：有没有自动优化功能？\n系统：可以开启智能调价和人群扩展，平台会自动优化。\n\n当前问题：有没有自动优化功能？\n请你根据任务规则输出重写结果。\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=0.5, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 21:25:41 [async_llm.py:252] Added request chatcmpl-be151899cf7b4ed4a92795ccfb5763a5.
INFO:     127.0.0.1:45880 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-19 21:25:41 [logger.py:39] Received request chatcmpl-1a1665d06eeb46f0b05c56d1d4493fc2: prompt: '<｜begin▁of▁sentence｜>你是一个专业的问题重写模块，专门用于多轮对话场景下的指代补全与语义还原任务。\n请严格按照以下规则执行：\n\n【任务目标】\n1. 你的目标是准确解析用户真实意图，从历史中发掘指代和意图，使得这个独立问题尽量完整，尽可能包含所有信息关键词,特别是要补充关键的动机，指代和场景（润色后的问题至少涉及：用户面对的前置情况，什么限制，什么疑问等）。\n2. 仅当历史信息能够提供明确上下文时才进行补全，否则保持当前问题不变。\n3. 对于明显不需要补全的问题如：吹水："呵呵"，命令："关机"，无关内容："人生的意义"等，返回原句\n4. 不进行任何无关发挥、扩写、润色、修辞性描述、解释、总结、感情色彩。\n5. 不编造任何不存在的假设背景或新信息。\n6. 输出格式必须严格遵循：仅输出最终重写结果文本，不包含任何前缀、提示词、说明性文字或换行符。重写的疑问句以："我想知道"开头，非疑问句你自行适配\n7. 当无需重写时，直接输出原问题。\n\n【重写示例】\n历史对话：\n用户：小红鞋什么时候有货？\n系统：请问您指的是哪款小红鞋？\n用户：就是上次缺货那款\n当前问题：就是上次缺货那款\n重写结果：我想知道上次缺货的那款小红鞋什么时候有货？\n\n历史对话：\n用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：那我们还有其他计划，能投放吗？\n当前问题：那我们还有其他计划，能投放吗？\n重写结果：我想知道当账号处于限流状态时，是否可以继续开启新的广告投放计划？\n\n历史对话：\n用户：你是谁？\n当前问题：你是谁？\n重写结果：你是谁？\n\n历史对话：\n用户：开心\n当前问题：开心\n重写结果：开心\n\n历史对话：\n用户：我要买它\n系统：请问您指的是哪款商品？\n用户：之前推荐的那双\n当前问题：之前推荐的那双\n重写结果：我想要买你之前推荐的那双鞋。\n\n注意：严格按照以上风格工作，禁止任何额外输出。\n\nUser: 用户：我们之前主推女装，现在开始转做童装了。\n系统：童装投放建议关注‘亲子人群’定向标签。\n用户：之前的计划还能用吗？\n系统：建议重新建计划并调整投放人群和素材方向。\n用户：素材方面我们要换吗？\n系统：是的，素材需与童装品类相匹配。\n用户：我们之前有一部分母婴粉，还能用吗？\n系统：可以基于历史人群创建相似人群包继续使用。\n用户：有没有推荐的时间段投放？\n系统：建议在早晚高峰或周末亲子时间段进行投放。\n用户：我们投放预算要调整吗？\n\n当前问题：我们投放预算要调整吗？\n请你根据任务规则输出重写结果。\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=0.5, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 21:25:41 [async_llm.py:252] Added request chatcmpl-1a1665d06eeb46f0b05c56d1d4493fc2.
INFO:     127.0.0.1:45864 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-19 21:25:41 [logger.py:39] Received request chatcmpl-c6fe6f0744c947ce84e7624b7d3be7d8: prompt: '<｜begin▁of▁sentence｜>你是一个专业的问题重写模块，专门用于多轮对话场景下的指代补全与语义还原任务。\n请严格按照以下规则执行：\n\n【任务目标】\n1. 你的目标是准确解析用户真实意图，从历史中发掘指代和意图，使得这个独立问题尽量完整，尽可能包含所有信息关键词,特别是要补充关键的动机，指代和场景（润色后的问题至少涉及：用户面对的前置情况，什么限制，什么疑问等）。\n2. 仅当历史信息能够提供明确上下文时才进行补全，否则保持当前问题不变。\n3. 对于明显不需要补全的问题如：吹水："呵呵"，命令："关机"，无关内容："人生的意义"等，返回原句\n4. 不进行任何无关发挥、扩写、润色、修辞性描述、解释、总结、感情色彩。\n5. 不编造任何不存在的假设背景或新信息。\n6. 输出格式必须严格遵循：仅输出最终重写结果文本，不包含任何前缀、提示词、说明性文字或换行符。重写的疑问句以："我想知道"开头，非疑问句你自行适配\n7. 当无需重写时，直接输出原问题。\n\n【重写示例】\n历史对话：\n用户：小红鞋什么时候有货？\n系统：请问您指的是哪款小红鞋？\n用户：就是上次缺货那款\n当前问题：就是上次缺货那款\n重写结果：我想知道上次缺货的那款小红鞋什么时候有货？\n\n历史对话：\n用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：那我们还有其他计划，能投放吗？\n当前问题：那我们还有其他计划，能投放吗？\n重写结果：我想知道当账号处于限流状态时，是否可以继续开启新的广告投放计划？\n\n历史对话：\n用户：你是谁？\n当前问题：你是谁？\n重写结果：你是谁？\n\n历史对话：\n用户：开心\n当前问题：开心\n重写结果：开心\n\n历史对话：\n用户：我要买它\n系统：请问您指的是哪款商品？\n用户：之前推荐的那双\n当前问题：之前推荐的那双\n重写结果：我想要买你之前推荐的那双鞋。\n\n注意：严格按照以上风格工作，禁止任何额外输出。\n\nUser: 用户：我们是线下商家，现在打算做线上引流，有推荐吗？\n系统：建议开启门店引流计划，支持设置门店范围和定向人群。\n用户：这种适合卖场类吗？\n系统：非常适合，尤其是结合地理位置定投能吸引附近用户。\n用户：那如果我们也想直播呢？\n系统：可以开设门店直播间，平台支持本地服务标签。\n用户：我们没有专业主播怎么办？\n系统：可申请官方培训，或招募兼职主播。\n用户：直播需要每天开吗？\n系统：建议固定时间开播，提升用户黏性。\n用户：我们品类比较杂，怎么规划内容？\n系统：可按品类做主题直播，如每周主推某类产品。\n\n当前问题：我们品类比较杂，怎么规划内容？\n请你根据任务规则输出重写结果。\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=0.5, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 21:25:41 [async_llm.py:252] Added request chatcmpl-c6fe6f0744c947ce84e7624b7d3be7d8.
INFO:     127.0.0.1:45866 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-19 21:25:41 [logger.py:39] Received request chatcmpl-5d5e2ca042004f1e9b361238ec759dc7: prompt: '<｜begin▁of▁sentence｜>你是一个专业的问题重写模块，专门用于多轮对话场景下的指代补全与语义还原任务。\n请严格按照以下规则执行：\n\n【任务目标】\n1. 你的目标是准确解析用户真实意图，从历史中发掘指代和意图，使得这个独立问题尽量完整，尽可能包含所有信息关键词,特别是要补充关键的动机，指代和场景（润色后的问题至少涉及：用户面对的前置情况，什么限制，什么疑问等）。\n2. 仅当历史信息能够提供明确上下文时才进行补全，否则保持当前问题不变。\n3. 对于明显不需要补全的问题如：吹水："呵呵"，命令："关机"，无关内容："人生的意义"等，返回原句\n4. 不进行任何无关发挥、扩写、润色、修辞性描述、解释、总结、感情色彩。\n5. 不编造任何不存在的假设背景或新信息。\n6. 输出格式必须严格遵循：仅输出最终重写结果文本，不包含任何前缀、提示词、说明性文字或换行符。重写的疑问句以："我想知道"开头，非疑问句你自行适配\n7. 当无需重写时，直接输出原问题。\n\n【重写示例】\n历史对话：\n用户：小红鞋什么时候有货？\n系统：请问您指的是哪款小红鞋？\n用户：就是上次缺货那款\n当前问题：就是上次缺货那款\n重写结果：我想知道上次缺货的那款小红鞋什么时候有货？\n\n历史对话：\n用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：那我们还有其他计划，能投放吗？\n当前问题：那我们还有其他计划，能投放吗？\n重写结果：我想知道当账号处于限流状态时，是否可以继续开启新的广告投放计划？\n\n历史对话：\n用户：你是谁？\n当前问题：你是谁？\n重写结果：你是谁？\n\n历史对话：\n用户：开心\n当前问题：开心\n重写结果：开心\n\n历史对话：\n用户：我要买它\n系统：请问您指的是哪款商品？\n用户：之前推荐的那双\n当前问题：之前推荐的那双\n重写结果：我想要买你之前推荐的那双鞋。\n\n注意：严格按照以上风格工作，禁止任何额外输出。\n\nUser: 用户：我们想在抖音推新款投影仪，大促怎么规划？\n系统：建议分预热期（3天）、爆发期（7天）、返场期（3天），预热期用短视频剧透功能亮点。\n用户：新品预售需要设置定金吗？比例多少合适？\n系统：建议定金比例10%-30%，需在详情页明确“定金不退”规则，避免法律风险。\n用户：素材里能对比竞品参数吗？比如亮度流明\n系统：可以对比，但需标注数据来源，禁止使用“碾压级”“吊打XX品牌”等攻击性表述。\n用户：达人直播能演示产品拆机吗？\n系统：可以，但需确保操作安全，避免展示内部电路缺陷，建议提前录制演示脚本。\n用户：以旧换新活动怎么和平台对接？\n系统：需在抖音小店后台开通“以旧换新”服务，填写回收商资质，用户下单后由第三方上门取件。\n用户：大促期间物流破损率高，怎么包装更安全？\n系统：建议使用双层瓦楞纸箱+泡沫填充物，外箱标注“易碎勿压”，购买物流责任险。\n用户：这么说我们得提前准备竞品对比数据和拆机脚本了？\n系统：是的，同时要确保对比数据真实可查，拆机演示需符合产品安全规范。\n\n当前问题：这么说我们得提前准备竞品对比数据和拆机脚本了？\n请你根据任务规则输出重写结果。\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=0.5, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 21:25:41 [async_llm.py:252] Added request chatcmpl-5d5e2ca042004f1e9b361238ec759dc7.
INFO:     127.0.0.1:45860 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-19 21:25:41 [logger.py:39] Received request chatcmpl-5a00d1efa4ad45f3bebb791f17f5a5e6: prompt: '<｜begin▁of▁sentence｜>你是一个专业的问题重写模块，专门用于多轮对话场景下的指代补全与语义还原任务。\n请严格按照以下规则执行：\n\n【任务目标】\n1. 你的目标是准确解析用户真实意图，从历史中发掘指代和意图，使得这个独立问题尽量完整，尽可能包含所有信息关键词,特别是要补充关键的动机，指代和场景（润色后的问题至少涉及：用户面对的前置情况，什么限制，什么疑问等）。\n2. 仅当历史信息能够提供明确上下文时才进行补全，否则保持当前问题不变。\n3. 对于明显不需要补全的问题如：吹水："呵呵"，命令："关机"，无关内容："人生的意义"等，返回原句\n4. 不进行任何无关发挥、扩写、润色、修辞性描述、解释、总结、感情色彩。\n5. 不编造任何不存在的假设背景或新信息。\n6. 输出格式必须严格遵循：仅输出最终重写结果文本，不包含任何前缀、提示词、说明性文字或换行符。重写的疑问句以："我想知道"开头，非疑问句你自行适配\n7. 当无需重写时，直接输出原问题。\n\n【重写示例】\n历史对话：\n用户：小红鞋什么时候有货？\n系统：请问您指的是哪款小红鞋？\n用户：就是上次缺货那款\n当前问题：就是上次缺货那款\n重写结果：我想知道上次缺货的那款小红鞋什么时候有货？\n\n历史对话：\n用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：那我们还有其他计划，能投放吗？\n当前问题：那我们还有其他计划，能投放吗？\n重写结果：我想知道当账号处于限流状态时，是否可以继续开启新的广告投放计划？\n\n历史对话：\n用户：你是谁？\n当前问题：你是谁？\n重写结果：你是谁？\n\n历史对话：\n用户：开心\n当前问题：开心\n重写结果：开心\n\n历史对话：\n用户：我要买它\n系统：请问您指的是哪款商品？\n用户：之前推荐的那双\n当前问题：之前推荐的那双\n重写结果：我想要买你之前推荐的那双鞋。\n\n注意：严格按照以上风格工作，禁止任何额外输出。\n\nUser: 用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：素材我们是用平台模板的也会违规？\n系统：如果文案、封面中有夸张描述也可能触发规则。\n用户：要怎么检查违规？\n系统：登录后台查看素材审核记录，有问题会有具体提示。\n用户：那账号还会恢复吗？\n系统：如果问题处理及时，一般48小时内可自动恢复。\n用户：需要申请解封吗？\n系统：如系统未自动恢复，可提交申诉申请。\n用户：我们还有其他计划，能投放吗？\n系统：建议暂停新投放，等账号状态正常后再开启。\n\n当前问题：我们还有其他计划，能投放吗？\n请你根据任务规则输出重写结果。\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=0.5, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 21:25:41 [async_llm.py:252] Added request chatcmpl-5a00d1efa4ad45f3bebb791f17f5a5e6.
INFO:     127.0.0.1:45880 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-19 21:25:41 [logger.py:39] Received request chatcmpl-8629ac03599a4f9b99a97c83f58ea9c4: prompt: '<｜begin▁of▁sentence｜>你是一个专业的问题重写模块，专门用于多轮对话场景下的指代补全与语义还原任务。\n请严格按照以下规则执行：\n\n【任务目标】\n1. 你的目标是准确解析用户真实意图，从历史中发掘指代和意图，使得这个独立问题尽量完整，尽可能包含所有信息关键词,特别是要补充关键的动机，指代和场景（润色后的问题至少涉及：用户面对的前置情况，什么限制，什么疑问等）。\n2. 仅当历史信息能够提供明确上下文时才进行补全，否则保持当前问题不变。\n3. 对于明显不需要补全的问题如：吹水："呵呵"，命令："关机"，无关内容："人生的意义"等，返回原句\n4. 不进行任何无关发挥、扩写、润色、修辞性描述、解释、总结、感情色彩。\n5. 不编造任何不存在的假设背景或新信息。\n6. 输出格式必须严格遵循：仅输出最终重写结果文本，不包含任何前缀、提示词、说明性文字或换行符。重写的疑问句以："我想知道"开头，非疑问句你自行适配\n7. 当无需重写时，直接输出原问题。\n\n【重写示例】\n历史对话：\n用户：小红鞋什么时候有货？\n系统：请问您指的是哪款小红鞋？\n用户：就是上次缺货那款\n当前问题：就是上次缺货那款\n重写结果：我想知道上次缺货的那款小红鞋什么时候有货？\n\n历史对话：\n用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：那我们还有其他计划，能投放吗？\n当前问题：那我们还有其他计划，能投放吗？\n重写结果：我想知道当账号处于限流状态时，是否可以继续开启新的广告投放计划？\n\n历史对话：\n用户：你是谁？\n当前问题：你是谁？\n重写结果：你是谁？\n\n历史对话：\n用户：开心\n当前问题：开心\n重写结果：开心\n\n历史对话：\n用户：我要买它\n系统：请问您指的是哪款商品？\n用户：之前推荐的那双\n当前问题：之前推荐的那双\n重写结果：我想要买你之前推荐的那双鞋。\n\n注意：严格按照以上风格工作，禁止任何额外输出。\n\nUser: 用户：我们最近想尝试图文推广，听说效果不错？\n系统：是的，图文内容在信息流中能快速吸引用户注意。\n用户：我们没有设计师，图要怎么做？\n系统：平台有模板和智能生成工具可用，不需要专业设计。\n用户：文案方面也有建议吗？\n系统：建议突出利益点和优惠信息，简洁直白。\n用户：我们属于母婴类产品，有什么特别注意的？\n系统：母婴类需注意合规表达，不能含有医疗或虚假描述。\n用户：那能不能写‘有效缓解’？\n系统：这种表述建议避免，可能涉及功效性用语。\n用户：那推广中出现用户评价可以吗？\n系统：可以使用，但需要标注为用户体验，不可夸大。\n\n当前问题：那推广中出现用户评价可以吗？\n请你根据任务规则输出重写结果。\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=0.5, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 21:25:41 [async_llm.py:252] Added request chatcmpl-8629ac03599a4f9b99a97c83f58ea9c4.
INFO:     127.0.0.1:45864 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-19 21:25:41 [logger.py:39] Received request chatcmpl-949f234e4f42411eb5aa67c16aaab994: prompt: '<｜begin▁of▁sentence｜>你是一个专业的问题重写模块，专门用于多轮对话场景下的指代补全与语义还原任务。\n请严格按照以下规则执行：\n\n【任务目标】\n1. 你的目标是准确解析用户真实意图，从历史中发掘指代和意图，使得这个独立问题尽量完整，尽可能包含所有信息关键词,特别是要补充关键的动机，指代和场景（润色后的问题至少涉及：用户面对的前置情况，什么限制，什么疑问等）。\n2. 仅当历史信息能够提供明确上下文时才进行补全，否则保持当前问题不变。\n3. 对于明显不需要补全的问题如：吹水："呵呵"，命令："关机"，无关内容："人生的意义"等，返回原句\n4. 不进行任何无关发挥、扩写、润色、修辞性描述、解释、总结、感情色彩。\n5. 不编造任何不存在的假设背景或新信息。\n6. 输出格式必须严格遵循：仅输出最终重写结果文本，不包含任何前缀、提示词、说明性文字或换行符。重写的疑问句以："我想知道"开头，非疑问句你自行适配\n7. 当无需重写时，直接输出原问题。\n\n【重写示例】\n历史对话：\n用户：小红鞋什么时候有货？\n系统：请问您指的是哪款小红鞋？\n用户：就是上次缺货那款\n当前问题：就是上次缺货那款\n重写结果：我想知道上次缺货的那款小红鞋什么时候有货？\n\n历史对话：\n用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：那我们还有其他计划，能投放吗？\n当前问题：那我们还有其他计划，能投放吗？\n重写结果：我想知道当账号处于限流状态时，是否可以继续开启新的广告投放计划？\n\n历史对话：\n用户：你是谁？\n当前问题：你是谁？\n重写结果：你是谁？\n\n历史对话：\n用户：开心\n当前问题：开心\n重写结果：开心\n\n历史对话：\n用户：我要买它\n系统：请问您指的是哪款商品？\n用户：之前推荐的那双\n当前问题：之前推荐的那双\n重写结果：我想要买你之前推荐的那双鞋。\n\n注意：严格按照以上风格工作，禁止任何额外输出。\n\nUser: 用户：美妆大促推精华液，素材怎么突出抗老效果？\n系统：可展示临床测试数据（如“28天淡化细纹23%”），但需标注“实验室数据，效果因人而异”。\n用户：成分表需要全部展示吗？比如视黄醇浓度\n系统：核心成分需标注，浓度超过1%需特别提醒，避免引发敏感肌用户投诉。\n用户：达人直播能现场测试过敏吗？\n系统：禁止现场做过敏测试，建议用斑贴试验报告代替，或强调“建议先做耳后测试”。\n用户：赠品小样的保质期怎么标注？\n系统：需在包装醒目位置标注“赠品”和“限用日期”，临期小样（<6个月）禁止赠送。\n用户：千川投放选“美妆爱好者”还是“成分党”人群？\n系统：抗老精华选“30+女性+成分党”组合，可叠加“护肤技巧”“抗老话题”兴趣标签。\n用户：平台对“医美级”表述有限制吗？\n系统：“医美级”“医学护肤”属于敏感词，可用“实验室研发”“皮肤科医生推荐”替代。\n用户：这么说我们得调整素材里的敏感表述和赠品日期了？\n系统：是的，同时要确保临床数据有完整检测报告存档。\n\n当前问题：这么说我们得调整素材里的敏感表述和赠品日期了？\n请你根据任务规则输出重写结果。\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=0.5, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 21:25:41 [async_llm.py:252] Added request chatcmpl-949f234e4f42411eb5aa67c16aaab994.
INFO:     127.0.0.1:45880 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-19 21:25:41 [logger.py:39] Received request chatcmpl-e26efb1cbd3d463cb4c95f256254e311: prompt: '<｜begin▁of▁sentence｜>你是一个专业的问题重写模块，专门用于多轮对话场景下的指代补全与语义还原任务。\n请严格按照以下规则执行：\n\n【任务目标】\n1. 你的目标是准确解析用户真实意图，从历史中发掘指代和意图，使得这个独立问题尽量完整，尽可能包含所有信息关键词,特别是要补充关键的动机，指代和场景（润色后的问题至少涉及：用户面对的前置情况，什么限制，什么疑问等）。\n2. 仅当历史信息能够提供明确上下文时才进行补全，否则保持当前问题不变。\n3. 对于明显不需要补全的问题如：吹水："呵呵"，命令："关机"，无关内容："人生的意义"等，返回原句\n4. 不进行任何无关发挥、扩写、润色、修辞性描述、解释、总结、感情色彩。\n5. 不编造任何不存在的假设背景或新信息。\n6. 输出格式必须严格遵循：仅输出最终重写结果文本，不包含任何前缀、提示词、说明性文字或换行符。重写的疑问句以："我想知道"开头，非疑问句你自行适配\n7. 当无需重写时，直接输出原问题。\n\n【重写示例】\n历史对话：\n用户：小红鞋什么时候有货？\n系统：请问您指的是哪款小红鞋？\n用户：就是上次缺货那款\n当前问题：就是上次缺货那款\n重写结果：我想知道上次缺货的那款小红鞋什么时候有货？\n\n历史对话：\n用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：那我们还有其他计划，能投放吗？\n当前问题：那我们还有其他计划，能投放吗？\n重写结果：我想知道当账号处于限流状态时，是否可以继续开启新的广告投放计划？\n\n历史对话：\n用户：你是谁？\n当前问题：你是谁？\n重写结果：你是谁？\n\n历史对话：\n用户：开心\n当前问题：开心\n重写结果：开心\n\n历史对话：\n用户：我要买它\n系统：请问您指的是哪款商品？\n用户：之前推荐的那双\n当前问题：之前推荐的那双\n重写结果：我想要买你之前推荐的那双鞋。\n\n注意：严格按照以上风格工作，禁止任何额外输出。\n\nUser: 用户：家纺大促想推四件套，怎么提升客单价？\n系统：可设置“床品套餐”（床单+被罩+枕套），搭配“满300减50”跨店满减。\n用户：素材里能宣传“抗菌防螨”吗？需要检测报告吗？\n系统：可以宣传，但需在详情页附省级以上机构的抗菌检测报告，标注“检测范围：面料表层”。\n用户：达人直播展示洗涤测试，需要注意什么？\n系统：洗涤次数需与实际产品说明一致，禁止演示“暴力搓洗不变形”等夸大场景。\n用户：大促期间退换货率高，怎么降低？\n系统：建议在详情页增加“尺寸测量视频”，提供免费退换货运费险，注明“水洗后不支持退换”。\n用户：平台对“纯棉”材质有认证要求吗？\n系统：需符合GB/T 29862-2013标准，含棉量≥95%可标注“纯棉”，否则需标注具体比例。\n用户：这么说我们得准备抗菌报告和材质检测文件了？\n系统：是的，建议将报告上传至抖音小店“资质中心”，以备平台抽查。\n\n当前问题：这么说我们得准备抗菌报告和材质检测文件了？\n请你根据任务规则输出重写结果。\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=0.5, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 21:25:41 [async_llm.py:252] Added request chatcmpl-e26efb1cbd3d463cb4c95f256254e311.
INFO:     127.0.0.1:45860 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-19 21:25:41 [logger.py:39] Received request chatcmpl-cdbf86ce2a694c6bbfd7500b158cdb36: prompt: '<｜begin▁of▁sentence｜>你是一个专业的问题重写模块，专门用于多轮对话场景下的指代补全与语义还原任务。\n请严格按照以下规则执行：\n\n【任务目标】\n1. 你的目标是准确解析用户真实意图，从历史中发掘指代和意图，使得这个独立问题尽量完整，尽可能包含所有信息关键词,特别是要补充关键的动机，指代和场景（润色后的问题至少涉及：用户面对的前置情况，什么限制，什么疑问等）。\n2. 仅当历史信息能够提供明确上下文时才进行补全，否则保持当前问题不变。\n3. 对于明显不需要补全的问题如：吹水："呵呵"，命令："关机"，无关内容："人生的意义"等，返回原句\n4. 不进行任何无关发挥、扩写、润色、修辞性描述、解释、总结、感情色彩。\n5. 不编造任何不存在的假设背景或新信息。\n6. 输出格式必须严格遵循：仅输出最终重写结果文本，不包含任何前缀、提示词、说明性文字或换行符。重写的疑问句以："我想知道"开头，非疑问句你自行适配\n7. 当无需重写时，直接输出原问题。\n\n【重写示例】\n历史对话：\n用户：小红鞋什么时候有货？\n系统：请问您指的是哪款小红鞋？\n用户：就是上次缺货那款\n当前问题：就是上次缺货那款\n重写结果：我想知道上次缺货的那款小红鞋什么时候有货？\n\n历史对话：\n用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：那我们还有其他计划，能投放吗？\n当前问题：那我们还有其他计划，能投放吗？\n重写结果：我想知道当账号处于限流状态时，是否可以继续开启新的广告投放计划？\n\n历史对话：\n用户：你是谁？\n当前问题：你是谁？\n重写结果：你是谁？\n\n历史对话：\n用户：开心\n当前问题：开心\n重写结果：开心\n\n历史对话：\n用户：我要买它\n系统：请问您指的是哪款商品？\n用户：之前推荐的那双\n当前问题：之前推荐的那双\n重写结果：我想要买你之前推荐的那双鞋。\n\n注意：严格按照以上风格工作，禁止任何额外输出。\n\nUser: 用户：我们母婴店想参加抖音618大促，不知道从哪入手？\n系统：首先要明确大促目标，是清库存还是拉新客？建议先优化商品橱窗的促销标签。\n用户：母婴产品促销有啥特别规则吗？比如奶粉\n系统：奶粉属于特殊类目，需提供食品经营许可证和品牌授权书，且不能宣传“婴幼儿专用”以外的功效。\n用户：那纸尿裤素材怎么突出卖点？\n系统：可以对比吸收量、透气性等实测数据，但不得使用“最柔软”“零红屁屁”等绝对化用语。\n用户：达人合作这块，选母婴垂类还是泛亲子类？\n系统：垂类达人转化率更高，但泛类达人曝光更广，建议搭建“头部垂类+腰部泛类”的组合矩阵。\n用户：达人直播时能卖预售商品吗？\n系统：可以，但需在直播间明确标注预售周期（如“7天内发货”），避免消费者投诉。\n用户：大促期间物流压力大，怎么避免售后纠纷？\n系统：建议提前和快递公司签订保价协议，发货时附加防拆封贴纸，并在详情页公示退换货政策。\n用户：售后客服需要培训哪些内容？\n系统：重点培训尺码咨询、过敏理赔、临期商品处理等场景，准备好常见问题QA文档。\n用户：千川投放选极速推广还是专业推广？\n系统：极速推广适合新手快速起量，专业推广适合精细化人群运营，建议先用极速版测试素材点击率。\n用户：人群定向选“妈妈群体”还是“备孕女性”？\n系统：根据产品阶段选择：奶粉辅食选“0-3岁妈妈”，孕妇装选“备孕-孕晚期”，可搭配“育儿知识”兴趣标签。\n用户：促销价设置错了怎么办？能立刻改吗？\n系统：若未产生订单可直接修改；若已售出，需联系用户协商补差价或取消订单，避免被平台处罚。\n用户：平台对赠品有啥要求？比如买奶粉送玩具\n系统：赠品需符合《反不正当竞争法》，需标注“赠品”字样，且玩具类赠品需提供3C认证，母婴类赠品禁止过期或临期。\n用户：数据复盘时发现达人直播间退货率高，怎么回事？\n系统：可能是直播间话术夸大功效，或物流时效未达标。建议对比直播间承诺与详情页描述是否一致。\n用户：那我们需要提前准备赠品的资质文件吗？\n系统：是的，需准备赠品的质检报告、品牌授权（如有），母婴类赠品还需通过GB6675等国家标准检测。\n用户：大促期间客服响应慢会被处罚吗？\n系统：平台考核“3分钟回复率”，低于50%会影响店铺评级，建议开启智能客服预答常见问题。\n用户：智能客服能处理售后纠纷吗？\n系统：复杂纠纷需人工介入，但智能客服可先收集用户问题类型，分类转接到对应售后小组。\n用户：这么说我们得提前整理赠品资质和客服QA了？\n\n当前问题：这么说我们得提前整理赠品资质和客服QA了？\n请你根据任务规则输出重写结果。\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=0.5, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 21:25:41 [async_llm.py:252] Added request chatcmpl-cdbf86ce2a694c6bbfd7500b158cdb36.
INFO:     127.0.0.1:45866 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:45864 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-19 21:25:41 [logger.py:39] Received request chatcmpl-251fe0519a9b4d9bb8645a2baa780ce1: prompt: '<｜begin▁of▁sentence｜>你是一个专业的问题重写模块，专门用于多轮对话场景下的指代补全与语义还原任务。\n请严格按照以下规则执行：\n\n【任务目标】\n1. 你的目标是准确解析用户真实意图，从历史中发掘指代和意图，使得这个独立问题尽量完整，尽可能包含所有信息关键词,特别是要补充关键的动机，指代和场景（润色后的问题至少涉及：用户面对的前置情况，什么限制，什么疑问等）。\n2. 仅当历史信息能够提供明确上下文时才进行补全，否则保持当前问题不变。\n3. 对于明显不需要补全的问题如：吹水："呵呵"，命令："关机"，无关内容："人生的意义"等，返回原句\n4. 不进行任何无关发挥、扩写、润色、修辞性描述、解释、总结、感情色彩。\n5. 不编造任何不存在的假设背景或新信息。\n6. 输出格式必须严格遵循：仅输出最终重写结果文本，不包含任何前缀、提示词、说明性文字或换行符。重写的疑问句以："我想知道"开头，非疑问句你自行适配\n7. 当无需重写时，直接输出原问题。\n\n【重写示例】\n历史对话：\n用户：小红鞋什么时候有货？\n系统：请问您指的是哪款小红鞋？\n用户：就是上次缺货那款\n当前问题：就是上次缺货那款\n重写结果：我想知道上次缺货的那款小红鞋什么时候有货？\n\n历史对话：\n用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：那我们还有其他计划，能投放吗？\n当前问题：那我们还有其他计划，能投放吗？\n重写结果：我想知道当账号处于限流状态时，是否可以继续开启新的广告投放计划？\n\n历史对话：\n用户：你是谁？\n当前问题：你是谁？\n重写结果：你是谁？\n\n历史对话：\n用户：开心\n当前问题：开心\n重写结果：开心\n\n历史对话：\n用户：我要买它\n系统：请问您指的是哪款商品？\n用户：之前推荐的那双\n当前问题：之前推荐的那双\n重写结果：我想要买你之前推荐的那双鞋。\n\n注意：严格按照以上风格工作，禁止任何额外输出。\n\nUser: 用户：生鲜大促推草莓，怎么保证运输新鲜度？\n系统：建议采用航空冷链运输，包装内放置冰袋+透气孔，标注“48小时直达”。\n用户：宣传“有机认证”需要什么材料？\n系统：需提供《有机产品认证证书》，且认证范围包含具体品类（如“草莓”），有效期内。\n用户：素材里能展示农药残留检测结果吗？\n系统：可以，但需完整展示检测机构名称、样品编号和未检出项目，禁止截取部分数据。\n用户：达人直播能试吃未清洗的水果吗？\n系统：禁止试吃生食材，建议展示清洗过程或提供加工后的试吃场景（如草莓沙拉）。\n用户：临期食品怎么处理？能打折促销吗？\n系统：保质期≤1年的食品，临期界定为到期前45天，需在标题标注“临期特惠”并公示剩余保质期。\n用户：平台对食品添加剂有啥要求？\n系统：需在详情页完整列出添加剂名称，符合GB 2760-2014标准，禁止使用“零添加”等模糊表述。\n用户：这么说我们得提前备案有机证书和检测报告了？\n系统：是的，生鲜类目审核严格，建议所有资质提前15天提交平台审核。\n\n当前问题：这么说我们得提前备案有机证书和检测报告了？\n请你根据任务规则输出重写结果。\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=0.5, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 21:25:41 [async_llm.py:252] Added request chatcmpl-251fe0519a9b4d9bb8645a2baa780ce1.
INFO 06-19 21:25:41 [logger.py:39] Received request chatcmpl-f194ba93e0ce46fdbe5a793e7de548d9: prompt: '<｜begin▁of▁sentence｜>你是一个专业的问题重写模块，专门用于多轮对话场景下的指代补全与语义还原任务。\n请严格按照以下规则执行：\n\n【任务目标】\n1. 你的目标是准确解析用户真实意图，从历史中发掘指代和意图，使得这个独立问题尽量完整，尽可能包含所有信息关键词,特别是要补充关键的动机，指代和场景（润色后的问题至少涉及：用户面对的前置情况，什么限制，什么疑问等）。\n2. 仅当历史信息能够提供明确上下文时才进行补全，否则保持当前问题不变。\n3. 对于明显不需要补全的问题如：吹水："呵呵"，命令："关机"，无关内容："人生的意义"等，返回原句\n4. 不进行任何无关发挥、扩写、润色、修辞性描述、解释、总结、感情色彩。\n5. 不编造任何不存在的假设背景或新信息。\n6. 输出格式必须严格遵循：仅输出最终重写结果文本，不包含任何前缀、提示词、说明性文字或换行符。重写的疑问句以："我想知道"开头，非疑问句你自行适配\n7. 当无需重写时，直接输出原问题。\n\n【重写示例】\n历史对话：\n用户：小红鞋什么时候有货？\n系统：请问您指的是哪款小红鞋？\n用户：就是上次缺货那款\n当前问题：就是上次缺货那款\n重写结果：我想知道上次缺货的那款小红鞋什么时候有货？\n\n历史对话：\n用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：那我们还有其他计划，能投放吗？\n当前问题：那我们还有其他计划，能投放吗？\n重写结果：我想知道当账号处于限流状态时，是否可以继续开启新的广告投放计划？\n\n历史对话：\n用户：你是谁？\n当前问题：你是谁？\n重写结果：你是谁？\n\n历史对话：\n用户：开心\n当前问题：开心\n重写结果：开心\n\n历史对话：\n用户：我要买它\n系统：请问您指的是哪款商品？\n用户：之前推荐的那双\n当前问题：之前推荐的那双\n重写结果：我想要买你之前推荐的那双鞋。\n\n注意：严格按照以上风格工作，禁止任何额外输出。\n\nUser: 用户：我们现在账号刚过冷启动，打算做点投放优化。\n系统：冷启动后建议从人群定向和素材优化入手。\n用户：人群要怎么定？\n系统：可以选择兴趣标签、人群包，或者使用智能放量模式。\n用户：我们之前选的是智能放量，投产不稳定。\n系统：那可以先圈定核心人群，逐步放量测试。\n用户：素材我们是用老的视频，会不会有影响？\n系统：建议结合冷启动阶段的数据，更新点击率低的素材。\n用户：素材频繁更换会不会影响审核？\n系统：只要不违规，频繁更换素材不会有审核问题。\n用户：那要怎么知道是素材的问题？\n系统：建议查看每条素材的CTR和转化率，低于平均就该替换。\n\n当前问题：那要怎么知道是素材的问题？\n请你根据任务规则输出重写结果。\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=0.5, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 21:25:41 [async_llm.py:252] Added request chatcmpl-f194ba93e0ce46fdbe5a793e7de548d9.
INFO:     127.0.0.1:45866 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-19 21:25:41 [logger.py:39] Received request chatcmpl-a503d6212e4944eabf251c3522c18936: prompt: '<｜begin▁of▁sentence｜>你是一个专业的问题重写模块，专门用于多轮对话场景下的指代补全与语义还原任务。\n请严格按照以下规则执行：\n\n【任务目标】\n1. 你的目标是准确解析用户真实意图，从历史中发掘指代和意图，使得这个独立问题尽量完整，尽可能包含所有信息关键词,特别是要补充关键的动机，指代和场景（润色后的问题至少涉及：用户面对的前置情况，什么限制，什么疑问等）。\n2. 仅当历史信息能够提供明确上下文时才进行补全，否则保持当前问题不变。\n3. 对于明显不需要补全的问题如：吹水："呵呵"，命令："关机"，无关内容："人生的意义"等，返回原句\n4. 不进行任何无关发挥、扩写、润色、修辞性描述、解释、总结、感情色彩。\n5. 不编造任何不存在的假设背景或新信息。\n6. 输出格式必须严格遵循：仅输出最终重写结果文本，不包含任何前缀、提示词、说明性文字或换行符。重写的疑问句以："我想知道"开头，非疑问句你自行适配\n7. 当无需重写时，直接输出原问题。\n\n【重写示例】\n历史对话：\n用户：小红鞋什么时候有货？\n系统：请问您指的是哪款小红鞋？\n用户：就是上次缺货那款\n当前问题：就是上次缺货那款\n重写结果：我想知道上次缺货的那款小红鞋什么时候有货？\n\n历史对话：\n用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：那我们还有其他计划，能投放吗？\n当前问题：那我们还有其他计划，能投放吗？\n重写结果：我想知道当账号处于限流状态时，是否可以继续开启新的广告投放计划？\n\n历史对话：\n用户：你是谁？\n当前问题：你是谁？\n重写结果：你是谁？\n\n历史对话：\n用户：开心\n当前问题：开心\n重写结果：开心\n\n历史对话：\n用户：我要买它\n系统：请问您指的是哪款商品？\n用户：之前推荐的那双\n当前问题：之前推荐的那双\n重写结果：我想要买你之前推荐的那双鞋。\n\n注意：严格按照以上风格工作，禁止任何额外输出。\n\nUser: 用户：我们最近活动期投放转化低。\n系统：活动期间竞争大，建议提升素材吸引力和时效性。\n用户：我们用的是老素材。\n系统：建议针对活动定制短视频，增加互动元素。\n用户：预算也没太大提升。\n系统：活动期应适当提升预算，否则流量不足。\n用户：如果我们错峰投放，有用吗？\n系统：错峰有助于避开高价时段，提升ROI。\n用户：转化低是不是平台限流？\n系统：限流可能性小，重点看CTR和落地页转化。\n\n当前问题：转化低是不是平台限流？\n请你根据任务规则输出重写结果。\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=0.5, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 21:25:41 [async_llm.py:252] Added request chatcmpl-a503d6212e4944eabf251c3522c18936.
INFO:     127.0.0.1:45880 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-19 21:25:41 [logger.py:39] Received request chatcmpl-1385304229b14eac9975f6fe932eccc8: prompt: '<｜begin▁of▁sentence｜>你是一个专业的问题重写模块，专门用于多轮对话场景下的指代补全与语义还原任务。\n请严格按照以下规则执行：\n\n【任务目标】\n1. 你的目标是准确解析用户真实意图，从历史中发掘指代和意图，使得这个独立问题尽量完整，尽可能包含所有信息关键词,特别是要补充关键的动机，指代和场景（润色后的问题至少涉及：用户面对的前置情况，什么限制，什么疑问等）。\n2. 仅当历史信息能够提供明确上下文时才进行补全，否则保持当前问题不变。\n3. 对于明显不需要补全的问题如：吹水："呵呵"，命令："关机"，无关内容："人生的意义"等，返回原句\n4. 不进行任何无关发挥、扩写、润色、修辞性描述、解释、总结、感情色彩。\n5. 不编造任何不存在的假设背景或新信息。\n6. 输出格式必须严格遵循：仅输出最终重写结果文本，不包含任何前缀、提示词、说明性文字或换行符。重写的疑问句以："我想知道"开头，非疑问句你自行适配\n7. 当无需重写时，直接输出原问题。\n\n【重写示例】\n历史对话：\n用户：小红鞋什么时候有货？\n系统：请问您指的是哪款小红鞋？\n用户：就是上次缺货那款\n当前问题：就是上次缺货那款\n重写结果：我想知道上次缺货的那款小红鞋什么时候有货？\n\n历史对话：\n用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：那我们还有其他计划，能投放吗？\n当前问题：那我们还有其他计划，能投放吗？\n重写结果：我想知道当账号处于限流状态时，是否可以继续开启新的广告投放计划？\n\n历史对话：\n用户：你是谁？\n当前问题：你是谁？\n重写结果：你是谁？\n\n历史对话：\n用户：开心\n当前问题：开心\n重写结果：开心\n\n历史对话：\n用户：我要买它\n系统：请问您指的是哪款商品？\n用户：之前推荐的那双\n当前问题：之前推荐的那双\n重写结果：我想要买你之前推荐的那双鞋。\n\n注意：严格按照以上风格工作，禁止任何额外输出。\n\nUser: 用户：跨境大促推进口保健品，需要啥资质？\n系统：需提供海关报关单、检验检疫证明、中文标签备案，保健食品需额外提供《进口保健食品批准证书》。\n用户：素材里能宣传“国外医院推荐”吗？\n系统：禁止使用医疗机构推荐用语，可展示“XX国家销量TOP1”等市场数据，但需标注数据来源。\n用户：保税仓发货和直邮有啥区别？\n系统：保税仓发货更快（2-3天），需提前备货；直邮周期长（7-15天），但库存压力小，可根据大促节奏选择。\n用户：关税怎么计算？会影响到手价吗？\n系统：跨境商品适用“跨境电商综合税”，税率一般为9.1%-26.3%，建议在商品页标注“税费自理”或包税政策。\n用户：用户投诉商品与描述不符怎么处理？\n系统：需提供国际物流单号、商品实拍图对比，若确属描述问题，需承担退货运费及可能的关税损失。\n用户：这么说我们得确保所有进口资质和宣传数据可溯源了？\n系统：是的，跨境商品一旦违规可能面临下架和罚款，建议建立资质台账管理制度。\n\n当前问题：这么说我们得确保所有进口资质和宣传数据可溯源了？\n请你根据任务规则输出重写结果。\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=0.5, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 21:25:41 [async_llm.py:252] Added request chatcmpl-1385304229b14eac9975f6fe932eccc8.
INFO:     127.0.0.1:45864 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-19 21:25:41 [logger.py:39] Received request chatcmpl-ce5d79f3bfa44d4abfce9b186f07a347: prompt: '<｜begin▁of▁sentence｜>你是一个专业的问题重写模块，专门用于多轮对话场景下的指代补全与语义还原任务。\n请严格按照以下规则执行：\n\n【任务目标】\n1. 你的目标是准确解析用户真实意图，从历史中发掘指代和意图，使得这个独立问题尽量完整，尽可能包含所有信息关键词,特别是要补充关键的动机，指代和场景（润色后的问题至少涉及：用户面对的前置情况，什么限制，什么疑问等）。\n2. 仅当历史信息能够提供明确上下文时才进行补全，否则保持当前问题不变。\n3. 对于明显不需要补全的问题如：吹水："呵呵"，命令："关机"，无关内容："人生的意义"等，返回原句\n4. 不进行任何无关发挥、扩写、润色、修辞性描述、解释、总结、感情色彩。\n5. 不编造任何不存在的假设背景或新信息。\n6. 输出格式必须严格遵循：仅输出最终重写结果文本，不包含任何前缀、提示词、说明性文字或换行符。重写的疑问句以："我想知道"开头，非疑问句你自行适配\n7. 当无需重写时，直接输出原问题。\n\n【重写示例】\n历史对话：\n用户：小红鞋什么时候有货？\n系统：请问您指的是哪款小红鞋？\n用户：就是上次缺货那款\n当前问题：就是上次缺货那款\n重写结果：我想知道上次缺货的那款小红鞋什么时候有货？\n\n历史对话：\n用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：那我们还有其他计划，能投放吗？\n当前问题：那我们还有其他计划，能投放吗？\n重写结果：我想知道当账号处于限流状态时，是否可以继续开启新的广告投放计划？\n\n历史对话：\n用户：你是谁？\n当前问题：你是谁？\n重写结果：你是谁？\n\n历史对话：\n用户：开心\n当前问题：开心\n重写结果：开心\n\n历史对话：\n用户：我要买它\n系统：请问您指的是哪款商品？\n用户：之前推荐的那双\n当前问题：之前推荐的那双\n重写结果：我想要买你之前推荐的那双鞋。\n\n注意：严格按照以上风格工作，禁止任何额外输出。\n\nUser: 用户：我们活动期间跑了个直播场，但效果很一般。\n系统：是否有提前预热或预约？预热不足会影响转化。\n用户：我们只是当天早上发了通知。\n系统：建议提前至少两天，通过短视频和图文做预热。\n用户：达人带播费用太高，我们就自己上了。\n系统：品牌自播也能做出效果，但需要内容结构更清晰。\n用户：我们没带节奏，就是一直讲产品。\n系统：建议设置互动环节和限时福利，提升停留和转化。\n用户：那脚本我们需要准备吗？\n系统：最好有脚本提纲，控制节奏，也方便数据分析。\n用户：我们下次活动还有机会改进吗？\n\n当前问题：我们下次活动还有机会改进吗？\n请你根据任务规则输出重写结果。\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=0.5, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 21:25:41 [async_llm.py:252] Added request chatcmpl-ce5d79f3bfa44d4abfce9b186f07a347.
INFO:     127.0.0.1:45860 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-19 21:25:41 [logger.py:39] Received request chatcmpl-35c9d38091cf4f0d9f9ec16b0156249b: prompt: '<｜begin▁of▁sentence｜>你是一个专业的问题重写模块，专门用于多轮对话场景下的指代补全与语义还原任务。\n请严格按照以下规则执行：\n\n【任务目标】\n1. 你的目标是准确解析用户真实意图，从历史中发掘指代和意图，使得这个独立问题尽量完整，尽可能包含所有信息关键词,特别是要补充关键的动机，指代和场景（润色后的问题至少涉及：用户面对的前置情况，什么限制，什么疑问等）。\n2. 仅当历史信息能够提供明确上下文时才进行补全，否则保持当前问题不变。\n3. 对于明显不需要补全的问题如：吹水："呵呵"，命令："关机"，无关内容："人生的意义"等，返回原句\n4. 不进行任何无关发挥、扩写、润色、修辞性描述、解释、总结、感情色彩。\n5. 不编造任何不存在的假设背景或新信息。\n6. 输出格式必须严格遵循：仅输出最终重写结果文本，不包含任何前缀、提示词、说明性文字或换行符。重写的疑问句以："我想知道"开头，非疑问句你自行适配\n7. 当无需重写时，直接输出原问题。\n\n【重写示例】\n历史对话：\n用户：小红鞋什么时候有货？\n系统：请问您指的是哪款小红鞋？\n用户：就是上次缺货那款\n当前问题：就是上次缺货那款\n重写结果：我想知道上次缺货的那款小红鞋什么时候有货？\n\n历史对话：\n用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：那我们还有其他计划，能投放吗？\n当前问题：那我们还有其他计划，能投放吗？\n重写结果：我想知道当账号处于限流状态时，是否可以继续开启新的广告投放计划？\n\n历史对话：\n用户：你是谁？\n当前问题：你是谁？\n重写结果：你是谁？\n\n历史对话：\n用户：开心\n当前问题：开心\n重写结果：开心\n\n历史对话：\n用户：我要买它\n系统：请问您指的是哪款商品？\n用户：之前推荐的那双\n当前问题：之前推荐的那双\n重写结果：我想要买你之前推荐的那双鞋。\n\n注意：严格按照以上风格工作，禁止任何额外输出。\n\nUser: 用户：我看到账户有‘自动投放’推荐，能用吗？\n系统：自动投放适合素材充足的店铺，能节省操作精力。\n用户：我们现在素材就3条，会不会不够？\n系统：建议至少准备10条素材，保证系统有效匹配。\n用户：那我们只能等素材做出来了？\n系统：可先用模板创意补充素材，临时应急。\n用户：这个计划系统每天都会变吗？\n系统：自动投放系统会根据数据实时调整计划。\n用户：我们还能看到每条消耗吗？\n系统：后台有投放分析功能，可查看素材和人群表现。\n\n当前问题：我们还能看到每条消耗吗？\n请你根据任务规则输出重写结果。\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=0.5, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 21:25:41 [async_llm.py:252] Added request chatcmpl-35c9d38091cf4f0d9f9ec16b0156249b.
INFO:     127.0.0.1:45866 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-19 21:25:41 [logger.py:39] Received request chatcmpl-e43824a18c1149019c33c73cbdad630b: prompt: '<｜begin▁of▁sentence｜>你是一个专业的问题重写模块，专门用于多轮对话场景下的指代补全与语义还原任务。\n请严格按照以下规则执行：\n\n【任务目标】\n1. 你的目标是准确解析用户真实意图，从历史中发掘指代和意图，使得这个独立问题尽量完整，尽可能包含所有信息关键词,特别是要补充关键的动机，指代和场景（润色后的问题至少涉及：用户面对的前置情况，什么限制，什么疑问等）。\n2. 仅当历史信息能够提供明确上下文时才进行补全，否则保持当前问题不变。\n3. 对于明显不需要补全的问题如：吹水："呵呵"，命令："关机"，无关内容："人生的意义"等，返回原句\n4. 不进行任何无关发挥、扩写、润色、修辞性描述、解释、总结、感情色彩。\n5. 不编造任何不存在的假设背景或新信息。\n6. 输出格式必须严格遵循：仅输出最终重写结果文本，不包含任何前缀、提示词、说明性文字或换行符。重写的疑问句以："我想知道"开头，非疑问句你自行适配\n7. 当无需重写时，直接输出原问题。\n\n【重写示例】\n历史对话：\n用户：小红鞋什么时候有货？\n系统：请问您指的是哪款小红鞋？\n用户：就是上次缺货那款\n当前问题：就是上次缺货那款\n重写结果：我想知道上次缺货的那款小红鞋什么时候有货？\n\n历史对话：\n用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：那我们还有其他计划，能投放吗？\n当前问题：那我们还有其他计划，能投放吗？\n重写结果：我想知道当账号处于限流状态时，是否可以继续开启新的广告投放计划？\n\n历史对话：\n用户：你是谁？\n当前问题：你是谁？\n重写结果：你是谁？\n\n历史对话：\n用户：开心\n当前问题：开心\n重写结果：开心\n\n历史对话：\n用户：我要买它\n系统：请问您指的是哪款商品？\n用户：之前推荐的那双\n当前问题：之前推荐的那双\n重写结果：我想要买你之前推荐的那双鞋。\n\n注意：严格按照以上风格工作，禁止任何额外输出。\n\nUser: 用户：知识付费大促推职场课程，怎么定价？\n系统：建议采用“阶梯定价”：前50名99元，51-200名149元，之后恢复199元，营造紧迫感。\n用户：宣传“学完月薪涨50%”合规吗？\n系统：禁止承诺具体收益，可用“学员平均薪资提升32%”等统计数据，需附学员评价截图和数据来源说明。\n用户：课程试听环节需要注意什么？\n系统：试听内容需与正课一致，禁止用“免费领资料”诱导点击后跳转付费页面。\n用户：虚拟产品怎么处理退款纠纷？\n系统：需在购买页明确“虚拟产品一经售出概不退款”，但因课程质量问题需协商退款，避免平台介入。\n用户：平台对课程内容审核哪些方面？\n系统：重点审核是否涉及敏感话题、盗版内容，建议提前提交课程大纲和讲师资质（如职业资格证书）。\n用户：这么说我们得调整收益宣传和完善退款政策了？\n系统：是的，同时要确保课程内容和讲师资质通过平台预审。\n\n当前问题：这么说我们得调整收益宣传和完善退款政策了？\n请你根据任务规则输出重写结果。\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=0.5, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 21:25:41 [async_llm.py:252] Added request chatcmpl-e43824a18c1149019c33c73cbdad630b.
INFO:     127.0.0.1:45864 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-19 21:25:42 [logger.py:39] Received request chatcmpl-825f6af451464afd85686d78e8a130a8: prompt: '<｜begin▁of▁sentence｜>你是一个专业的问题重写模块，专门用于多轮对话场景下的指代补全与语义还原任务。\n请严格按照以下规则执行：\n\n【任务目标】\n1. 你的目标是准确解析用户真实意图，从历史中发掘指代和意图，使得这个独立问题尽量完整，尽可能包含所有信息关键词,特别是要补充关键的动机，指代和场景（润色后的问题至少涉及：用户面对的前置情况，什么限制，什么疑问等）。\n2. 仅当历史信息能够提供明确上下文时才进行补全，否则保持当前问题不变。\n3. 对于明显不需要补全的问题如：吹水："呵呵"，命令："关机"，无关内容："人生的意义"等，返回原句\n4. 不进行任何无关发挥、扩写、润色、修辞性描述、解释、总结、感情色彩。\n5. 不编造任何不存在的假设背景或新信息。\n6. 输出格式必须严格遵循：仅输出最终重写结果文本，不包含任何前缀、提示词、说明性文字或换行符。重写的疑问句以："我想知道"开头，非疑问句你自行适配\n7. 当无需重写时，直接输出原问题。\n\n【重写示例】\n历史对话：\n用户：小红鞋什么时候有货？\n系统：请问您指的是哪款小红鞋？\n用户：就是上次缺货那款\n当前问题：就是上次缺货那款\n重写结果：我想知道上次缺货的那款小红鞋什么时候有货？\n\n历史对话：\n用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：那我们还有其他计划，能投放吗？\n当前问题：那我们还有其他计划，能投放吗？\n重写结果：我想知道当账号处于限流状态时，是否可以继续开启新的广告投放计划？\n\n历史对话：\n用户：你是谁？\n当前问题：你是谁？\n重写结果：你是谁？\n\n历史对话：\n用户：开心\n当前问题：开心\n重写结果：开心\n\n历史对话：\n用户：我要买它\n系统：请问您指的是哪款商品？\n用户：之前推荐的那双\n当前问题：之前推荐的那双\n重写结果：我想要买你之前推荐的那双鞋。\n\n注意：严格按照以上风格工作，禁止任何额外输出。\n\nUser: 用户：我看有同行用全自动投放，效果还不错。\n系统：平台确实支持智能投放，适合素材储备丰富的商家。\n用户：我们素材也不少，但不清楚该怎么配置。\n系统：你可以通过素材库上传多个视频并打标签，系统会自动匹配。\n用户：那预算也自动调节吗？\n系统：预算支持设置上下限，平台会在范围内动态分配。\n用户：我们担心会烧得太快。\n系统：可以设置预算上限和消耗节奏，比如日消耗平滑分配。\n用户：那这种智能计划我们要每天看数据吗？\n系统：初期建议每天复盘，后期稳定后可降低频率。\n用户：我们这种属于中型店，适合智能模式吗？\n\n当前问题：我们这种属于中型店，适合智能模式吗？\n请你根据任务规则输出重写结果。\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=0.5, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 21:25:42 [async_llm.py:252] Added request chatcmpl-825f6af451464afd85686d78e8a130a8.
INFO:     127.0.0.1:45880 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-19 21:25:42 [logger.py:39] Received request chatcmpl-676a5b84c7c44021b2de5e9f21aa7196: prompt: '<｜begin▁of▁sentence｜>你是一个专业的问题重写模块，专门用于多轮对话场景下的指代补全与语义还原任务。\n请严格按照以下规则执行：\n\n【任务目标】\n1. 你的目标是准确解析用户真实意图，从历史中发掘指代和意图，使得这个独立问题尽量完整，尽可能包含所有信息关键词,特别是要补充关键的动机，指代和场景（润色后的问题至少涉及：用户面对的前置情况，什么限制，什么疑问等）。\n2. 仅当历史信息能够提供明确上下文时才进行补全，否则保持当前问题不变。\n3. 对于明显不需要补全的问题如：吹水："呵呵"，命令："关机"，无关内容："人生的意义"等，返回原句\n4. 不进行任何无关发挥、扩写、润色、修辞性描述、解释、总结、感情色彩。\n5. 不编造任何不存在的假设背景或新信息。\n6. 输出格式必须严格遵循：仅输出最终重写结果文本，不包含任何前缀、提示词、说明性文字或换行符。重写的疑问句以："我想知道"开头，非疑问句你自行适配\n7. 当无需重写时，直接输出原问题。\n\n【重写示例】\n历史对话：\n用户：小红鞋什么时候有货？\n系统：请问您指的是哪款小红鞋？\n用户：就是上次缺货那款\n当前问题：就是上次缺货那款\n重写结果：我想知道上次缺货的那款小红鞋什么时候有货？\n\n历史对话：\n用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：那我们还有其他计划，能投放吗？\n当前问题：那我们还有其他计划，能投放吗？\n重写结果：我想知道当账号处于限流状态时，是否可以继续开启新的广告投放计划？\n\n历史对话：\n用户：你是谁？\n当前问题：你是谁？\n重写结果：你是谁？\n\n历史对话：\n用户：开心\n当前问题：开心\n重写结果：开心\n\n历史对话：\n用户：我要买它\n系统：请问您指的是哪款商品？\n用户：之前推荐的那双\n当前问题：之前推荐的那双\n重写结果：我想要买你之前推荐的那双鞋。\n\n注意：严格按照以上风格工作，禁止任何额外输出。\n\nUser: 用户：餐厅参加抖音团购大促，怎么设计套餐？\n系统：建议设计“爆款单品+小吃饮料”的组合套餐，如“招牌牛排套餐（含沙拉+可乐）”，价格比单点低30%。\n用户：宣传“食材当天现杀”需要证明吗？\n系统：需提供供应商的屠宰检疫证明和每日采购记录，可在门店公示或拍摄食材处理过程视频。\n用户：达人探店能拍摄后厨吗？\n系统：可以，但需确保后厨卫生符合《餐饮服务食品安全操作规范》，禁止拍摄加工中的违规行为。\n用户：团购券有效期怎么设置？\n系统：建议设置30-90天有效期，支持过期自动退，避免用户投诉“霸王条款”。\n用户：大促期间备货不足怎么办？\n系统：需在团购页标注“爆款限量XX份”，售罄后及时下架，避免到店无货引发差评。\n用户：平台对餐饮卫生评级有要求吗？\n系统：需在店铺页展示《食品经营许可证》和“餐饮服务食品安全等级”（如“笑脸”“平脸”标识）。\n用户：这么说我们得准备食材证明和后厨合规记录了？\n系统：是的，建议每周进行后厨卫生自查，并留存影像记录。\n\n当前问题：这么说我们得准备食材证明和后厨合规记录了？\n请你根据任务规则输出重写结果。\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=0.5, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 21:25:42 [async_llm.py:252] Added request chatcmpl-676a5b84c7c44021b2de5e9f21aa7196.
INFO:     127.0.0.1:45860 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-19 21:25:42 [logger.py:39] Received request chatcmpl-6cf110f502dc42b9be04a6b3dca614f5: prompt: '<｜begin▁of▁sentence｜>你是一个专业的问题重写模块，专门用于多轮对话场景下的指代补全与语义还原任务。\n请严格按照以下规则执行：\n\n【任务目标】\n1. 你的目标是准确解析用户真实意图，从历史中发掘指代和意图，使得这个独立问题尽量完整，尽可能包含所有信息关键词,特别是要补充关键的动机，指代和场景（润色后的问题至少涉及：用户面对的前置情况，什么限制，什么疑问等）。\n2. 仅当历史信息能够提供明确上下文时才进行补全，否则保持当前问题不变。\n3. 对于明显不需要补全的问题如：吹水："呵呵"，命令："关机"，无关内容："人生的意义"等，返回原句\n4. 不进行任何无关发挥、扩写、润色、修辞性描述、解释、总结、感情色彩。\n5. 不编造任何不存在的假设背景或新信息。\n6. 输出格式必须严格遵循：仅输出最终重写结果文本，不包含任何前缀、提示词、说明性文字或换行符。重写的疑问句以："我想知道"开头，非疑问句你自行适配\n7. 当无需重写时，直接输出原问题。\n\n【重写示例】\n历史对话：\n用户：小红鞋什么时候有货？\n系统：请问您指的是哪款小红鞋？\n用户：就是上次缺货那款\n当前问题：就是上次缺货那款\n重写结果：我想知道上次缺货的那款小红鞋什么时候有货？\n\n历史对话：\n用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：那我们还有其他计划，能投放吗？\n当前问题：那我们还有其他计划，能投放吗？\n重写结果：我想知道当账号处于限流状态时，是否可以继续开启新的广告投放计划？\n\n历史对话：\n用户：你是谁？\n当前问题：你是谁？\n重写结果：你是谁？\n\n历史对话：\n用户：开心\n当前问题：开心\n重写结果：开心\n\n历史对话：\n用户：我要买它\n系统：请问您指的是哪款商品？\n用户：之前推荐的那双\n当前问题：之前推荐的那双\n重写结果：我想要买你之前推荐的那双鞋。\n\n注意：严格按照以上风格工作，禁止任何额外输出。\n\nUser: 用户：我们最近直播间流量不稳定。\n系统：是否有进行投放计划或者参加活动？\n用户：我们只做了自然流量。\n系统：建议搭配投放引流包，提升进房率。\n用户：我们预算不高，划算吗？\n系统：可以从低预算起测，效果好再放量。\n用户：系统推荐的是直播上分计划，能用吗？\n系统：适合提升单场数据表现，建议与自然流量配合使用。\n用户：那开计划会影响直播权重吗？\n系统：不会，优质数据反而有助于权重提升。\n\n当前问题：那开计划会影响直播权重吗？\n请你根据任务规则输出重写结果。\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=0.5, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 21:25:42 [async_llm.py:252] Added request chatcmpl-6cf110f502dc42b9be04a6b3dca614f5.
INFO:     127.0.0.1:45864 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-19 21:25:42 [logger.py:39] Received request chatcmpl-f0e9f6637bb242b2a29c85ba1b7858f4: prompt: '<｜begin▁of▁sentence｜>你是一个专业的问题重写模块，专门用于多轮对话场景下的指代补全与语义还原任务。\n请严格按照以下规则执行：\n\n【任务目标】\n1. 你的目标是准确解析用户真实意图，从历史中发掘指代和意图，使得这个独立问题尽量完整，尽可能包含所有信息关键词,特别是要补充关键的动机，指代和场景（润色后的问题至少涉及：用户面对的前置情况，什么限制，什么疑问等）。\n2. 仅当历史信息能够提供明确上下文时才进行补全，否则保持当前问题不变。\n3. 对于明显不需要补全的问题如：吹水："呵呵"，命令："关机"，无关内容："人生的意义"等，返回原句\n4. 不进行任何无关发挥、扩写、润色、修辞性描述、解释、总结、感情色彩。\n5. 不编造任何不存在的假设背景或新信息。\n6. 输出格式必须严格遵循：仅输出最终重写结果文本，不包含任何前缀、提示词、说明性文字或换行符。重写的疑问句以："我想知道"开头，非疑问句你自行适配\n7. 当无需重写时，直接输出原问题。\n\n【重写示例】\n历史对话：\n用户：小红鞋什么时候有货？\n系统：请问您指的是哪款小红鞋？\n用户：就是上次缺货那款\n当前问题：就是上次缺货那款\n重写结果：我想知道上次缺货的那款小红鞋什么时候有货？\n\n历史对话：\n用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：那我们还有其他计划，能投放吗？\n当前问题：那我们还有其他计划，能投放吗？\n重写结果：我想知道当账号处于限流状态时，是否可以继续开启新的广告投放计划？\n\n历史对话：\n用户：你是谁？\n当前问题：你是谁？\n重写结果：你是谁？\n\n历史对话：\n用户：开心\n当前问题：开心\n重写结果：开心\n\n历史对话：\n用户：我要买它\n系统：请问您指的是哪款商品？\n用户：之前推荐的那双\n当前问题：之前推荐的那双\n重写结果：我想要买你之前推荐的那双鞋。\n\n注意：严格按照以上风格工作，禁止任何额外输出。\n\nUser: 用户：工业品参加抖音大促，怎么吸引B端客户？\n系统：建议推出“企业采购专场”，提供批量折扣（如“满10件打8折”）和免费样品试用服务。\n用户：素材里能展示工厂生产流程吗？\n系统：可以展示非涉密生产环节，如设备组装、质量检测，但需打码处理核心技术细节。\n用户：工业设备的安装视频能直播吗？\n系统：直播需确保操作人员佩戴安全装备，禁止展示违规操作，建议提前报备平台并标注“专业操作请勿模仿”。\n用户：大促期间签订的合同，交货期怎么约定？\n系统：需在商品页明确“大促订单交货期延长至15个工作日”，并在合同中注明不可抗力条款。\n用户：平台对工业品资质有啥要求？\n系统：需上传营业执照、ISO质量认证、特种设备生产许可证（如有），危险化学品需额外提供《危险化学品经营许可证》。\n用户：这么说我们得审核直播内容和完善合同条款了？\n系统：是的，工业品客户更关注合规性和服务能力，建议组建专项客服对接订单。\n\n当前问题：这么说我们得审核直播内容和完善合同条款了？\n请你根据任务规则输出重写结果。\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=0.5, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 21:25:42 [async_llm.py:252] Added request chatcmpl-f0e9f6637bb242b2a29c85ba1b7858f4.
INFO:     127.0.0.1:45860 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-19 21:25:42 [logger.py:39] Received request chatcmpl-69c38d49f7cc40cb97e1582281280871: prompt: '<｜begin▁of▁sentence｜>你是一个专业的问题重写模块，专门用于多轮对话场景下的指代补全与语义还原任务。\n请严格按照以下规则执行：\n\n【任务目标】\n1. 你的目标是准确解析用户真实意图，从历史中发掘指代和意图，使得这个独立问题尽量完整，尽可能包含所有信息关键词,特别是要补充关键的动机，指代和场景（润色后的问题至少涉及：用户面对的前置情况，什么限制，什么疑问等）。\n2. 仅当历史信息能够提供明确上下文时才进行补全，否则保持当前问题不变。\n3. 对于明显不需要补全的问题如：吹水："呵呵"，命令："关机"，无关内容："人生的意义"等，返回原句\n4. 不进行任何无关发挥、扩写、润色、修辞性描述、解释、总结、感情色彩。\n5. 不编造任何不存在的假设背景或新信息。\n6. 输出格式必须严格遵循：仅输出最终重写结果文本，不包含任何前缀、提示词、说明性文字或换行符。重写的疑问句以："我想知道"开头，非疑问句你自行适配\n7. 当无需重写时，直接输出原问题。\n\n【重写示例】\n历史对话：\n用户：小红鞋什么时候有货？\n系统：请问您指的是哪款小红鞋？\n用户：就是上次缺货那款\n当前问题：就是上次缺货那款\n重写结果：我想知道上次缺货的那款小红鞋什么时候有货？\n\n历史对话：\n用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：那我们还有其他计划，能投放吗？\n当前问题：那我们还有其他计划，能投放吗？\n重写结果：我想知道当账号处于限流状态时，是否可以继续开启新的广告投放计划？\n\n历史对话：\n用户：你是谁？\n当前问题：你是谁？\n重写结果：你是谁？\n\n历史对话：\n用户：开心\n当前问题：开心\n重写结果：开心\n\n历史对话：\n用户：我要买它\n系统：请问您指的是哪款商品？\n用户：之前推荐的那双\n当前问题：之前推荐的那双\n重写结果：我想要买你之前推荐的那双鞋。\n\n注意：严格按照以上风格工作，禁止任何额外输出。\n\nUser: 用户：我们在节日前做了组合投放，现在复盘中。\n系统：组合投放涉及多个渠道，建议分渠道查看数据。\n用户：我们用的是直播+短视频组合。\n系统：可分析直播引流和短视频转化效果，判断协同效率。\n用户：我们发现点击率高但转化低。\n系统：可能是落地页内容与广告素材不一致。\n用户：落地页我们没动过。\n系统：建议优化文案结构和加载速度，提升停留率。\n用户：有没有工具可以测这些？\n系统：可使用平台的转化分析工具或第三方数据平台。\n\n当前问题：有没有工具可以测这些？\n请你根据任务规则输出重写结果。\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=0.5, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 21:25:42 [async_llm.py:252] Added request chatcmpl-69c38d49f7cc40cb97e1582281280871.
INFO:     127.0.0.1:45866 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-19 21:25:42 [logger.py:39] Received request chatcmpl-3ded0cd0617e4bb1849c57b6b33950e6: prompt: '<｜begin▁of▁sentence｜>你是一个专业的问题重写模块，专门用于多轮对话场景下的指代补全与语义还原任务。\n请严格按照以下规则执行：\n\n【任务目标】\n1. 你的目标是准确解析用户真实意图，从历史中发掘指代和意图，使得这个独立问题尽量完整，尽可能包含所有信息关键词,特别是要补充关键的动机，指代和场景（润色后的问题至少涉及：用户面对的前置情况，什么限制，什么疑问等）。\n2. 仅当历史信息能够提供明确上下文时才进行补全，否则保持当前问题不变。\n3. 对于明显不需要补全的问题如：吹水："呵呵"，命令："关机"，无关内容："人生的意义"等，返回原句\n4. 不进行任何无关发挥、扩写、润色、修辞性描述、解释、总结、感情色彩。\n5. 不编造任何不存在的假设背景或新信息。\n6. 输出格式必须严格遵循：仅输出最终重写结果文本，不包含任何前缀、提示词、说明性文字或换行符。重写的疑问句以："我想知道"开头，非疑问句你自行适配\n7. 当无需重写时，直接输出原问题。\n\n【重写示例】\n历史对话：\n用户：小红鞋什么时候有货？\n系统：请问您指的是哪款小红鞋？\n用户：就是上次缺货那款\n当前问题：就是上次缺货那款\n重写结果：我想知道上次缺货的那款小红鞋什么时候有货？\n\n历史对话：\n用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：那我们还有其他计划，能投放吗？\n当前问题：那我们还有其他计划，能投放吗？\n重写结果：我想知道当账号处于限流状态时，是否可以继续开启新的广告投放计划？\n\n历史对话：\n用户：你是谁？\n当前问题：你是谁？\n重写结果：你是谁？\n\n历史对话：\n用户：开心\n当前问题：开心\n重写结果：开心\n\n历史对话：\n用户：我要买它\n系统：请问您指的是哪款商品？\n用户：之前推荐的那双\n当前问题：之前推荐的那双\n重写结果：我想要买你之前推荐的那双鞋。\n\n注意：严格按照以上风格工作，禁止任何额外输出。\n\nUser: 用户：农产品大促推土鸡蛋，怎么证明是散养的？\n系统：可拍摄鸡舍散养视频，附《动物防疫条件合格证》和“散养禽蛋”认证标签，标注“每枚鸡蛋可追溯”。\n用户：素材里能宣传“孕妇小孩都能吃”吗？\n系统：需谨慎使用人群定向表述，可用“全家营养”替代，避免引发特殊人群食用风险争议。\n用户：大促期间快递损坏怎么理赔？\n系统：建议采用防震泡沫蛋托包装，发货前称重拍照留存，用户签收时若破损可凭照片全额理赔。\n用户：农产品需要QS认证吗？\n系统：初级农产品（如新鲜果蔬）无需QS，但预包装产品（如腌制咸菜）需取得SC食品生产许可。\n用户：平台对“农家自制”标签有要求吗？\n系统：需真实反映生产主体，若为个体农户需上传身份证和产地证明，企业生产则需标注公司名称。\n用户：这么说我们得准备散养证明和包装理赔方案了？\n系统：是的，农产品品控和售后是大促关键，建议建立从养殖到发货的全流程溯源体系。\n\n当前问题：这么说我们得准备散养证明和包装理赔方案了？\n请你根据任务规则输出重写结果。\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=0.5, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 21:25:42 [async_llm.py:252] Added request chatcmpl-3ded0cd0617e4bb1849c57b6b33950e6.
INFO:     127.0.0.1:45880 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:45864 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-19 21:25:42 [logger.py:39] Received request chatcmpl-c8ea9f5c547446bfa255438ed8e152a1: prompt: '<｜begin▁of▁sentence｜>你是一个专业的问题重写模块，专门用于多轮对话场景下的指代补全与语义还原任务。\n请严格按照以下规则执行：\n\n【任务目标】\n1. 你的目标是准确解析用户真实意图，从历史中发掘指代和意图，使得这个独立问题尽量完整，尽可能包含所有信息关键词,特别是要补充关键的动机，指代和场景（润色后的问题至少涉及：用户面对的前置情况，什么限制，什么疑问等）。\n2. 仅当历史信息能够提供明确上下文时才进行补全，否则保持当前问题不变。\n3. 对于明显不需要补全的问题如：吹水："呵呵"，命令："关机"，无关内容："人生的意义"等，返回原句\n4. 不进行任何无关发挥、扩写、润色、修辞性描述、解释、总结、感情色彩。\n5. 不编造任何不存在的假设背景或新信息。\n6. 输出格式必须严格遵循：仅输出最终重写结果文本，不包含任何前缀、提示词、说明性文字或换行符。重写的疑问句以："我想知道"开头，非疑问句你自行适配\n7. 当无需重写时，直接输出原问题。\n\n【重写示例】\n历史对话：\n用户：小红鞋什么时候有货？\n系统：请问您指的是哪款小红鞋？\n用户：就是上次缺货那款\n当前问题：就是上次缺货那款\n重写结果：我想知道上次缺货的那款小红鞋什么时候有货？\n\n历史对话：\n用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：那我们还有其他计划，能投放吗？\n当前问题：那我们还有其他计划，能投放吗？\n重写结果：我想知道当账号处于限流状态时，是否可以继续开启新的广告投放计划？\n\n历史对话：\n用户：你是谁？\n当前问题：你是谁？\n重写结果：你是谁？\n\n历史对话：\n用户：开心\n当前问题：开心\n重写结果：开心\n\n历史对话：\n用户：我要买它\n系统：请问您指的是哪款商品？\n用户：之前推荐的那双\n当前问题：之前推荐的那双\n重写结果：我想要买你之前推荐的那双鞋。\n\n注意：严格按照以上风格工作，禁止任何额外输出。\n\nUser: 用户：我们是首次做千川直播投放，流程不清楚。\n系统：你可以从直播计划设置开始，选择目标和人群。\n用户：目标选哪种合适？\n系统：首投建议选‘进房量’或‘直播间互动’作为目标。\n用户：人群怎么配置？\n系统：新账号建议从系统推荐人群起步，再逐步优化。\n用户：是不是每天都要盯数据？\n系统：建议前三天重点关注，后期根据数据调整频次。\n用户：我们如果暂停计划再开，会影响吗？\n系统：暂停过久可能影响系统学习，建议保持稳定。\n\n当前问题：我们如果暂停计划再开，会影响吗？\n请你根据任务规则输出重写结果。\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=0.5, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 21:25:42 [async_llm.py:252] Added request chatcmpl-c8ea9f5c547446bfa255438ed8e152a1.
INFO 06-19 21:25:42 [logger.py:39] Received request chatcmpl-a655e5e4508d41b0a55c4402280396ac: prompt: '<｜begin▁of▁sentence｜>你是一个专业的问题重写模块，专门用于多轮对话场景下的指代补全与语义还原任务。\n请严格按照以下规则执行：\n\n【任务目标】\n1. 你的目标是准确解析用户真实意图，从历史中发掘指代和意图，使得这个独立问题尽量完整，尽可能包含所有信息关键词,特别是要补充关键的动机，指代和场景（润色后的问题至少涉及：用户面对的前置情况，什么限制，什么疑问等）。\n2. 仅当历史信息能够提供明确上下文时才进行补全，否则保持当前问题不变。\n3. 对于明显不需要补全的问题如：吹水："呵呵"，命令："关机"，无关内容："人生的意义"等，返回原句\n4. 不进行任何无关发挥、扩写、润色、修辞性描述、解释、总结、感情色彩。\n5. 不编造任何不存在的假设背景或新信息。\n6. 输出格式必须严格遵循：仅输出最终重写结果文本，不包含任何前缀、提示词、说明性文字或换行符。重写的疑问句以："我想知道"开头，非疑问句你自行适配\n7. 当无需重写时，直接输出原问题。\n\n【重写示例】\n历史对话：\n用户：小红鞋什么时候有货？\n系统：请问您指的是哪款小红鞋？\n用户：就是上次缺货那款\n当前问题：就是上次缺货那款\n重写结果：我想知道上次缺货的那款小红鞋什么时候有货？\n\n历史对话：\n用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：那我们还有其他计划，能投放吗？\n当前问题：那我们还有其他计划，能投放吗？\n重写结果：我想知道当账号处于限流状态时，是否可以继续开启新的广告投放计划？\n\n历史对话：\n用户：你是谁？\n当前问题：你是谁？\n重写结果：你是谁？\n\n历史对话：\n用户：开心\n当前问题：开心\n重写结果：开心\n\n历史对话：\n用户：我要买它\n系统：请问您指的是哪款商品？\n用户：之前推荐的那双\n当前问题：之前推荐的那双\n重写结果：我想要买你之前推荐的那双鞋。\n\n注意：严格按照以上风格工作，禁止任何额外输出。\n\nUser: 用户：我们现在正在跑一场全域推广。\n系统：全域适合大场景推广，建议搭配品牌广告。\n用户：品牌广告需要什么资质？\n系统：需提供品牌授权或商标证书，提交后台审核。\n用户：这个流程多久？\n系统：一般1-3个工作日，具体看资料完整度。\n用户：素材也需要单独审核吗？\n系统：品牌广告素材审核更严格，建议提前准备。\n用户：我们活动下周就开始了，来得及吗？\n系统：尽快提交资料并使用推荐模板，可提升审核效率。\n\n当前问题：我们活动下周就开始了，来得及吗？\n请你根据任务规则输出重写结果。\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=0.5, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 21:25:42 [async_llm.py:252] Added request chatcmpl-a655e5e4508d41b0a55c4402280396ac.
INFO:     127.0.0.1:45860 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-19 21:25:42 [logger.py:39] Received request chatcmpl-e1632bd6182b458e88b631b0719c0aa2: prompt: '<｜begin▁of▁sentence｜>你是一个专业的问题重写模块，专门用于多轮对话场景下的指代补全与语义还原任务。\n请严格按照以下规则执行：\n\n【任务目标】\n1. 你的目标是准确解析用户真实意图，从历史中发掘指代和意图，使得这个独立问题尽量完整，尽可能包含所有信息关键词,特别是要补充关键的动机，指代和场景（润色后的问题至少涉及：用户面对的前置情况，什么限制，什么疑问等）。\n2. 仅当历史信息能够提供明确上下文时才进行补全，否则保持当前问题不变。\n3. 对于明显不需要补全的问题如：吹水："呵呵"，命令："关机"，无关内容："人生的意义"等，返回原句\n4. 不进行任何无关发挥、扩写、润色、修辞性描述、解释、总结、感情色彩。\n5. 不编造任何不存在的假设背景或新信息。\n6. 输出格式必须严格遵循：仅输出最终重写结果文本，不包含任何前缀、提示词、说明性文字或换行符。重写的疑问句以："我想知道"开头，非疑问句你自行适配\n7. 当无需重写时，直接输出原问题。\n\n【重写示例】\n历史对话：\n用户：小红鞋什么时候有货？\n系统：请问您指的是哪款小红鞋？\n用户：就是上次缺货那款\n当前问题：就是上次缺货那款\n重写结果：我想知道上次缺货的那款小红鞋什么时候有货？\n\n历史对话：\n用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：那我们还有其他计划，能投放吗？\n当前问题：那我们还有其他计划，能投放吗？\n重写结果：我想知道当账号处于限流状态时，是否可以继续开启新的广告投放计划？\n\n历史对话：\n用户：你是谁？\n当前问题：你是谁？\n重写结果：你是谁？\n\n历史对话：\n用户：开心\n当前问题：开心\n重写结果：开心\n\n历史对话：\n用户：我要买它\n系统：请问您指的是哪款商品？\n用户：之前推荐的那双\n当前问题：之前推荐的那双\n重写结果：我想要买你之前推荐的那双鞋。\n\n注意：严格按照以上风格工作，禁止任何额外输出。\n\nUser: 用户：教育培训大促推编程课程，怎么设计试听课？\n系统：建议设置“免费试听+付费课程”模式，试听课内容需与正课一致，时长控制在30分钟内。\n用户：素材里能宣传“学完可进大厂”吗？\n系统：禁止承诺具体就业结果，可用“90%学员进入知名企业实习”替代，但需附学员就业数据来源。\n用户：达人直播能现场编程吗？\n系统：可以，但需确保操作规范，禁止展示违规代码或破解行为，建议提前准备演示脚本。\n用户：课程价格怎么定？\n系统：建议采用“阶梯定价”，前100名99元，101-300名199元，之后恢复299元，营造紧迫感。\n用户：平台对教育资质有啥要求？\n系统：需提供《办学许可证》或《培训机构备案证明》，讲师需具备相关职业资格证书。\n用户：这么说我们得审核课程内容和讲师资质了？\n系统：是的，同时要确保试听课内容真实有效，避免虚假宣传引发投诉。\n\n当前问题：这么说我们得审核课程内容和讲师资质了？\n请你根据任务规则输出重写结果。\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=0.5, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 21:25:42 [async_llm.py:252] Added request chatcmpl-e1632bd6182b458e88b631b0719c0aa2.
INFO:     127.0.0.1:45880 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-19 21:25:42 [logger.py:39] Received request chatcmpl-af81e1ce4021407c934a32a27c2eea3f: prompt: '<｜begin▁of▁sentence｜>你是一个专业的问题重写模块，专门用于多轮对话场景下的指代补全与语义还原任务。\n请严格按照以下规则执行：\n\n【任务目标】\n1. 你的目标是准确解析用户真实意图，从历史中发掘指代和意图，使得这个独立问题尽量完整，尽可能包含所有信息关键词,特别是要补充关键的动机，指代和场景（润色后的问题至少涉及：用户面对的前置情况，什么限制，什么疑问等）。\n2. 仅当历史信息能够提供明确上下文时才进行补全，否则保持当前问题不变。\n3. 对于明显不需要补全的问题如：吹水："呵呵"，命令："关机"，无关内容："人生的意义"等，返回原句\n4. 不进行任何无关发挥、扩写、润色、修辞性描述、解释、总结、感情色彩。\n5. 不编造任何不存在的假设背景或新信息。\n6. 输出格式必须严格遵循：仅输出最终重写结果文本，不包含任何前缀、提示词、说明性文字或换行符。重写的疑问句以："我想知道"开头，非疑问句你自行适配\n7. 当无需重写时，直接输出原问题。\n\n【重写示例】\n历史对话：\n用户：小红鞋什么时候有货？\n系统：请问您指的是哪款小红鞋？\n用户：就是上次缺货那款\n当前问题：就是上次缺货那款\n重写结果：我想知道上次缺货的那款小红鞋什么时候有货？\n\n历史对话：\n用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：那我们还有其他计划，能投放吗？\n当前问题：那我们还有其他计划，能投放吗？\n重写结果：我想知道当账号处于限流状态时，是否可以继续开启新的广告投放计划？\n\n历史对话：\n用户：你是谁？\n当前问题：你是谁？\n重写结果：你是谁？\n\n历史对话：\n用户：开心\n当前问题：开心\n重写结果：开心\n\n历史对话：\n用户：我要买它\n系统：请问您指的是哪款商品？\n用户：之前推荐的那双\n当前问题：之前推荐的那双\n重写结果：我想要买你之前推荐的那双鞋。\n\n注意：严格按照以上风格工作，禁止任何额外输出。\n\nUser: 用户：汽车用品大促推轮胎，怎么保证质量？\n系统：需提供《轮胎生产许可证》和3C认证，标注“正品保障，假一赔十”。\n用户：素材里能宣传“耐磨性提升30%”吗？\n系统：可以，但需附上第三方检测报告，禁止使用“超越同类产品”等夸大表述。\n用户：达人直播能现场测试轮胎抓地力吗？\n系统：可以，但需确保安全操作，禁止展示危险驾驶行为，建议提前准备测试脚本。\n用户：大促期间物流破损率高，怎么包装更安全？\n系统：建议使用双层瓦楞纸箱+防震泡沫填充物，外箱标注“易碎勿压”，购买物流责任险。\n用户：平台对汽车用品资质有啥要求？\n系统：需上传营业执照、产品合格证、3C认证等资质文件，确保产品符合国家标准。\n用户：这么说我们得准备质量证明和包装方案了？\n系统：是的，同时要确保所有资质文件真实有效，以备平台抽查。\n\n当前问题：这么说我们得准备质量证明和包装方案了？\n请你根据任务规则输出重写结果。\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=0.5, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 21:25:42 [async_llm.py:252] Added request chatcmpl-af81e1ce4021407c934a32a27c2eea3f.
INFO:     127.0.0.1:45866 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:45864 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-19 21:25:42 [logger.py:39] Received request chatcmpl-a805f74862e2422ca512b779155ba264: prompt: '<｜begin▁of▁sentence｜>你是一个专业的问题重写模块，专门用于多轮对话场景下的指代补全与语义还原任务。\n请严格按照以下规则执行：\n\n【任务目标】\n1. 你的目标是准确解析用户真实意图，从历史中发掘指代和意图，使得这个独立问题尽量完整，尽可能包含所有信息关键词,特别是要补充关键的动机，指代和场景（润色后的问题至少涉及：用户面对的前置情况，什么限制，什么疑问等）。\n2. 仅当历史信息能够提供明确上下文时才进行补全，否则保持当前问题不变。\n3. 对于明显不需要补全的问题如：吹水："呵呵"，命令："关机"，无关内容："人生的意义"等，返回原句\n4. 不进行任何无关发挥、扩写、润色、修辞性描述、解释、总结、感情色彩。\n5. 不编造任何不存在的假设背景或新信息。\n6. 输出格式必须严格遵循：仅输出最终重写结果文本，不包含任何前缀、提示词、说明性文字或换行符。重写的疑问句以："我想知道"开头，非疑问句你自行适配\n7. 当无需重写时，直接输出原问题。\n\n【重写示例】\n历史对话：\n用户：小红鞋什么时候有货？\n系统：请问您指的是哪款小红鞋？\n用户：就是上次缺货那款\n当前问题：就是上次缺货那款\n重写结果：我想知道上次缺货的那款小红鞋什么时候有货？\n\n历史对话：\n用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：那我们还有其他计划，能投放吗？\n当前问题：那我们还有其他计划，能投放吗？\n重写结果：我想知道当账号处于限流状态时，是否可以继续开启新的广告投放计划？\n\n历史对话：\n用户：你是谁？\n当前问题：你是谁？\n重写结果：你是谁？\n\n历史对话：\n用户：开心\n当前问题：开心\n重写结果：开心\n\n历史对话：\n用户：我要买它\n系统：请问您指的是哪款商品？\n用户：之前推荐的那双\n当前问题：之前推荐的那双\n重写结果：我想要买你之前推荐的那双鞋。\n\n注意：严格按照以上风格工作，禁止任何额外输出。\n\nUser: 用户：我们计划在618做大促，准备中。\n系统：建议制定明确的预算计划和素材储备。\n用户：预算我们分了日预算，但怕烧不完。\n系统：平台会自动根据转化情况调整节奏，建议设最低预算线。\n用户：素材我们准备了20条够吗？\n系统：基本够用，建议至少覆盖4个核心人群。\n用户：人群我们还没细分。\n系统：可以基于历史订单划分人群，如高转化老客、新客等。\n用户：投放期间可以实时调整吗？\n系统：可以，后台支持计划暂停、预算调整和素材替换。\n\n当前问题：投放期间可以实时调整吗？\n请你根据任务规则输出重写结果。\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=0.5, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 21:25:42 [async_llm.py:252] Added request chatcmpl-a805f74862e2422ca512b779155ba264.
INFO 06-19 21:25:42 [logger.py:39] Received request chatcmpl-8fba64aad8bd4b2d97913b815e822117: prompt: '<｜begin▁of▁sentence｜>你是一个专业的问题重写模块，专门用于多轮对话场景下的指代补全与语义还原任务。\n请严格按照以下规则执行：\n\n【任务目标】\n1. 你的目标是准确解析用户真实意图，从历史中发掘指代和意图，使得这个独立问题尽量完整，尽可能包含所有信息关键词,特别是要补充关键的动机，指代和场景（润色后的问题至少涉及：用户面对的前置情况，什么限制，什么疑问等）。\n2. 仅当历史信息能够提供明确上下文时才进行补全，否则保持当前问题不变。\n3. 对于明显不需要补全的问题如：吹水："呵呵"，命令："关机"，无关内容："人生的意义"等，返回原句\n4. 不进行任何无关发挥、扩写、润色、修辞性描述、解释、总结、感情色彩。\n5. 不编造任何不存在的假设背景或新信息。\n6. 输出格式必须严格遵循：仅输出最终重写结果文本，不包含任何前缀、提示词、说明性文字或换行符。重写的疑问句以："我想知道"开头，非疑问句你自行适配\n7. 当无需重写时，直接输出原问题。\n\n【重写示例】\n历史对话：\n用户：小红鞋什么时候有货？\n系统：请问您指的是哪款小红鞋？\n用户：就是上次缺货那款\n当前问题：就是上次缺货那款\n重写结果：我想知道上次缺货的那款小红鞋什么时候有货？\n\n历史对话：\n用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：那我们还有其他计划，能投放吗？\n当前问题：那我们还有其他计划，能投放吗？\n重写结果：我想知道当账号处于限流状态时，是否可以继续开启新的广告投放计划？\n\n历史对话：\n用户：你是谁？\n当前问题：你是谁？\n重写结果：你是谁？\n\n历史对话：\n用户：开心\n当前问题：开心\n重写结果：开心\n\n历史对话：\n用户：我要买它\n系统：请问您指的是哪款商品？\n用户：之前推荐的那双\n当前问题：之前推荐的那双\n重写结果：我想要买你之前推荐的那双鞋。\n\n注意：严格按照以上风格工作，禁止任何额外输出。\n\nUser: 用户：宠物用品大促推狗粮，怎么保证品质？\n系统：需提供《宠物食品生产许可证》和产品检验报告，标注“无添加防腐剂，健康配方”。\n用户：素材里能宣传“狗狗吃了更聪明”吗？\n系统：禁止使用“提高智商”等虚假宣传，可用“科学配方助力成长”替代。\n用户：达人直播能现场喂食吗？\n系统：可以，但需确保宠物健康状况良好，禁止展示过量喂食或不当行为。\n用户：大促期间退换货率高，怎么降低？\n系统：建议在详情页增加“宠物粮食测评视频”，提供免费退换货运费险，注明“开封后不支持退换”。\n用户：平台对宠物食品标签有啥要求？\n系统：需符合GB 16798-2016标准，标注成分表、营养成分、适用年龄段等信息。\n用户：这么说我们得准备品质证明和标签合规了？\n系统：是的，宠物食品安全性和透明度是关键，建议建立完善的质量追溯体系。\n\n当前问题：这么说我们得准备品质证明和标签合规了？\n请你根据任务规则输出重写结果。\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=0.5, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 21:25:42 [async_llm.py:252] Added request chatcmpl-8fba64aad8bd4b2d97913b815e822117.
INFO:     127.0.0.1:45860 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-19 21:25:42 [logger.py:39] Received request chatcmpl-12b780670ffd410fbf153f4984adadef: prompt: '<｜begin▁of▁sentence｜>你是一个专业的问题重写模块，专门用于多轮对话场景下的指代补全与语义还原任务。\n请严格按照以下规则执行：\n\n【任务目标】\n1. 你的目标是准确解析用户真实意图，从历史中发掘指代和意图，使得这个独立问题尽量完整，尽可能包含所有信息关键词,特别是要补充关键的动机，指代和场景（润色后的问题至少涉及：用户面对的前置情况，什么限制，什么疑问等）。\n2. 仅当历史信息能够提供明确上下文时才进行补全，否则保持当前问题不变。\n3. 对于明显不需要补全的问题如：吹水："呵呵"，命令："关机"，无关内容："人生的意义"等，返回原句\n4. 不进行任何无关发挥、扩写、润色、修辞性描述、解释、总结、感情色彩。\n5. 不编造任何不存在的假设背景或新信息。\n6. 输出格式必须严格遵循：仅输出最终重写结果文本，不包含任何前缀、提示词、说明性文字或换行符。重写的疑问句以："我想知道"开头，非疑问句你自行适配\n7. 当无需重写时，直接输出原问题。\n\n【重写示例】\n历史对话：\n用户：小红鞋什么时候有货？\n系统：请问您指的是哪款小红鞋？\n用户：就是上次缺货那款\n当前问题：就是上次缺货那款\n重写结果：我想知道上次缺货的那款小红鞋什么时候有货？\n\n历史对话：\n用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：那我们还有其他计划，能投放吗？\n当前问题：那我们还有其他计划，能投放吗？\n重写结果：我想知道当账号处于限流状态时，是否可以继续开启新的广告投放计划？\n\n历史对话：\n用户：你是谁？\n当前问题：你是谁？\n重写结果：你是谁？\n\n历史对话：\n用户：开心\n当前问题：开心\n重写结果：开心\n\n历史对话：\n用户：我要买它\n系统：请问您指的是哪款商品？\n用户：之前推荐的那双\n当前问题：之前推荐的那双\n重写结果：我想要买你之前推荐的那双鞋。\n\n注意：严格按照以上风格工作，禁止任何额外输出。\n\nUser: 用户：我们想做短视频投放，但是没有经验。\n系统：短视频适合做产品亮点展示，建议控制在15秒内。\n用户：那视频需要真人出镜吗？\n系统：不强制，但真人出镜有助于提升信任感和转化率。\n用户：我们是美妆类产品，有必要找达人吗？\n系统：美妆类非常依赖达人口碑，建议尝试合作。\n用户：预算不高，怎么办？\n系统：可尝试中腰部达人或参与平台的达人共创计划。\n用户：我们还没开通素材中心，影响大吗？\n系统：建议开通，有助于管理和测试不同创意的效果。\n用户：投放过程中可以临时换素材吗？\n系统：可以更换，但建议保持主素材稳定，防止数据波动。\n\n当前问题：投放过程中可以临时换素材吗？\n请你根据任务规则输出重写结果。\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=0.5, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 21:25:42 [async_llm.py:252] Added request chatcmpl-12b780670ffd410fbf153f4984adadef.
INFO:     127.0.0.1:45864 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-19 21:25:42 [logger.py:39] Received request chatcmpl-67b0119260624bd3a839c24a0e3e12c3: prompt: '<｜begin▁of▁sentence｜>你是一个专业的问题重写模块，专门用于多轮对话场景下的指代补全与语义还原任务。\n请严格按照以下规则执行：\n\n【任务目标】\n1. 你的目标是准确解析用户真实意图，从历史中发掘指代和意图，使得这个独立问题尽量完整，尽可能包含所有信息关键词,特别是要补充关键的动机，指代和场景（润色后的问题至少涉及：用户面对的前置情况，什么限制，什么疑问等）。\n2. 仅当历史信息能够提供明确上下文时才进行补全，否则保持当前问题不变。\n3. 对于明显不需要补全的问题如：吹水："呵呵"，命令："关机"，无关内容："人生的意义"等，返回原句\n4. 不进行任何无关发挥、扩写、润色、修辞性描述、解释、总结、感情色彩。\n5. 不编造任何不存在的假设背景或新信息。\n6. 输出格式必须严格遵循：仅输出最终重写结果文本，不包含任何前缀、提示词、说明性文字或换行符。重写的疑问句以："我想知道"开头，非疑问句你自行适配\n7. 当无需重写时，直接输出原问题。\n\n【重写示例】\n历史对话：\n用户：小红鞋什么时候有货？\n系统：请问您指的是哪款小红鞋？\n用户：就是上次缺货那款\n当前问题：就是上次缺货那款\n重写结果：我想知道上次缺货的那款小红鞋什么时候有货？\n\n历史对话：\n用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：那我们还有其他计划，能投放吗？\n当前问题：那我们还有其他计划，能投放吗？\n重写结果：我想知道当账号处于限流状态时，是否可以继续开启新的广告投放计划？\n\n历史对话：\n用户：你是谁？\n当前问题：你是谁？\n重写结果：你是谁？\n\n历史对话：\n用户：开心\n当前问题：开心\n重写结果：开心\n\n历史对话：\n用户：我要买它\n系统：请问您指的是哪款商品？\n用户：之前推荐的那双\n当前问题：之前推荐的那双\n重写结果：我想要买你之前推荐的那双鞋。\n\n注意：严格按照以上风格工作，禁止任何额外输出。\n\nUser: 用户：我们最近素材通过率太低了。\n系统：建议检查是否有违规用词或画面不清晰。\n用户：我们用的是直播录屏，会不会有影响？\n系统：录屏内容不清晰、低质量可能被判定为低质素材。\n用户：那怎么判断素材是不是合格？\n系统：可以用平台素材质检功能，或参考审核规则文档。\n用户：有没有能提高通过率的方法？\n系统：提升清晰度、去除违规文案和添加平台推荐模板可提升通过率。\n用户：我们素材都用统一结尾，这样会影响吗？\n系统：结尾一致没问题，但内容重复度高仍需多样性优化。\n\n当前问题：我们素材都用统一结尾，这样会影响吗？\n请你根据任务规则输出重写结果。\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=0.5, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 21:25:42 [async_llm.py:252] Added request chatcmpl-67b0119260624bd3a839c24a0e3e12c3.
INFO:     127.0.0.1:45880 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:45860 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:45866 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:45864 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-19 21:25:48 [loggers.py:111] Engine 000: Avg prompt throughput: 3531.5 tokens/s, Avg generation throughput: 91.5 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 84.3%
INFO 06-19 21:25:58 [loggers.py:111] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 84.3%
INFO 06-19 21:26:27 [logger.py:39] Received request chatcmpl-9cab865ee881411dbae12c9ae6cc9461: prompt: '<｜begin▁of▁sentence｜>你是一个专业的问题重写模块，专门用于多轮对话场景下的指代补全与语义还原任务。\n请严格按照以下规则执行：\n\n【任务目标】\n1. 你的目标是准确解析用户真实意图，从历史中发掘指代和意图，使得这个独立问题尽量完整，尽可能包含所有信息关键词,特别是要补充关键的动机，指代和场景（润色后的问题至少涉及：用户面对的前置情况，什么限制，什么疑问等）。\n2. 仅当历史信息能够提供明确上下文时才进行补全，否则保持当前问题不变。\n3. 对于明显不需要补全的问题如：吹水："呵呵"，命令："关机"，无关内容："人生的意义"等，返回原句\n4. 不进行任何无关发挥、扩写、润色、修辞性描述、解释、总结、感情色彩。\n5. 不编造任何不存在的假设背景或新信息。\n6. 输出格式必须严格遵循：仅输出最终重写结果文本，不包含任何前缀、提示词、说明性文字或换行符。重写的疑问句以："我想知道"开头，非疑问句你自行适配\n7. 当无需重写时，直接输出原问题。\n\n【重写示例】\n历史对话：\n用户：小红鞋什么时候有货？\n系统：请问您指的是哪款小红鞋？\n用户：就是上次缺货那款\n当前问题：就是上次缺货那款\n重写结果：我想知道上次缺货的那款小红鞋什么时候有货？\n\n历史对话：\n用户：我们前几天账号被限流了，不知道什么原因\n系统：限流可能是因为素材违规或账户表现不佳。\n用户：那我们还有其他计划，能投放吗？\n当前问题：那我们还有其他计划，能投放吗？\n重写结果：我想知道当账号处于限流状态时，是否可以继续开启新的广告投放计划？\n\n历史对话：\n用户：你是谁？\n当前问题：你是谁？\n重写结果：你是谁？\n\n历史对话：\n用户：开心\n当前问题：开心\n重写结果：开心\n\n历史对话：\n用户：我要买它\n系统：请问您指的是哪款商品？\n用户：之前推荐的那双\n当前问题：之前推荐的那双\n重写结果：我想要买你之前推荐的那双鞋。\n\n注意：严格按照以上风格工作，禁止任何额外输出。\n\nUser: 用户：哈哈一张图片\n系统：哎呀～您发的是图片呀，我这边暂时看不到具体内容呢😅 如果是想咨询抖音广告相关的问题，可以文字描述下需求哈～\n用户：五张图片\n系统：哎呀～您是想问图片相关的广告投放吗？比如用图片素材做信息流广告之类的？可以具体说说需求嘛～\n用户：图片\n系统：哈哈您是想问图片广告怎么投吗？比如用图片做信息流或者开屏广告？可以具体说说想推广啥产品呀～\n用户：五张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/589a675592ab4d878cdd56b8f9dcf43a..png\n用户：五张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/a2dbe01a4d6848848935dca6e5f91911..png\n用户：五张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/2d54d547c2384735a90189966f0fcd8f..png\n用户：五张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/ffdf42977895448d8ffaa78d5cbd271b..png\n用户：五张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/c3f19ce42ba9490185fd3a622c51a918..png\n用户：五张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/589a675592ab4d878cdd56b8f9dcf43a..png\n用户：五张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/a2dbe01a4d6848848935dca6e5f91911..png\n用户：五张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/2d54d547c2384735a90189966f0fcd8f..png\n用户：五张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/ffdf42977895448d8ffaa78d5cbd271b..png\n用户：五张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/c3f19ce42ba9490185fd3a622c51a918..png\n用户：五张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/589a675592ab4d878cdd56b8f9dcf43a..png\n用户：五张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/a2dbe01a4d6848848935dca6e5f91911..png\n用户：五张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/2d54d547c2384735a90189966f0fcd8f..png\n用户：五张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/ffdf42977895448d8ffaa78d5cbd271b..png\n用户：五张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/c3f19ce42ba9490185fd3a622c51a918..png\n用户：新加入问答库的一张图片\n系统：文本答案\n用户：新加入问答库的一张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/b62b8cadc2d34a1883c8a102955d66c1.jpg\n用户：新加入问答库的一张图片\n系统：https://cs-test.zhilingsd.com/static/1/00/00/c019db5cfbf04d2eb0ae76633261c688.jpg\n用户：一个超长文本\n\n当前问题：一个超长文本\n请你根据任务规则输出重写结果。\n\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=0.5, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-19 21:26:27 [async_llm.py:252] Added request chatcmpl-9cab865ee881411dbae12c9ae6cc9461.
INFO:     127.0.0.1:37132 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-19 21:26:28 [loggers.py:111] Engine 000: Avg prompt throughput: 180.4 tokens/s, Avg generation throughput: 1.2 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 83.3%
INFO 06-19 21:26:38 [loggers.py:111] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 83.3%
ERROR 06-20 02:47:58 [multiproc_executor.py:123] Worker proc VllmWorker-0 died unexpectedly, shutting down executor.
ERROR 06-20 02:48:00 [core.py:398] EngineCore encountered a fatal error.
ERROR 06-20 02:48:00 [core.py:398] Traceback (most recent call last):
ERROR 06-20 02:48:00 [core.py:398]   File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/v1/engine/core.py", line 389, in run_engine_core
ERROR 06-20 02:48:00 [core.py:398]     engine_core.run_busy_loop()
ERROR 06-20 02:48:00 [core.py:398]   File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/v1/engine/core.py", line 411, in run_busy_loop
ERROR 06-20 02:48:00 [core.py:398]     self._process_input_queue()
ERROR 06-20 02:48:00 [core.py:398]   File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/v1/engine/core.py", line 424, in _process_input_queue
ERROR 06-20 02:48:00 [core.py:398]     self._handle_client_request(*req)
ERROR 06-20 02:48:00 [core.py:398]   File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/v1/engine/core.py", line 465, in _handle_client_request
ERROR 06-20 02:48:00 [core.py:398]     raise RuntimeError("Executor failed.")
ERROR 06-20 02:48:00 [core.py:398] RuntimeError: Executor failed.
Process EngineCore_0:
Traceback (most recent call last):
  File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/v1/engine/core.py", line 400, in run_engine_core
    raise e
  File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/v1/engine/core.py", line 389, in run_engine_core
    engine_core.run_busy_loop()
  File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/v1/engine/core.py", line 411, in run_busy_loop
    self._process_input_queue()
  File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/v1/engine/core.py", line 424, in _process_input_queue
    self._handle_client_request(*req)
  File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/v1/engine/core.py", line 465, in _handle_client_request
    raise RuntimeError("Executor failed.")
RuntimeError: Executor failed.
ERROR 06-20 02:48:00 [async_llm.py:399] AsyncLLM output_handler failed.
ERROR 06-20 02:48:00 [async_llm.py:399] Traceback (most recent call last):
ERROR 06-20 02:48:00 [async_llm.py:399]   File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/v1/engine/async_llm.py", line 357, in output_handler
ERROR 06-20 02:48:00 [async_llm.py:399]     outputs = await engine_core.get_output_async()
ERROR 06-20 02:48:00 [async_llm.py:399]   File "/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/site-packages/vllm/v1/engine/core_client.py", line 716, in get_output_async
ERROR 06-20 02:48:00 [async_llm.py:399]     raise self._format_exception(outputs) from None
ERROR 06-20 02:48:00 [async_llm.py:399] vllm.v1.engine.exceptions.EngineDeadError: EngineCore encountered an issue. See stack trace (above) for the root cause.
INFO:     Shutting down
INFO:     Waiting for application shutdown.
INFO:     Application shutdown complete.
INFO:     Finished server process [2951169]
/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
/home/algo/miniconda3/envs/htmlRAG/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 2 leaked shared_memory objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
[2025-06-24 21:59:55] [INFO] ====================================================================================================
[2025-06-24 21:59:55] [INFO] ✅ 全局配置和日志系统初始化完成，服务器时区GMT-4注意转换（+12h为北京时间）
Building prefix dict from the default dictionary ...
Loading model from cache /tmp/jieba.cache
Loading model cost 0.374 seconds.
Prefix dict has been built successfully.
INFO:     Started server process [1241854]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8080 (Press CTRL+C to quit)
INFO:     192.168.7.58:54065 - "GET /ping HTTP/1.1" 200 OK
[2025-06-24 22:01:20] [INFO] 重写请求: 原始query=Hello!
[2025-06-24 22:01:20] [DEBUG] 🔍 对话历史过短，跳过 Query 重写：Hello!
[2025-06-24 22:01:20] [INFO] Hello! 重写完成: Hello!
INFO:     192.168.7.58:54103 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-24 22:06:39] [INFO] 重写请求: 原始query=品牌曝光度
[2025-06-24 22:06:39] [DEBUG] 🚀 请求 vLLM: http://192.168.7.247:8011/v1/chat/completions
[2025-06-24 22:06:40] [DEBUG] 品牌曝光度 重写成功，用时 0.33s，结果：我想知道如何通过抖音广告提高品牌曝光度？
[2025-06-24 22:06:40] [INFO] 品牌曝光度 重写完成: 我想知道如何通过抖音广告提高品牌曝光度？
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-24 22:28:25] [INFO] 重写请求: 原始query=询问问答库可以得到什么答案
[2025-06-24 22:28:25] [DEBUG] 🚀 请求 vLLM: http://192.168.7.247:8011/v1/chat/completions
[2025-06-24 22:28:25] [DEBUG] 询问问答库可以得到什么答案 重写成功，用时 0.31s，结果：我想知道问答库可以提供哪些答案。
[2025-06-24 22:28:25] [INFO] 询问问答库可以得到什么答案 重写完成: 我想知道问答库可以提供哪些答案。
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-24 22:28:53] [INFO] 重写请求: 原始query=一张图片9文本
[2025-06-24 22:28:53] [DEBUG] 🚀 请求 vLLM: http://192.168.7.247:8011/v1/chat/completions
[2025-06-24 22:28:53] [DEBUG] 一张图片9文本 重写成功，用时 0.32s，结果：我想知道一张图片可以包含多少个文本描述？
[2025-06-24 22:28:53] [INFO] 一张图片9文本 重写完成: 我想知道一张图片可以包含多少个文本描述？
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-24 22:29:45] [INFO] 重写请求: 原始query=一张图片9文本
[2025-06-24 22:29:45] [DEBUG] 🚀 请求 vLLM: http://192.168.7.247:8011/v1/chat/completions
[2025-06-24 22:29:46] [DEBUG] 一张图片9文本 重写成功，用时 0.32s，结果：我想知道一张图片9文本可以得到什么答案。
[2025-06-24 22:29:46] [INFO] 一张图片9文本 重写完成: 我想知道一张图片9文本可以得到什么答案。
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-24 22:34:38] [INFO] 重写请求: 原始query=一个超长文本
[2025-06-24 22:34:38] [DEBUG] 🚀 请求 vLLM: http://192.168.7.247:8011/v1/chat/completions
[2025-06-24 22:34:39] [DEBUG] 一个超长文本 重写成功，用时 0.35s，结果：我想知道你想要分享的"一个超长文本"是什么内容？
[2025-06-24 22:34:39] [INFO] 一个超长文本 重写完成: 我想知道你想要分享的"一个超长文本"是什么内容？
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 03:16:14] [INFO] 重写请求: 原始query=设置网络
[2025-06-25 03:16:14] [DEBUG] 🚀 请求 vLLM: http://192.168.7.247:8011/v1/chat/completions
[2025-06-25 03:16:15] [DEBUG] 设置网络 重写成功，用时 0.43s，结果：我想知道您是想了解网络推广的广告投放对吧？具体是想投信息流广告还是搜索广告呢？这两块玩法不太一样哦～
[2025-06-25 03:16:15] [INFO] 设置网络 重写完成: 我想知道您是想了解网络推广的广告投放对吧？具体是想投信息流广告还是搜索广告呢？这两块玩法不太一样哦～
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 03:16:24] [INFO] 重写请求: 原始query=怎么快速短视频优化策略
[2025-06-25 03:16:24] [DEBUG] 🔍 对话历史过短，跳过 Query 重写：怎么快速短视频优化策略
[2025-06-25 03:16:24] [INFO] 怎么快速短视频优化策略 重写完成: 怎么快速短视频优化策略
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 03:16:34] [INFO] 重写请求: 原始query=平台怎么识别造假的
[2025-06-25 03:16:34] [DEBUG] 🚀 请求 vLLM: http://192.168.7.247:8011/v1/chat/completions
[2025-06-25 03:16:34] [DEBUG] 平台怎么识别造假的 重写成功，用时 0.34s，结果：我想知道抖音平台如何识别广告投放中的资质造假行为。
[2025-06-25 03:16:34] [INFO] 平台怎么识别造假的 重写完成: 我想知道抖音平台如何识别广告投放中的资质造假行为。
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 03:26:31] [INFO] 重写请求: 原始query=你好，请做自我介绍
[2025-06-25 03:26:31] [DEBUG] 🚀 请求 vLLM: http://192.168.7.247:8011/v1/chat/completions
[2025-06-25 03:26:31] [DEBUG] 你好，请做自我介绍 重写成功，用时 0.28s，结果：我想知道如何进行自我介绍。
[2025-06-25 03:26:31] [INFO] 你好，请做自我介绍 重写完成: 我想知道如何进行自我介绍。
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 03:31:16] [INFO] 重写请求: 原始query=你好，请做自我介绍
[2025-06-25 03:31:16] [DEBUG] 🔍 对话历史过短，跳过 Query 重写：你好，请做自我介绍
[2025-06-25 03:31:16] [INFO] 你好，请做自我介绍 重写完成: 你好，请做自我介绍
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 03:31:25] [INFO] 重写请求: 原始query=你好，请做自我介绍
[2025-06-25 03:31:25] [DEBUG] 🔍 对话历史过短，跳过 Query 重写：你好，请做自我介绍
[2025-06-25 03:31:25] [INFO] 你好，请做自我介绍 重写完成: 你好，请做自我介绍
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 03:31:35] [INFO] 重写请求: 原始query=你好，请做自我介绍
[2025-06-25 03:31:35] [DEBUG] 🚀 请求 vLLM: http://192.168.7.247:8011/v1/chat/completions
[2025-06-25 03:31:35] [DEBUG] 你好，请做自我介绍 重写成功，用时 0.35s，结果：我想知道你希望我提供哪些信息来完成自我介绍？
[2025-06-25 03:31:35] [INFO] 你好，请做自我介绍 重写完成: 我想知道你希望我提供哪些信息来完成自我介绍？
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 03:31:51] [INFO] 重写请求: 原始query=你好，请做自我介绍
[2025-06-25 03:31:51] [DEBUG] 🔍 对话历史过短，跳过 Query 重写：你好，请做自我介绍
[2025-06-25 03:31:51] [INFO] 你好，请做自我介绍 重写完成: 你好，请做自我介绍
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 03:32:00] [INFO] 重写请求: 原始query=你好，请做自我介绍
[2025-06-25 03:32:00] [DEBUG] 🚀 请求 vLLM: http://192.168.7.247:8011/v1/chat/completions
[2025-06-25 03:32:00] [DEBUG] 你好，请做自我介绍 重写成功，用时 0.16s，结果：我想知道您是谁？
[2025-06-25 03:32:00] [INFO] 你好，请做自我介绍 重写完成: 我想知道您是谁？
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 03:32:12] [INFO] 重写请求: 原始query=你好，请做自我介绍
[2025-06-25 03:32:12] [DEBUG] 🚀 请求 vLLM: http://192.168.7.247:8011/v1/chat/completions
[2025-06-25 03:32:12] [DEBUG] 你好，请做自我介绍 重写成功，用时 0.55s，结果：抱歉，根据您的提示，我无法确定您希望我执行的任务。请提供更具体的信息和任务目标，以便我能够更好地回答您的问题。
[2025-06-25 03:32:12] [INFO] 你好，请做自我介绍 重写完成: 抱歉，根据您的提示，我无法确定您希望我执行的任务。请提供更具体的信息和任务目标，以便我能够更好地回答您的问题。
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 03:32:25] [INFO] 重写请求: 原始query=你好，请做自我介绍
[2025-06-25 03:32:25] [DEBUG] 🚀 请求 vLLM: http://192.168.7.247:8011/v1/chat/completions
[2025-06-25 03:32:25] [WARNING] ⚠️ vLLM 请求失败: http://192.168.7.247:8011/v1/chat/completions → 400, message='Bad Request', url='http://192.168.7.247:8011/v1/chat/completions'
[2025-06-25 03:32:25] [ERROR] ⚠️ 重写请求失败，返回原问题：所有 vLLM 实例请求失败: http://192.168.7.247:8011/v1/chat/completions: 400, message='Bad Request', url='http://192.168.7.247:8011/v1/chat/completions'
[2025-06-25 03:32:25] [INFO] 你好，请做自我介绍 重写完成: 你好，请做自我介绍
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 03:32:43] [INFO] 重写请求: 原始query=你好，请做自我介绍
[2025-06-25 03:32:43] [DEBUG] 🔍 对话历史过短，跳过 Query 重写：你好，请做自我介绍
[2025-06-25 03:32:43] [INFO] 你好，请做自我介绍 重写完成: 你好，请做自我介绍
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 03:32:52] [INFO] 重写请求: 原始query=你好，请做自我介绍
[2025-06-25 03:32:52] [DEBUG] 🔍 对话历史过短，跳过 Query 重写：你好，请做自我介绍
[2025-06-25 03:32:52] [INFO] 你好，请做自我介绍 重写完成: 你好，请做自我介绍
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 03:33:59] [INFO] 重写请求: 原始query=你好帮我查一下下单了招募任务 达人交付完成后 怎么取消该计划
[2025-06-25 03:33:59] [DEBUG] 🔍 对话历史过短，跳过 Query 重写：你好帮我查一下下单了招募任务 达人交付完成后 怎么取消该计划
[2025-06-25 03:33:59] [INFO] 你好帮我查一下下单了招募任务 达人交付完成后 怎么取消该计划 重写完成: 你好帮我查一下下单了招募任务 达人交付完成后 怎么取消该计划
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 03:34:25] [INFO] 重写请求: 原始query=完成交付后怎么取消该任务呢
[2025-06-25 03:34:25] [DEBUG] 🔍 对话历史过短，跳过 Query 重写：完成交付后怎么取消该任务呢
[2025-06-25 03:34:25] [INFO] 完成交付后怎么取消该任务呢 重写完成: 完成交付后怎么取消该任务呢
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 03:34:52] [INFO] 重写请求: 原始query=直播全域推广roi如何设置比较合理
[2025-06-25 03:34:52] [DEBUG] 🔍 对话历史过短，跳过 Query 重写：直播全域推广roi如何设置比较合理
[2025-06-25 03:34:52] [INFO] 直播全域推广roi如何设置比较合理 重写完成: 直播全域推广roi如何设置比较合理
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 03:34:53] [INFO] 重写请求: 原始query=你好，请做自我介绍
[2025-06-25 03:34:53] [DEBUG] 🔍 对话历史过短，跳过 Query 重写：你好，请做自我介绍
[2025-06-25 03:34:53] [INFO] 你好，请做自我介绍 重写完成: 你好，请做自我介绍
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 03:37:34] [INFO] 重写请求: 原始query=你好帮我查一下下单了招募任务 达人交付完成后 怎么取消该计划
[2025-06-25 03:37:34] [DEBUG] 🚀 请求 vLLM: http://192.168.7.247:8011/v1/chat/completions
[2025-06-25 03:37:34] [DEBUG] 你好帮我查一下下单了招募任务 达人交付完成后 怎么取消该计划 重写成功，用时 0.24s，结果：我想知道招募任务达人交付完成后，如何取消该计划？
[2025-06-25 03:37:34] [INFO] 你好帮我查一下下单了招募任务 达人交付完成后 怎么取消该计划 重写完成: 我想知道招募任务达人交付完成后，如何取消该计划？
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 03:37:58] [INFO] 重写请求: 原始query=开启网络直播条件设置
[2025-06-25 03:37:58] [DEBUG] 🚀 请求 vLLM: http://192.168.7.247:8011/v1/chat/completions
[2025-06-25 03:37:59] [DEBUG] 开启网络直播条件设置 重写成功，用时 0.30s，结果：我想知道开启网络直播的条件设置是什么？
[2025-06-25 03:37:59] [INFO] 开启网络直播条件设置 重写完成: 我想知道开启网络直播的条件设置是什么？
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 03:38:10] [INFO] 重写请求: 原始query=你好，请做自我介绍
[2025-06-25 03:38:10] [DEBUG] 🚀 请求 vLLM: http://192.168.7.247:8011/v1/chat/completions
[2025-06-25 03:38:10] [DEBUG] 你好，请做自我介绍 重写成功，用时 0.24s，结果：我想知道阿理是谁，以及她可以提供哪些帮助？
[2025-06-25 03:38:10] [INFO] 你好，请做自我介绍 重写完成: 我想知道阿理是谁，以及她可以提供哪些帮助？
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 03:38:19] [INFO] 重写请求: 原始query=今天天气怎样
[2025-06-25 03:38:19] [DEBUG] 🔍 对话历史过短，跳过 Query 重写：今天天气怎样
[2025-06-25 03:38:19] [INFO] 今天天气怎样 重写完成: 今天天气怎样
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 03:38:27] [INFO] 重写请求: 原始query=新加群聊，大家好
[2025-06-25 03:38:27] [DEBUG] 🚀 请求 vLLM: http://192.168.7.247:8011/v1/chat/completions
[2025-06-25 03:38:28] [DEBUG] 新加群聊，大家好 重写成功，用时 0.28s，结果：重写结果：大家好，我刚刚加入群聊，可以开始提问了吗？
[2025-06-25 03:38:28] [INFO] 新加群聊，大家好 重写完成: 重写结果：大家好，我刚刚加入群聊，可以开始提问了吗？
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 03:38:36] [INFO] 重写请求: 原始query=你好，请做自我介绍
[2025-06-25 03:38:36] [DEBUG] 🚀 请求 vLLM: http://192.168.7.247:8011/v1/chat/completions
[2025-06-25 03:38:36] [DEBUG] 你好，请做自我介绍 重写成功，用时 0.21s，结果：我想知道您需要了解关于我的哪些信息？
[2025-06-25 03:38:36] [INFO] 你好，请做自我介绍 重写完成: 我想知道您需要了解关于我的哪些信息？
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 03:38:45] [INFO] 重写请求: 原始query=你好，请做自我介绍
[2025-06-25 03:38:45] [DEBUG] 🚀 请求 vLLM: http://192.168.7.247:8011/v1/chat/completions
[2025-06-25 03:38:45] [DEBUG] 你好，请做自我介绍 重写成功，用时 0.24s，结果：我想知道您需要了解关于阿理的哪些信息？
[2025-06-25 03:38:45] [INFO] 你好，请做自我介绍 重写完成: 我想知道您需要了解关于阿理的哪些信息？
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 03:38:54] [INFO] 重写请求: 原始query=怎么利用网络服务赚钱
[2025-06-25 03:38:54] [DEBUG] 🚀 请求 vLLM: http://192.168.7.247:8011/v1/chat/completions
[2025-06-25 03:38:54] [DEBUG] 怎么利用网络服务赚钱 重写成功，用时 0.20s，结果：我想知道如何利用网络服务来赚钱。
[2025-06-25 03:38:54] [INFO] 怎么利用网络服务赚钱 重写完成: 我想知道如何利用网络服务来赚钱。
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 03:39:26] [INFO] 重写请求: 原始query=达人户如何开千川
[2025-06-25 03:39:26] [DEBUG] 🔍 对话历史过短，跳过 Query 重写：达人户如何开千川
[2025-06-25 03:39:26] [INFO] 达人户如何开千川 重写完成: 达人户如何开千川
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 04:19:45] [INFO] 重写请求: 原始query=直播全域推广roi如何设置比较合理
[2025-06-25 04:19:45] [DEBUG] 🚀 请求 vLLM: http://192.168.7.247:8011/v1/chat/completions
[2025-06-25 04:19:45] [DEBUG] 直播全域推广roi如何设置比较合理 重写成功，用时 0.38s，结果：我想知道在设置直播全域推广ROI时，应该如何设置比较合理？
[2025-06-25 04:19:45] [INFO] 直播全域推广roi如何设置比较合理 重写完成: 我想知道在设置直播全域推广ROI时，应该如何设置比较合理？
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 04:20:19] [INFO] 重写请求: 原始query=怎么利用网络服务赚钱
[2025-06-25 04:20:19] [DEBUG] 🚀 请求 vLLM: http://192.168.7.247:8011/v1/chat/completions
[2025-06-25 04:20:20] [DEBUG] 怎么利用网络服务赚钱 重写成功，用时 0.30s，结果：我想知道在抖音上赚钱有哪些方式？
[2025-06-25 04:20:20] [INFO] 怎么利用网络服务赚钱 重写完成: 我想知道在抖音上赚钱有哪些方式？
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 04:20:29] [INFO] 重写请求: 原始query=今天天气怎样
[2025-06-25 04:20:29] [DEBUG] 🚀 请求 vLLM: http://192.168.7.247:8011/v1/chat/completions
[2025-06-25 04:20:30] [DEBUG] 今天天气怎样 重写成功，用时 0.46s，结果：抱歉，我无法执行这个任务。我是一个AI语言模型，只能回答与计算机科学相关的问题。对于天气等非计算机科学的问题，我无法提供答案。
[2025-06-25 04:20:30] [INFO] 今天天气怎样 重写完成: 抱歉，我无法执行这个任务。我是一个AI语言模型，只能回答与计算机科学相关的问题。对于天气等非计算机科学的问题，我无法提供答案。
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 04:20:38] [INFO] 重写请求: 原始query=你好，请做自我介绍
[2025-06-25 04:20:38] [DEBUG] 🚀 请求 vLLM: http://192.168.7.247:8011/v1/chat/completions
[2025-06-25 04:20:38] [DEBUG] 你好，请做自我介绍 重写成功，用时 0.22s，结果：我想知道您是谁，可以做自我介绍吗？
[2025-06-25 04:20:38] [INFO] 你好，请做自我介绍 重写完成: 我想知道您是谁，可以做自我介绍吗？
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 04:20:49] [INFO] 重写请求: 原始query=新加群聊，大家好
[2025-06-25 04:20:49] [DEBUG] 🚀 请求 vLLM: http://192.168.7.247:8011/v1/chat/completions
[2025-06-25 04:20:49] [DEBUG] 新加群聊，大家好 重写成功，用时 0.24s，结果：重写结果：大家好，我是新加群聊的。
[2025-06-25 04:20:49] [INFO] 新加群聊，大家好 重写完成: 重写结果：大家好，我是新加群聊的。
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 04:20:57] [INFO] 重写请求: 原始query=达人户如何开千川
[2025-06-25 04:20:57] [DEBUG] 🚀 请求 vLLM: http://192.168.7.247:8011/v1/chat/completions
[2025-06-25 04:20:58] [DEBUG] 达人户如何开千川 重写成功，用时 0.30s，结果：我想知道达人户如何开通千川。
[2025-06-25 04:20:58] [INFO] 达人户如何开千川 重写完成: 我想知道达人户如何开通千川。
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 04:21:21] [INFO] 重写请求: 原始query=开启网络直播条件设置
[2025-06-25 04:21:21] [DEBUG] 🚀 请求 vLLM: http://192.168.7.247:8011/v1/chat/completions
[2025-06-25 04:21:22] [DEBUG] 开启网络直播条件设置 重写成功，用时 0.36s，结果：抱歉，根据任务规则，我无法输出任何内容。请提供一个独立的问题，我将尽力帮助您。
[2025-06-25 04:21:22] [INFO] 开启网络直播条件设置 重写完成: 抱歉，根据任务规则，我无法输出任何内容。请提供一个独立的问题，我将尽力帮助您。
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 04:21:34] [INFO] 重写请求: 原始query=你好，请做自我介绍
[2025-06-25 04:21:34] [DEBUG] 🚀 请求 vLLM: http://192.168.7.247:8011/v1/chat/completions
[2025-06-25 04:21:34] [DEBUG] 你好，请做自我介绍 重写成功，用时 0.19s，结果：我想知道你好，请做自我介绍
[2025-06-25 04:21:34] [INFO] 你好，请做自我介绍 重写完成: 我想知道你好，请做自我介绍
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 04:21:42] [INFO] 重写请求: 原始query=全域推广怎么投
[2025-06-25 04:21:42] [DEBUG] 🚀 请求 vLLM: http://192.168.7.247:8011/v1/chat/completions
[2025-06-25 04:21:42] [DEBUG] 全域推广怎么投 重写成功，用时 0.21s，结果：我想知道如何在抖音上进行全域推广投放？
[2025-06-25 04:21:42] [INFO] 全域推广怎么投 重写完成: 我想知道如何在抖音上进行全域推广投放？
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 04:21:59] [INFO] 重写请求: 原始query=redis截图1
[2025-06-25 04:21:59] [DEBUG] 🔍 对话历史过短，跳过 Query 重写：redis截图1
[2025-06-25 04:21:59] [INFO] redis截图1 重写完成: redis截图1
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 04:22:07] [INFO] 重写请求: 原始query=你好帮我查一下下单了招募任务 达人交付完成后 怎么取消该计划
[2025-06-25 04:22:07] [DEBUG] 🚀 请求 vLLM: http://192.168.7.247:8011/v1/chat/completions
[2025-06-25 04:22:07] [DEBUG] 你好帮我查一下下单了招募任务 达人交付完成后 怎么取消该计划 重写成功，用时 0.34s，结果：我想知道招募任务达人交付完成后，如何取消该计划？
[2025-06-25 04:22:07] [INFO] 你好帮我查一下下单了招募任务 达人交付完成后 怎么取消该计划 重写完成: 我想知道招募任务达人交付完成后，如何取消该计划？
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 04:22:18] [INFO] 重写请求: 原始query=你好，请做自我介绍
[2025-06-25 04:22:18] [DEBUG] 🚀 请求 vLLM: http://192.168.7.247:8011/v1/chat/completions
[2025-06-25 04:22:18] [DEBUG] 你好，请做自我介绍 重写成功，用时 0.17s，结果：我想知道您是谁？
[2025-06-25 04:22:18] [INFO] 你好，请做自我介绍 重写完成: 我想知道您是谁？
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 04:22:27] [INFO] 重写请求: 原始query=我是电商达人
[2025-06-25 04:22:27] [DEBUG] 🔍 对话历史过短，跳过 Query 重写：我是电商达人
[2025-06-25 04:22:27] [INFO] 我是电商达人 重写完成: 我是电商达人
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 04:22:36] [INFO] 重写请求: 原始query=你好，请做自我介绍
[2025-06-25 04:22:36] [DEBUG] 🚀 请求 vLLM: http://192.168.7.247:8011/v1/chat/completions
[2025-06-25 04:22:36] [WARNING] ⚠️ vLLM 请求失败: http://192.168.7.247:8011/v1/chat/completions → 400, message='Bad Request', url='http://192.168.7.247:8011/v1/chat/completions'
[2025-06-25 04:22:36] [ERROR] ⚠️ 重写请求失败，返回原问题：所有 vLLM 实例请求失败: http://192.168.7.247:8011/v1/chat/completions: 400, message='Bad Request', url='http://192.168.7.247:8011/v1/chat/completions'
[2025-06-25 04:22:36] [INFO] 你好，请做自我介绍 重写完成: 你好，请做自我介绍
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 04:22:50] [INFO] 重写请求: 原始query=你好，请做自我介绍
[2025-06-25 04:22:50] [DEBUG] 🔍 对话历史过短，跳过 Query 重写：你好，请做自我介绍
[2025-06-25 04:22:50] [INFO] 你好，请做自我介绍 重写完成: 你好，请做自我介绍
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 04:23:05] [INFO] 重写请求: 原始query=你的主要工作是什么
[2025-06-25 04:23:05] [DEBUG] 🔍 对话历史过短，跳过 Query 重写：你的主要工作是什么
[2025-06-25 04:23:05] [INFO] 你的主要工作是什么 重写完成: 你的主要工作是什么
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 04:23:16] [INFO] 重写请求: 原始query=千川账户如何授权抖音投放
[2025-06-25 04:23:16] [DEBUG] 🚀 请求 vLLM: http://192.168.7.247:8011/v1/chat/completions
[2025-06-25 04:23:17] [DEBUG] 千川账户如何授权抖音投放 重写成功，用时 0.25s，结果：我想知道如何将千川账户授权给抖音进行广告投放。
[2025-06-25 04:23:17] [INFO] 千川账户如何授权抖音投放 重写完成: 我想知道如何将千川账户授权给抖音进行广告投放。
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 04:23:33] [INFO] 重写请求: 原始query=你主要是做哪方面的
[2025-06-25 04:23:33] [DEBUG] 🚀 请求 vLLM: http://192.168.7.247:8011/v1/chat/completions
[2025-06-25 04:23:33] [DEBUG] 你主要是做哪方面的 重写成功，用时 0.31s，结果：我想知道阿理主要负责哪方面的业务。
[2025-06-25 04:23:33] [INFO] 你主要是做哪方面的 重写完成: 我想知道阿理主要负责哪方面的业务。
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 04:25:57] [INFO] 重写请求: 原始query=完成交付后怎么取消该任务呢
[2025-06-25 04:25:57] [DEBUG] 🚀 请求 vLLM: http://192.168.7.247:8011/v1/chat/completions
[2025-06-25 04:25:57] [DEBUG] 完成交付后怎么取消该任务呢 重写成功，用时 0.22s，结果：我想知道完成交付后怎么取消这个任务呢？
[2025-06-25 04:25:57] [INFO] 完成交付后怎么取消该任务呢 重写完成: 我想知道完成交付后怎么取消这个任务呢？
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 04:26:06] [INFO] 重写请求: 原始query=@范文洁 短视频优化策略
[2025-06-25 04:26:06] [DEBUG] 🔍 对话历史过短，跳过 Query 重写：@范文洁 短视频优化策略
[2025-06-25 04:26:06] [INFO] @范文洁 短视频优化策略 重写完成: @范文洁 短视频优化策略
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 04:33:51] [INFO] 重写请求: 原始query=直播全域推广roi如何设置比较合理
[2025-06-25 04:33:51] [DEBUG] 🚀 请求 vLLM: http://192.168.7.247:8011/v1/chat/completions
[2025-06-25 04:33:51] [DEBUG] 直播全域推广roi如何设置比较合理 重写成功，用时 0.40s，结果：我想知道在抖音上设置直播全域推广ROI时，应该如何设置比较合理？
[2025-06-25 04:33:51] [INFO] 直播全域推广roi如何设置比较合理 重写完成: 我想知道在抖音上设置直播全域推广ROI时，应该如何设置比较合理？
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 04:35:59] [INFO] 重写请求: 原始query=直播全域推广roi如何设置比较合理
[2025-06-25 04:35:59] [DEBUG] 🚀 请求 vLLM: http://192.168.7.247:8011/v1/chat/completions
[2025-06-25 04:35:59] [DEBUG] 直播全域推广roi如何设置比较合理 重写成功，用时 0.26s，结果：我想知道设置直播全域推广ROI时应该如何设置比较合理？
[2025-06-25 04:35:59] [INFO] 直播全域推广roi如何设置比较合理 重写完成: 我想知道设置直播全域推广ROI时应该如何设置比较合理？
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 04:36:17] [INFO] 重写请求: 原始query=开启网络直播条件设置
[2025-06-25 04:36:17] [DEBUG] 🚀 请求 vLLM: http://192.168.7.247:8011/v1/chat/completions
[2025-06-25 04:36:17] [DEBUG] 开启网络直播条件设置 重写成功，用时 0.31s，结果：我想知道开启网络直播的条件和设置步骤。
[2025-06-25 04:36:17] [INFO] 开启网络直播条件设置 重写完成: 我想知道开启网络直播的条件和设置步骤。
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 04:36:29] [INFO] 重写请求: 原始query=我是电商达人
[2025-06-25 04:36:29] [DEBUG] 🚀 请求 vLLM: http://192.168.7.247:8011/v1/chat/completions
[2025-06-25 04:36:29] [DEBUG] 我是电商达人 重写成功，用时 0.19s，结果：我想知道电商达人的身份是什么？
[2025-06-25 04:36:29] [INFO] 我是电商达人 重写完成: 我想知道电商达人的身份是什么？
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 04:36:40] [INFO] 重写请求: 原始query=你好，请做自我介绍
[2025-06-25 04:36:40] [DEBUG] 🚀 请求 vLLM: http://192.168.7.247:8011/v1/chat/completions
[2025-06-25 04:36:41] [DEBUG] 你好，请做自我介绍 重写成功，用时 0.18s，结果：我想知道你是一个什么样的客服？
[2025-06-25 04:36:41] [INFO] 你好，请做自我介绍 重写完成: 我想知道你是一个什么样的客服？
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 04:36:49] [INFO] 重写请求: 原始query=完成交付后怎么取消该任务呢
[2025-06-25 04:36:49] [DEBUG] 🚀 请求 vLLM: http://192.168.7.247:8011/v1/chat/completions
[2025-06-25 04:36:49] [DEBUG] 完成交付后怎么取消该任务呢 重写成功，用时 0.22s，结果：我想知道完成交付后怎么取消该任务呢？
[2025-06-25 04:36:49] [INFO] 完成交付后怎么取消该任务呢 重写完成: 我想知道完成交付后怎么取消该任务呢？
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 04:36:58] [INFO] 重写请求: 原始query=你的主要工作是什么
[2025-06-25 04:36:58] [DEBUG] 🚀 请求 vLLM: http://192.168.7.247:8011/v1/chat/completions
[2025-06-25 04:36:58] [DEBUG] 你的主要工作是什么 重写成功，用时 0.18s，结果：我想知道你的主要工作是什么。
[2025-06-25 04:36:58] [INFO] 你的主要工作是什么 重写完成: 我想知道你的主要工作是什么。
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 04:37:25] [INFO] 重写请求: 原始query=千川账户如何授权抖音投放
[2025-06-25 04:37:25] [DEBUG] 🚀 请求 vLLM: http://192.168.7.247:8011/v1/chat/completions
[2025-06-25 04:37:25] [DEBUG] 千川账户如何授权抖音投放 重写成功，用时 0.31s，结果：我想知道千川账户如何授权抖音投放？
[2025-06-25 04:37:25] [INFO] 千川账户如何授权抖音投放 重写完成: 我想知道千川账户如何授权抖音投放？
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 04:37:39] [INFO] 重写请求: 原始query=@范文洁 短视频优化策略
[2025-06-25 04:37:39] [DEBUG] 🚀 请求 vLLM: http://192.168.7.247:8011/v1/chat/completions
[2025-06-25 04:37:39] [DEBUG] @范文洁 短视频优化策略 重写成功，用时 0.29s，结果：我想知道短视频优化的策略有哪些？
[2025-06-25 04:37:39] [INFO] @范文洁 短视频优化策略 重写完成: 我想知道短视频优化的策略有哪些？
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 04:37:51] [INFO] 重写请求: 原始query=你主要是做哪方面的
[2025-06-25 04:37:51] [DEBUG] 🚀 请求 vLLM: http://192.168.7.247:8011/v1/chat/completions
[2025-06-25 04:37:51] [DEBUG] 你主要是做哪方面的 重写成功，用时 0.29s，结果：我想知道你主要是做哪方面的？
[2025-06-25 04:37:51] [INFO] 你主要是做哪方面的 重写完成: 我想知道你主要是做哪方面的？
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 04:38:06] [INFO] 重写请求: 原始query=你好帮我查一下下单了招募任务 达人交付完成后 怎么取消该计划
[2025-06-25 04:38:06] [DEBUG] 🚀 请求 vLLM: http://192.168.7.247:8011/v1/chat/completions
[2025-06-25 04:38:06] [DEBUG] 你好帮我查一下下单了招募任务 达人交付完成后 怎么取消该计划 重写成功，用时 0.24s，结果：我想知道招募任务达人交付完成后，如何取消该计划？
[2025-06-25 04:38:06] [INFO] 你好帮我查一下下单了招募任务 达人交付完成后 怎么取消该计划 重写完成: 我想知道招募任务达人交付完成后，如何取消该计划？
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 04:38:15] [INFO] 重写请求: 原始query=你好，请做自我介绍
[2025-06-25 04:38:15] [DEBUG] 🚀 请求 vLLM: http://192.168.7.247:8011/v1/chat/completions
[2025-06-25 04:38:15] [WARNING] ⚠️ vLLM 请求失败: http://192.168.7.247:8011/v1/chat/completions → 400, message='Bad Request', url='http://192.168.7.247:8011/v1/chat/completions'
[2025-06-25 04:38:15] [ERROR] ⚠️ 重写请求失败，返回原问题：所有 vLLM 实例请求失败: http://192.168.7.247:8011/v1/chat/completions: 400, message='Bad Request', url='http://192.168.7.247:8011/v1/chat/completions'
[2025-06-25 04:38:15] [INFO] 你好，请做自我介绍 重写完成: 你好，请做自我介绍
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 04:38:24] [INFO] 重写请求: 原始query=redis截图1
[2025-06-25 04:38:24] [DEBUG] 🚀 请求 vLLM: http://192.168.7.247:8011/v1/chat/completions
[2025-06-25 04:38:24] [DEBUG] redis截图1 重写成功，用时 0.31s，结果：我想知道您是否需要查询账户余额，如果是的话，麻烦提供一下业务号哦～
[2025-06-25 04:38:24] [INFO] redis截图1 重写完成: 我想知道您是否需要查询账户余额，如果是的话，麻烦提供一下业务号哦～
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 04:38:35] [INFO] 重写请求: 原始query=全域推广怎么投
[2025-06-25 04:38:35] [DEBUG] 🚀 请求 vLLM: http://192.168.7.247:8011/v1/chat/completions
[2025-06-25 04:38:35] [DEBUG] 全域推广怎么投 重写成功，用时 0.35s，结果：我想知道如何在抖音上进行全域推广，具体操作步骤是什么？
[2025-06-25 04:38:35] [INFO] 全域推广怎么投 重写完成: 我想知道如何在抖音上进行全域推广，具体操作步骤是什么？
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 04:38:53] [INFO] 重写请求: 原始query=你好，请做自我介绍
[2025-06-25 04:38:53] [DEBUG] 🚀 请求 vLLM: http://192.168.7.247:8011/v1/chat/completions
[2025-06-25 04:38:53] [DEBUG] 你好，请做自我介绍 重写成功，用时 0.16s，结果：我想知道您是谁？
[2025-06-25 04:38:53] [INFO] 你好，请做自我介绍 重写完成: 我想知道您是谁？
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 04:39:03] [INFO] 重写请求: 原始query=你好，请做自我介绍
[2025-06-25 04:39:03] [DEBUG] 🚀 请求 vLLM: http://192.168.7.247:8011/v1/chat/completions
[2025-06-25 04:39:03] [DEBUG] 你好，请做自我介绍 重写成功，用时 0.17s，结果：我想知道您是谁？
[2025-06-25 04:39:03] [INFO] 你好，请做自我介绍 重写完成: 我想知道您是谁？
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 04:39:12] [INFO] 重写请求: 原始query=达人户如何开千川
[2025-06-25 04:39:12] [DEBUG] 🚀 请求 vLLM: http://192.168.7.247:8011/v1/chat/completions
[2025-06-25 04:39:12] [DEBUG] 达人户如何开千川 重写成功，用时 0.30s，结果：我想知道达人户如何开通千川。
[2025-06-25 04:39:12] [INFO] 达人户如何开千川 重写完成: 我想知道达人户如何开通千川。
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 04:39:36] [INFO] 重写请求: 原始query=新加群聊，大家好
[2025-06-25 04:39:36] [DEBUG] 🚀 请求 vLLM: http://192.168.7.247:8011/v1/chat/completions
[2025-06-25 04:39:37] [DEBUG] 新加群聊，大家好 重写成功，用时 0.26s，结果：我想知道在抖音广告投放方面，阿理可以提供哪些帮助？
[2025-06-25 04:39:37] [INFO] 新加群聊，大家好 重写完成: 我想知道在抖音广告投放方面，阿理可以提供哪些帮助？
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 04:39:47] [INFO] 重写请求: 原始query=今天天气怎样
[2025-06-25 04:39:47] [DEBUG] 🚀 请求 vLLM: http://192.168.7.247:8011/v1/chat/completions
[2025-06-25 04:39:47] [DEBUG] 今天天气怎样 重写成功，用时 0.19s，结果：抱歉，我无法回答这个问题。
[2025-06-25 04:39:47] [INFO] 今天天气怎样 重写完成: 抱歉，我无法回答这个问题。
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 05:02:57] [INFO] 重写请求: 原始query=全域推广怎么投
[2025-06-25 05:02:57] [DEBUG] 🚀 请求 vLLM: http://192.168.7.247:8011/v1/chat/completions
[2025-06-25 05:02:58] [DEBUG] 全域推广怎么投 重写成功，用时 0.45s，结果：我想知道如何在抖音上进行全域推广，以及如何设置ROI目标和平台发券功能来提升转化？
[2025-06-25 05:02:58] [INFO] 全域推广怎么投 重写完成: 我想知道如何在抖音上进行全域推广，以及如何设置ROI目标和平台发券功能来提升转化？
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 05:03:13] [INFO] 重写请求: 原始query=你好，请做自我介绍
[2025-06-25 05:03:13] [DEBUG] 🚀 请求 vLLM: http://192.168.7.247:8011/v1/chat/completions
[2025-06-25 05:03:13] [DEBUG] 你好，请做自我介绍 重写成功，用时 0.30s，结果：我想知道你好，请做自我介绍。
[2025-06-25 05:03:13] [INFO] 你好，请做自我介绍 重写完成: 我想知道你好，请做自我介绍。
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 05:03:23] [INFO] 重写请求: 原始query=你好帮我查一下下单了招募任务 达人交付完成后 怎么取消该计划
[2025-06-25 05:03:23] [DEBUG] 🚀 请求 vLLM: http://192.168.7.247:8011/v1/chat/completions
[2025-06-25 05:03:23] [DEBUG] 你好帮我查一下下单了招募任务 达人交付完成后 怎么取消该计划 重写成功，用时 0.24s，结果：我想知道招募任务达人交付完成后，如何取消该计划？
[2025-06-25 05:03:23] [INFO] 你好帮我查一下下单了招募任务 达人交付完成后 怎么取消该计划 重写完成: 我想知道招募任务达人交付完成后，如何取消该计划？
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 05:03:31] [INFO] 重写请求: 原始query=今天天气怎样
[2025-06-25 05:03:31] [DEBUG] 🚀 请求 vLLM: http://192.168.7.247:8011/v1/chat/completions
[2025-06-25 05:03:31] [DEBUG] 今天天气怎样 重写成功，用时 0.19s，结果：抱歉，我无法回答这个问题。
[2025-06-25 05:03:31] [INFO] 今天天气怎样 重写完成: 抱歉，我无法回答这个问题。
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 05:03:40] [INFO] 重写请求: 原始query=达人户如何开千川
[2025-06-25 05:03:40] [DEBUG] 🚀 请求 vLLM: http://192.168.7.247:8011/v1/chat/completions
[2025-06-25 05:03:40] [DEBUG] 达人户如何开千川 重写成功，用时 0.23s，结果：我想知道作为达人，如何开通千川广告？
[2025-06-25 05:03:40] [INFO] 达人户如何开千川 重写完成: 我想知道作为达人，如何开通千川广告？
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 05:40:20] [INFO] 重写请求: 原始query=你的主要工作是什么
[2025-06-25 05:40:20] [DEBUG] 🚀 请求 vLLM: http://192.168.7.247:8011/v1/chat/completions
[2025-06-25 05:40:21] [DEBUG] 你的主要工作是什么 重写成功，用时 0.18s，结果：我想知道你的主要工作是什么。
[2025-06-25 05:40:21] [INFO] 你的主要工作是什么 重写完成: 我想知道你的主要工作是什么。
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 05:40:30] [INFO] 重写请求: 原始query=@范文洁 短视频优化策略
[2025-06-25 05:40:30] [DEBUG] 🚀 请求 vLLM: http://192.168.7.247:8011/v1/chat/completions
[2025-06-25 05:40:30] [DEBUG] @范文洁 短视频优化策略 重写成功，用时 0.30s，结果：我想知道短视频优化可以从哪些方向入手？
[2025-06-25 05:40:30] [INFO] @范文洁 短视频优化策略 重写完成: 我想知道短视频优化可以从哪些方向入手？
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 05:40:42] [INFO] 重写请求: 原始query=你好，请做自我介绍
[2025-06-25 05:40:42] [DEBUG] 🚀 请求 vLLM: http://192.168.7.247:8011/v1/chat/completions
[2025-06-25 05:40:42] [DEBUG] 你好，请做自我介绍 重写成功，用时 0.32s，结果：我想知道阿理是谁，以及他如何帮助商户在抖音平台上投放广告和流量。
[2025-06-25 05:40:42] [INFO] 你好，请做自我介绍 重写完成: 我想知道阿理是谁，以及他如何帮助商户在抖音平台上投放广告和流量。
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 05:41:07] [INFO] 重写请求: 原始query=今天天气怎样
[2025-06-25 05:41:07] [DEBUG] 🚀 请求 vLLM: http://192.168.7.247:8011/v1/chat/completions
[2025-06-25 05:41:07] [DEBUG] 今天天气怎样 重写成功，用时 0.28s，结果：抱歉，我无法回答这个问题。我主要负责抖音广告投放相关咨询。
[2025-06-25 05:41:07] [INFO] 今天天气怎样 重写完成: 抱歉，我无法回答这个问题。我主要负责抖音广告投放相关咨询。
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 05:41:15] [INFO] 重写请求: 原始query=你好，请做自我介绍
[2025-06-25 05:41:15] [DEBUG] 🚀 请求 vLLM: http://192.168.7.247:8011/v1/chat/completions
[2025-06-25 05:41:15] [DEBUG] 你好，请做自我介绍 重写成功，用时 0.17s，结果：我想知道阿理是谁？
[2025-06-25 05:41:15] [INFO] 你好，请做自我介绍 重写完成: 我想知道阿理是谁？
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 05:41:24] [INFO] 重写请求: 原始query=你好，请做自我介绍
[2025-06-25 05:41:24] [DEBUG] 🚀 请求 vLLM: http://192.168.7.247:8011/v1/chat/completions
[2025-06-25 05:41:24] [DEBUG] 你好，请做自我介绍 重写成功，用时 0.18s，结果：我想知道你是一个什么样的客服？
[2025-06-25 05:41:24] [INFO] 你好，请做自我介绍 重写完成: 我想知道你是一个什么样的客服？
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 05:41:34] [INFO] 重写请求: 原始query=redis截图1
[2025-06-25 05:41:34] [DEBUG] 🚀 请求 vLLM: http://192.168.7.247:8011/v1/chat/completions
[2025-06-25 05:41:34] [DEBUG] redis截图1 重写成功，用时 0.42s，结果：我想知道您是否需要查询账户余额，如果是的话，请提供业务号以便我为您截图。
[2025-06-25 05:41:34] [INFO] redis截图1 重写完成: 我想知道您是否需要查询账户余额，如果是的话，请提供业务号以便我为您截图。
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 05:41:42] [INFO] 重写请求: 原始query=全域推广怎么投
[2025-06-25 05:41:42] [DEBUG] 🚀 请求 vLLM: http://192.168.7.247:8011/v1/chat/completions
[2025-06-25 05:41:43] [DEBUG] 全域推广怎么投 重写成功，用时 0.37s，结果：我想知道如何在抖音上进行全域推广，以及如何设置ROI目标和使用平台发券功能来提升转化？
[2025-06-25 05:41:43] [INFO] 全域推广怎么投 重写完成: 我想知道如何在抖音上进行全域推广，以及如何设置ROI目标和使用平台发券功能来提升转化？
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 05:42:01] [INFO] 重写请求: 原始query=你好帮我查一下下单了招募任务 达人交付完成后 怎么取消该计划
[2025-06-25 05:42:01] [DEBUG] 🚀 请求 vLLM: http://192.168.7.247:8011/v1/chat/completions
[2025-06-25 05:42:01] [DEBUG] 你好帮我查一下下单了招募任务 达人交付完成后 怎么取消该计划 重写成功，用时 0.24s，结果：我想知道招募任务达人交付完成后，如何取消该计划？
[2025-06-25 05:42:01] [INFO] 你好帮我查一下下单了招募任务 达人交付完成后 怎么取消该计划 重写完成: 我想知道招募任务达人交付完成后，如何取消该计划？
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 05:42:10] [INFO] 重写请求: 原始query=达人户如何开千川
[2025-06-25 05:42:10] [DEBUG] 🚀 请求 vLLM: http://192.168.7.247:8011/v1/chat/completions
[2025-06-25 05:42:10] [DEBUG] 达人户如何开千川 重写成功，用时 0.33s，结果：我想知道作为达人户，如何开通千川？
[2025-06-25 05:42:10] [INFO] 达人户如何开千川 重写完成: 我想知道作为达人户，如何开通千川？
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 05:42:34] [INFO] 重写请求: 原始query=你好，请做自我介绍
[2025-06-25 05:42:34] [DEBUG] 🚀 请求 vLLM: http://192.168.7.247:8011/v1/chat/completions
[2025-06-25 05:42:34] [WARNING] ⚠️ vLLM 请求失败: http://192.168.7.247:8011/v1/chat/completions → 400, message='Bad Request', url='http://192.168.7.247:8011/v1/chat/completions'
[2025-06-25 05:42:34] [ERROR] ⚠️ 重写请求失败，返回原问题：所有 vLLM 实例请求失败: http://192.168.7.247:8011/v1/chat/completions: 400, message='Bad Request', url='http://192.168.7.247:8011/v1/chat/completions'
[2025-06-25 05:42:34] [INFO] 你好，请做自我介绍 重写完成: 你好，请做自我介绍
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 05:42:46] [INFO] 重写请求: 原始query=千川账户如何授权抖音投放
[2025-06-25 05:42:46] [DEBUG] 🚀 请求 vLLM: http://192.168.7.247:8011/v1/chat/completions
[2025-06-25 05:42:47] [DEBUG] 千川账户如何授权抖音投放 重写成功，用时 0.31s，结果：我想知道千川账户如何授权抖音投放。
[2025-06-25 05:42:47] [INFO] 千川账户如何授权抖音投放 重写完成: 我想知道千川账户如何授权抖音投放。
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 05:43:00] [INFO] 重写请求: 原始query=新加群聊，大家好
[2025-06-25 05:43:00] [DEBUG] 🚀 请求 vLLM: http://192.168.7.247:8011/v1/chat/completions
[2025-06-25 05:43:00] [DEBUG] 新加群聊，大家好 重写成功，用时 0.25s，结果：我想知道新加群聊时，如何向大家介绍自己？
[2025-06-25 05:43:00] [INFO] 新加群聊，大家好 重写完成: 我想知道新加群聊时，如何向大家介绍自己？
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 05:43:08] [INFO] 重写请求: 原始query=你主要是做哪方面的
你主要是做哪方面的
[2025-06-25 05:43:08] [DEBUG] 🚀 请求 vLLM: http://192.168.7.247:8011/v1/chat/completions
[2025-06-25 05:43:08] [DEBUG] 你主要是做哪方面的
你主要是做哪方面的 重写成功，用时 0.19s，结果：我想知道你主要是做哪方面的？
[2025-06-25 05:43:08] [INFO] 你主要是做哪方面的
你主要是做哪方面的 重写完成: 我想知道你主要是做哪方面的？
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 05:43:21] [INFO] 重写请求: 原始query=完成交付后怎么取消该任务呢
[2025-06-25 05:43:21] [DEBUG] 🚀 请求 vLLM: http://192.168.7.247:8011/v1/chat/completions
[2025-06-25 05:43:21] [DEBUG] 完成交付后怎么取消该任务呢 重写成功，用时 0.22s，结果：我想知道完成交付后怎么取消该任务呢？
[2025-06-25 05:43:21] [INFO] 完成交付后怎么取消该任务呢 重写完成: 我想知道完成交付后怎么取消该任务呢？
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 05:43:30] [INFO] 重写请求: 原始query=我是电商达人
[2025-06-25 05:43:30] [DEBUG] 🚀 请求 vLLM: http://192.168.7.247:8011/v1/chat/completions
[2025-06-25 05:43:30] [DEBUG] 我是电商达人 重写成功，用时 0.20s，结果：我想知道电商达人这个身份是什么意思？
[2025-06-25 05:43:30] [INFO] 我是电商达人 重写完成: 我想知道电商达人这个身份是什么意思？
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 05:43:41] [INFO] 重写请求: 原始query=开启网络直播条件设置
[2025-06-25 05:43:41] [DEBUG] 🚀 请求 vLLM: http://192.168.7.247:8011/v1/chat/completions
[2025-06-25 05:43:41] [DEBUG] 开启网络直播条件设置 重写成功，用时 0.19s，结果：我想知道开启网络直播的条件是什么？
[2025-06-25 05:43:41] [INFO] 开启网络直播条件设置 重写完成: 我想知道开启网络直播的条件是什么？
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 05:43:53] [INFO] 重写请求: 原始query=你好，请做自我介绍
[2025-06-25 05:43:53] [DEBUG] 🚀 请求 vLLM: http://192.168.7.247:8011/v1/chat/completions
[2025-06-25 05:43:53] [DEBUG] 你好，请做自我介绍 重写成功，用时 0.22s，结果：我想知道您是谁，可以做自我介绍吗？
[2025-06-25 05:43:53] [INFO] 你好，请做自我介绍 重写完成: 我想知道您是谁，可以做自我介绍吗？
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
INFO:     192.168.7.58:64678 - "POST /rag/query HTTP/1.1" 404 Not Found
INFO:     192.168.7.58:64680 - "POST /query HTTP/1.1" 404 Not Found
[2025-06-25 22:06:08] [INFO] 重写请求: 原始query=新加入问答库的一张图片
[2025-06-25 22:06:08] [DEBUG] 🚀 请求 vLLM: http://192.168.7.247:8011/v1/chat/completions
[2025-06-25 22:06:08] [DEBUG] 新加入问答库的一张图片 重写成功，用时 0.33s，结果：我想知道新加入问答库的一张图片是什么？
[2025-06-25 22:06:08] [INFO] 新加入问答库的一张图片 重写完成: 我想知道新加入问答库的一张图片是什么？
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 22:06:17] [INFO] 重写请求: 原始query=怎么利用网络服务赚钱
[2025-06-25 22:06:17] [DEBUG] 🚀 请求 vLLM: http://192.168.7.247:8011/v1/chat/completions
[2025-06-25 22:06:17] [DEBUG] 怎么利用网络服务赚钱 重写成功，用时 0.51s，结果：我想知道有哪些方法可以在抖音上赚钱，比如开小店卖货、直播带货、接广告合作，或者投流放大销量。
[2025-06-25 22:06:17] [INFO] 怎么利用网络服务赚钱 重写完成: 我想知道有哪些方法可以在抖音上赚钱，比如开小店卖货、直播带货、接广告合作，或者投流放大销量。
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 22:06:26] [INFO] 重写请求: 原始query=你好，请做自我介绍
[2025-06-25 22:06:26] [DEBUG] 🚀 请求 vLLM: http://192.168.7.247:8011/v1/chat/completions
[2025-06-25 22:06:26] [WARNING] ⚠️ vLLM 请求失败: http://192.168.7.247:8011/v1/chat/completions → 400, message='Bad Request', url='http://192.168.7.247:8011/v1/chat/completions'
[2025-06-25 22:06:26] [ERROR] ⚠️ 重写请求失败，返回原问题：所有 vLLM 实例请求失败: http://192.168.7.247:8011/v1/chat/completions: 400, message='Bad Request', url='http://192.168.7.247:8011/v1/chat/completions'
[2025-06-25 22:06:26] [INFO] 你好，请做自我介绍 重写完成: 你好，请做自我介绍
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 22:06:35] [INFO] 重写请求: 原始query=我是电商达人
[2025-06-25 22:06:35] [DEBUG] 🚀 请求 vLLM: http://192.168.7.247:8011/v1/chat/completions
[2025-06-25 22:06:35] [DEBUG] 我是电商达人 重写成功，用时 0.20s，结果：我想知道电商达人这个身份是什么意思？
[2025-06-25 22:06:35] [INFO] 我是电商达人 重写完成: 我想知道电商达人这个身份是什么意思？
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 22:06:43] [INFO] 重写请求: 原始query=全域推广怎么投
[2025-06-25 22:06:43] [DEBUG] 🚀 请求 vLLM: http://192.168.7.247:8011/v1/chat/completions
[2025-06-25 22:06:44] [DEBUG] 全域推广怎么投 重写成功，用时 0.22s，结果：我想知道如何在抖音上使用全域推广来赚钱？
[2025-06-25 22:06:44] [INFO] 全域推广怎么投 重写完成: 我想知道如何在抖音上使用全域推广来赚钱？
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 22:07:00] [INFO] 重写请求: 原始query=redis截图1
[2025-06-25 22:07:00] [DEBUG] 🚀 请求 vLLM: http://192.168.7.247:8011/v1/chat/completions
[2025-06-25 22:07:00] [DEBUG] redis截图1 重写成功，用时 0.28s，结果：我想知道您是否需要查询账户余额，麻烦提供一下业务号哦～
[2025-06-25 22:07:00] [INFO] redis截图1 重写完成: 我想知道您是否需要查询账户余额，麻烦提供一下业务号哦～
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 22:07:08] [INFO] 重写请求: 原始query=开启网络直播条件设置
[2025-06-25 22:07:08] [DEBUG] 🚀 请求 vLLM: http://192.168.7.247:8011/v1/chat/completions
[2025-06-25 22:07:08] [DEBUG] 开启网络直播条件设置 重写成功，用时 0.28s，结果：重写结果：我想知道开启抖音直播的条件是什么，包括哪些准备工作？
[2025-06-25 22:07:08] [INFO] 开启网络直播条件设置 重写完成: 重写结果：我想知道开启抖音直播的条件是什么，包括哪些准备工作？
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 22:07:21] [INFO] 重写请求: 原始query=你好，请做自我介绍
[2025-06-25 22:07:21] [DEBUG] 🚀 请求 vLLM: http://192.168.7.247:8011/v1/chat/completions
[2025-06-25 22:07:21] [DEBUG] 你好，请做自我介绍 重写成功，用时 0.16s，结果：我想知道你是谁？
[2025-06-25 22:07:21] [INFO] 你好，请做自我介绍 重写完成: 我想知道你是谁？
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 22:07:30] [INFO] 重写请求: 原始query=完成交付后怎么取消该任务呢
[2025-06-25 22:07:30] [DEBUG] 🚀 请求 vLLM: http://192.168.7.247:8011/v1/chat/completions
[2025-06-25 22:07:31] [DEBUG] 完成交付后怎么取消该任务呢 重写成功，用时 0.22s，结果：我想知道完成交付后怎么取消这个任务呢？
[2025-06-25 22:07:31] [INFO] 完成交付后怎么取消该任务呢 重写完成: 我想知道完成交付后怎么取消这个任务呢？
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 22:07:39] [INFO] 重写请求: 原始query=你好，请做自我介绍
[2025-06-25 22:07:39] [DEBUG] 🚀 请求 vLLM: http://192.168.7.247:8011/v1/chat/completions
[2025-06-25 22:07:40] [DEBUG] 你好，请做自我介绍 重写成功，用时 0.22s，结果：我想知道您是谁，可以做自我介绍吗？
[2025-06-25 22:07:40] [INFO] 你好，请做自我介绍 重写完成: 我想知道您是谁，可以做自我介绍吗？
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 22:07:48] [INFO] 重写请求: 原始query=你好帮我查一下下单了招募任务 达人交付完成后 怎么取消该计划
[2025-06-25 22:07:48] [DEBUG] 🚀 请求 vLLM: http://192.168.7.247:8011/v1/chat/completions
[2025-06-25 22:07:48] [DEBUG] 你好帮我查一下下单了招募任务 达人交付完成后 怎么取消该计划 重写成功，用时 0.24s，结果：我想知道招募任务达人交付完成后，如何取消该计划？
[2025-06-25 22:07:48] [INFO] 你好帮我查一下下单了招募任务 达人交付完成后 怎么取消该计划 重写完成: 我想知道招募任务达人交付完成后，如何取消该计划？
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 22:23:07] [INFO] 重写请求: 原始query=我是电商达人
[2025-06-25 22:23:07] [DEBUG] 🚀 请求 vLLM: http://192.168.7.247:8011/v1/chat/completions
[2025-06-25 22:23:07] [DEBUG] 我是电商达人 重写成功，用时 0.22s，结果：我想知道您主要是想带货还是做品牌曝光呢？
[2025-06-25 22:23:07] [INFO] 我是电商达人 重写完成: 我想知道您主要是想带货还是做品牌曝光呢？
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 22:23:18] [INFO] 重写请求: 原始query=你的主要工作是什么
[2025-06-25 22:23:18] [DEBUG] 🚀 请求 vLLM: http://192.168.7.247:8011/v1/chat/completions
[2025-06-25 22:23:19] [DEBUG] 你的主要工作是什么 重写成功，用时 0.18s，结果：我想知道你的主要工作是什么。
[2025-06-25 22:23:19] [INFO] 你的主要工作是什么 重写完成: 我想知道你的主要工作是什么。
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 22:23:31] [INFO] 重写请求: 原始query=redis截图1
[2025-06-25 22:23:31] [DEBUG] 🚀 请求 vLLM: http://192.168.7.247:8011/v1/chat/completions
[2025-06-25 22:23:31] [DEBUG] redis截图1 重写成功，用时 0.21s，结果：我想知道您所说的redis截图1是指什么？
[2025-06-25 22:23:31] [INFO] redis截图1 重写完成: 我想知道您所说的redis截图1是指什么？
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 22:23:39] [INFO] 重写请求: 原始query=你好，请做自我介绍
[2025-06-25 22:23:39] [DEBUG] 🚀 请求 vLLM: http://192.168.7.247:8011/v1/chat/completions
[2025-06-25 22:23:39] [DEBUG] 你好，请做自我介绍 重写成功，用时 0.19s，结果：我想知道请做一下自我介绍。
[2025-06-25 22:23:39] [INFO] 你好，请做自我介绍 重写完成: 我想知道请做一下自我介绍。
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 22:23:50] [INFO] 重写请求: 原始query=你好，请做自我介绍
[2025-06-25 22:23:50] [DEBUG] 🚀 请求 vLLM: http://192.168.7.247:8011/v1/chat/completions
[2025-06-25 22:23:50] [DEBUG] 你好，请做自我介绍 重写成功，用时 0.70s，结果：抱歉，我不能执行这个任务。作为一个AI语言模型，我的设计目的是为了帮助用户回答问题和提供信息，而不是参与多轮对话或进行自我介绍。如果您有任何其他问题需要帮助，请随时告诉我。
[2025-06-25 22:23:50] [INFO] 你好，请做自我介绍 重写完成: 抱歉，我不能执行这个任务。作为一个AI语言模型，我的设计目的是为了帮助用户回答问题和提供信息，而不是参与多轮对话或进行自我介绍。如果您有任何其他问题需要帮助，请随时告诉我。
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 22:23:58] [INFO] 重写请求: 原始query=完成交付后怎么取消该任务呢
[2025-06-25 22:23:58] [DEBUG] 🚀 请求 vLLM: http://192.168.7.247:8011/v1/chat/completions
[2025-06-25 22:23:59] [DEBUG] 完成交付后怎么取消该任务呢 重写成功，用时 0.34s，结果：我想知道完成交付后怎么取消已经投放完成的广告计划？
[2025-06-25 22:23:59] [INFO] 完成交付后怎么取消该任务呢 重写完成: 我想知道完成交付后怎么取消已经投放完成的广告计划？
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 22:24:08] [INFO] 重写请求: 原始query=开启网络直播条件设置
[2025-06-25 22:24:08] [DEBUG] 🔍 对话历史过短，跳过 Query 重写：开启网络直播条件设置
[2025-06-25 22:24:08] [INFO] 开启网络直播条件设置 重写完成: 开启网络直播条件设置
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
[2025-06-25 22:24:32] [INFO] 重写请求: 原始query=短视频优化策略
[2025-06-25 22:24:32] [DEBUG] 🚀 请求 vLLM: http://192.168.7.247:8011/v1/chat/completions
[2025-06-25 22:24:32] [DEBUG] 短视频优化策略 重写成功，用时 0.17s，结果：我想知道短视频优化策略。
[2025-06-25 22:24:32] [INFO] 短视频优化策略 重写完成: 我想知道短视频优化策略。
INFO:     192.168.18.23:0 - "POST /query_rewrite HTTP/1.1" 200 OK
