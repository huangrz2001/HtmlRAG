import json
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from utils.llm_api import rewrite_query_ChatGLM
from transformers import AutoTokenizer, AutoModel
import argparse
import time

# ======================== ä¸»å…¥å£ ========================
if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--lang", type=str, default="zh")
    # parser.add_argument("--summary_tokenizer", type=str, default="/data/huangruizhi/htmlRAG/chatglm3-6b")
    # parser.add_argument("--summary_model", type=str, default="/data/huangruizhi/htmlRAG/chatglm3-6b")
    parser.add_argument("--summary_tokenizer", type=str, default="THUDM/glm-4-9b-chat")
    parser.add_argument("--summary_model", type=str, default="THUDM/glm-4-9b-chat")
    parser.add_argument("--max_node_words_embed", type=int, default=4096)
    parser.add_argument("--min_node_words_embed", type=int, default=48)
    args = parser.parse_args()


    # åŠ è½½æ¨¡å‹ï¼ˆå¯æ³¨é‡Šæ‰æ‘˜è¦éƒ¨åˆ†ä»¥åŠ é€Ÿï¼‰
    device = "cuda:1" if torch.cuda.is_available() else "cpu"
    tokenizer = AutoTokenizer.from_pretrained(args.summary_tokenizer, trust_remote_code=True)
    model = AutoModel.from_pretrained(args.summary_model, trust_remote_code=True).half().to(device)
    model.eval()

    start_time = time.time()  # è®°å½•å¼€å§‹æ—¶é—´
    # ======== éå†é‡å†™ demo ========
    with open("rewriting_test_set.jsonl", "r", encoding="utf-8") as f:
        for line in f:
            item = json.loads(line)
            dialogue = item["dialogue"]
            final_query = item["final_query"]
            predicted = rewrite_query_ChatGLM(dialogue, final_query, model, tokenizer)
            print(f"ğŸ”¹ åŸé—®é¢˜: {final_query}")
            print(f"âœ… é‡å†™å: {predicted}")
            print(f"ğŸ¯ æ ‡å‡†ç­”æ¡ˆ: {item['rewriting_target']}")
            print("------\n")
    end_time = time.time()  # è®°å½•ç»“æŸæ—¶é—´
    total_time = end_time - start_time
    print(f"åæ¡é‡å†™æ€»è€—æ—¶: {total_time} ç§’")

    while True:
        pass